---
title: "Estimation"
bibliography: references.bib
---

```{r }
#| label: setup
#| echo: false
#| message: false
#| 
library(here)

source(here('scripts', 'setup.R'), echo=F)
# Set seed for reproducibility
set.seed(42)

 #increase size for bootstrapping procedure
  options(future.globals.maxSize = 4000*1024^2) 

  grid.draw.ggsurvplot = function(x) {
    survminer:::print.ggsurvplot(x, newpage=F)
  }
  
d_cloned = readRDS(here('dta', 'survdta_cloned.R'))
```

# Introduction

I recommend the following stepwise procedure in estimation:

PLR

:   Pooled Logistic Regression, a model which approximates the hazards by discretizing follow-up time.

KM

:   Kaplan-Meier, non-parametric estimator which computes the instantaneous hazard at points of follow-up. Does not allow for time-varying confounding, but the non-parametric evaluation of time-trends can be useful for diagnostics and model specification of the PLR.

1.  Estimate unweighted/unadjusted cumulative incidences with KM method
2.  Estimate a PLR model without weights

<!-- -->

i)  Compare KM vs PLR

<!-- -->

3.  Estimate censoring weights

<!-- -->

i)  Estimate PLR for treatment initiation

    A. Estimate treatment model

    B. Compare KM / PLR estimates

    C. Compute IPCW

    D. Examine IPCW weights for extreme values, non-sensical values

    E. Examine balance in covariates across time

ii) Generate "table 1" with some weighted difference statistic (e.g. wSMD)

<!-- -->

4.  Estimate weighted outcome models

<!-- -->

i)  Estimate a weighted outcome model, no covariate adjustment
ii) Estimate a weighted outcome + covariates (usually main estimate)

<!-- -->

5.  Once satisfied with stability from steps 1-4, execute bootstraps
6.  Finalize report

<!-- -->

i)  provide KM, Cox unweighted, and PLR unweighted estimates in an appendix
ii) main results reported should be the IPCW-weighted PLR analysis

::: callout-note
Pooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of [Causal Inference: What If](https://www.hsph.harvard.edu/miguel-hernan/wp-content/uploads/sites/1268/2024/04/hernanrobins_WhatIf_26apr24.pdf), which explains why the odds approximates the hazard at a given time-point *k*, as long as the hazard is small (i.e. rare event) at time *k*.
:::

# Kaplan-Meier Estimator

A reasonable starting point is to evaluate the cloned dataset without probability weighting or pooled logistic regression models. Although this estimator is "naive" in that the artificial censoring is not considered, it is useful for the following reasons:

1)  It allows examination of the overall time trends, censoring and sample size which can reveal fatal issues with the target trial emulation. For example, treatment is too rare in the grace window used, or event rate is unexpectedly high or low.

2)  The non-parametric model allows a visual examination of the cumulative incidences (or event free survival probabilities) which are modeled parametrically in the pooled logistic regression. In other words, examining the time trend can help you determine if a simple polynomial or some more complex spline function is needed. This is not definitive however, because time-varying weights may change the time trends. But in my experience the unadjusted curve provides a good starting point, and practitioners should be very skeptical of weighted analysis that shows dramatically different time trends then the unweighted analysis (e.g. more likely to be an error in coding than real).

## Data

We continue the below using the cloned dataset generated in [Data](01_syndata.v2.qmd).

```{r }
#| label: desc-dta
glimpse(d_cloned)
```

## Estimation

```{r }
#| label: KM-est

d_person_c = select(data_cloned, id, time, outcome, assign, X, 
                    age, female) %>%
  group_by(id, assign) %>%
  dplyr::filter(time == max(time)) %>%
  ungroup

  d_km_est = broom::tidy(
    survfit(Surv(time, outcome) ~ assign, data=d_person_c)) %>% # <1>
    mutate(assign = str_extract(strata,  '(?<=assign=)\\d')
    ) %>%
    arrange(time) %>%
    select(-strata) %>%
    mutate(pr_ev = 1-estimate) %>% # <2>
    rename(pr_s = estimate) %>% # <2>
    pivot_wider(id_cols = c(time), # <3>
                names_from = assign, # <3>
                values_from = c(pr_ev, pr_s, # <3>
                                n.risk, n.event)) %>% # <3>
    mutate(cir = pr_ev_1 / pr_ev_0, # <4>
           cid = (pr_ev_1 - pr_ev_0)) # <4>
```

1.  Naive estimator, K-M estimator, assignment is by clone (not person). `t_clone` is the follow-up time for each clone, taking into account artificial censoring.
2.  `estimate` from the model is the survival probability, so probability of event is 1 - estimate
3.  Data is in long form, so one colum per group with cumulative incidence/survival
4.  Estimands, `cir` is ratio analogous to relative risk, and `cid` is analogous to risk difference

## Summarize KM estimator

```{r }
#| label: gg-km
#| echo: false
ggsurvplot(survfit(Surv(enter, exit, outcome) ~ assign, data=d_cloned),
                            data=d_cloned,
                            fun = 'event',
                            xlim = c(0, 60),
                            xlab = 'Follow-up',
                            break.time.by=6,
                            palette = c('red', 'blue'),
                            linetype = 'strata',
                            censor=F,
                            cumevents=T,
                            conf.int=F, pval=F,
                            risk.table=T, risk.table.col = 'strata',
                            risk.table.height=0.25, 
                            ggtheme = theme_classic())
```

# Pooled logistic regression

Next, we proceed to essentially replicate the finding of the KM estimator with a PLR model. No weights or covariate-adjustment is applied yet. These steps are computationally intensive and so this step is important to test your code. Make sure you have a good starting parametric model for the outcome to avoid any major issues downstream when you add weights. An unadusted PLR model should closely approximate the KM curves, if it doesn't then something is wrong.

## Estimation

The PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time\^2). Choose either to estimate the outcome in a model with both clones combined in one dataset OR estimate cumulative incidences separately (two models with data limited to `assign==1` & `assign==0` respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. `time + time^2 + treat + treat*time + treat*time^2`, shorthand below is `poly(time, 2, raw=T)*assign`.

```{r }
#| label: plr-naive-est
  
  # defined above
  d_glm = glm(outcome ~ poly(time, 2, raw=T)*assign, data=d_cloned, family=binomial()) # <1>
  
  d_cloned$pr_ev = d_glm$fitted.values # <2>
  d_cloned[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] # <3>
  
  d_plr_naive_est = d_cloned %>% # <4>
    group_by(assign, time) %>% # <4>
    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %>% # <4>
    ungroup %>% # <4>
    pivot_wider(., id_cols =c('time'), # <4>
                names_from = assign, # <4>
                names_prefix = 'pr_ev_', # <4>
                values_from = pr_ev # <4>
    ) %>%
    mutate(cid = pr_ev_1 - pr_ev_0, # <4>
           cir = pr_ev_1 / pr_ev_0) # <4>
```

1.  PLR model, with time\*treat interaction. Binomial family for logistic regression.
2.  If event_outc = 1 when event occurs, the fitted values are the probability of the outcome at time k.
3.  The event-free survival probability is 1 - cumulative probability (incidence) of an event. To compute, you take the cumulative product of the fitted probabilities `pr_ev`, making sure to only compute within group and clone.
4.  After you have cumulative event-free survival probability, then you can summarize by group/time as described above for the KM estimator.

```{r }
#| label: gg-plr-naive
#| fig-cap: Unadjusted, unweighted cumulative incidences by treatment group (PLR)
#| fig-cap-location: top
#| echo: false
    d_gg_ci = d_plr_naive_est %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = pr_ev_0), color='red', linewidth=1.2) +
      geom_line(aes(y = pr_ev_1), color='blue', linewidth=1.2) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(0, 1)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Cumulative incidence')
    
    d_gg_ci
```

At first glance, the plot looks reasonable. Direct comparison of the PLR and KM estimators is helpful for diagnosing problems in code or modeling steps.

```{r }
#| label: gg-cmp-naive-models
#| fig-cap: "Comparison of Naive PLR versus KM estimator"
#| fig-cap-location: top
#| echo: FALSE
d_ci_cmp = inner_join(
  select(d_plr_naive_est, time, pr_ev_0, pr_ev_1, cid) %>%
    rename(plr_0 = pr_ev_0, plr_1 = pr_ev_1, plr_cid = cid),
  select(d_km_est, time, pr_ev_1, pr_ev_0, cid) %>%
    rename(km_0 = pr_ev_0, km_1 = pr_ev_1, km_cid = cid),
  by = join_by(time)
)

d_gg_ci = d_ci_cmp %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = plr_0), color='#A60A3D', linetype=1, linewidth=1.1) +
      geom_point(aes(y = km_0), color='red', shape=2, size=1.5) + 
      geom_line(aes(y = plr_1), color='navyblue', linetype=1, linewidth=1.1) +
      geom_point(aes(y = km_1), color='blue', shape=2, size=1.5) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(0, 1)) +
      theme_bw() +
      labs(x = '', y = 'Cumulative incidence')
    
 d_gg_rr = d_ci_cmp %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = km_cid), color='green', linetype=1, linewidth=1.5) + 
      geom_line(aes(y = plr_cid), color='forestgreen', linetype=2, linewidth=1.5) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(-0.2, 0.2)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Risk Differences')
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr,
                       nrow=2)
    
d_gg_1
```

So the PLR model using a simple polynomial can reasonably approximate the KM estimate. This should work because the underlying synthetic data was generated with an exponential distribution for time, but real-world data will not play so nicely. There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider the following regarding the parametric time trend:

1.  It may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.

2.  The non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.

3.  These initial steps will not guarantee a good fit after weighting is applied, it is only a first-look for diagnostic purposes.

## Comparison of Covariate-adjusted PLR with Cox model

Another approach is to compare a pooled logistic regression analysis to a time-invariant Cox model. If time-varying confounding not an issue, this should give a similar result. However, censoring weights are still necessary in final analysis even if no confounding.[@cain2010]

```{r }
#| label: glm-plr-covadj
 
# defined above
  d_glm = glm(outcome ~ poly(time, 2, raw=T)*assign + female + age + X + X_t, 
              data=d_cloned, family=binomial()) # <1>

  d_cloned$p.event0 <- predict(d_glm, mutate(d_cloned, assign=0), type="response") # <2>
  d_cloned$p.event1 <- predict(d_glm, mutate(d_cloned, assign=1), type="response") # <2>
  
  d_cloned[, `:=`(pr_surv_1 = cumprod(1 - p.event1)), by=list(id, assign)] # <3>
  d_cloned[, `:=`(pr_surv_0 = cumprod(1 - p.event0)), by=list(id, assign)] # <3>
  
  d_plr_adj_est = d_cloned %>% # <4>
    group_by(time) %>% # <4>
    summarize(pr_ev_1 = mean(1-pr_surv_1),
              pr_ev_0 = mean(1-pr_surv_0), 
              .groups = 'drop') %>% # <4>
    ungroup %>% # <4>
    mutate(cid = pr_ev_1 - pr_ev_0, # <4>
           cir = pr_ev_1 / pr_ev_0) # <4>
```

1.  PLR model with some covariates for adjustment. (model fit for each treatment group)
2.  Predict outcome from each PLR.
3.  Estimate cumulative survival using cumulative product, within person-clone
4.  Summarize mean survival rates by time-period

::: callout-note
Note the outcome regression without weights is just for comparison and understanding, the final analysis must include probability weights for artificial censoring even if no confounding.
:::

```{r }
#| label: time-dependent cox model
#| message: FALSE
#| warning: FALSE
  d_cox = coxph(Surv(enter, exit, outcome) ~ assign + female + age + X + X_t, data = d_cloned) # <1>

  d_cox_est = surv_adjustedcurves(fit = d_cox, variable = "assign") %>% # <2>
      group_by(variable, time) %>% # <3>
        dplyr::filter(row_number()==1) %>% # <3>
      ungroup %>% # <3>
      mutate(pr_ev = 1 - surv) %>% # <3>
      pivot_wider(., id_cols =c('time'), # <3>
                  names_from = variable, # <3> 
                  names_prefix = 'pr_ev_', # <3>
                  values_from = pr_ev # <3> 
      ) %>% # <3>
      mutate(cid = pr_ev_1 - pr_ev_0, # <3> 
             cir = pr_ev_1 / pr_ev_0) # <3> 
```

For comparison, A time-dependent Cox model is fit.

1.  Cox model, with time1, time2 parameters which represent the start and stop time of each interval
2.  Estimation of adjusted survival curves by treatment group, with subpopulations balanced using `conditional` method.[@acompar]
3.  Summarizing survival probabilities by time

```{r }
#| label: gg-cmp-outcadj-models
#| echo: FALSE
#| fig-label: Time-dependent Cox model versus Covariate-adjusted PLR (no weights)
#| fig-cap-location: top

d_ci_cmp = inner_join(
  select(d_plr_adj_est, time, pr_ev_0, pr_ev_1, cid) %>%
    rename(plr_0 = pr_ev_0, plr_1 = pr_ev_1, plr_cid = cid),
  select(d_cox_est, time, pr_ev_1, pr_ev_0, cid) %>%
    rename(cox_0 = pr_ev_0, cox_1 = pr_ev_1, cox_cid = cid),
  by = join_by(time)
)

d_gg_ci = d_ci_cmp %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = plr_0), color='#A60A3D', linetype=1, linewidth=1.1) +
      geom_point(aes(y = cox_0), color='red', shape=2, size=1.5) + 
      geom_line(aes(y = plr_1), color='navyblue', linetype=1, linewidth=1.1) +
      geom_point(aes(y = cox_1), color='blue', shape=2, size=1.5) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(0, 1)) +
      theme_bw() +
      labs(x = '', y = 'Cumulative incidence')
    
 d_gg_rr = d_ci_cmp %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = cox_cid), color='green', linetype=1, linewidth=1.5) + 
      geom_line(aes(y = plr_cid), color='forestgreen', linetype=2, linewidth=1.5) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(-0.2, 0.2)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Risk Differences')
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr,
                       nrow=2)
    
d_gg_1
```

So the PLR also is also very similar to Cox model estimates and both appropriately find no difference in risk by treatment assignment (by synthetic data design).

# IPCW Pooled Logistic Regression

Once the basic procedures are setup and there is some confidence in the ability to model the cumulative incidences, weighting can begin.

::: callout-note
There is no consideration here to model training, causal DAGs or covariate selection. It is simply a toy example to show the procedure.
:::

## Build a longitudinal panel

The person- or clone-level data must be expanded to a longitudinal panel by time. Two datasets are constructed, one for estimation of censoring probabilities and one for estimation of outcome probabilities.

### Censoring dataset

In our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is specific to a project's definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we generate a dataset that is at the person-level with time to treatment as the outcome (no cloning):

```{r }
#| label: dta-censor
d_treat = d_cloned %>%
    dplyr::filter(assign==0) 
```

1.  This may be a little confusing, but I am taking clones where assign=0 from the `d_cloned` dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.
2.  We are estimating time to treatment, not outcome
3.  *Artificial* censoring time is same as treatment time unless dead first, and outcome = 1 if treated.

Then we expand the dataset as described above:

```{r }
#| label: exp-dta-censor
  setDT(d_treat)
  d_treat[, event_treat := if_else(t_treat <= time, 1L, 0L), by = list(id)]
  d_treat = select(d_treat, id, time, event_treat, t_treat, enter, exit, age, female, X, X_t) # <1>
```

1.  Note we keep covariates for estimation of censoring weights.

## Estimation of weights

The probability weight, AKA inverse probability censoring weight (IPCW) or inverse probability of no loss to follow-up is estimated in this way:

$$
W^C_i = \prod^{T}_{t_0} \frac{P(C_t = 0| \overline{C}_{t-1} = 0)}{P(C_t=0 | X=x, \overline{L}_{t-1}=l_{i, t-1}, \overline{C}_{t-1}=0)}
$$

C in this case means censoring, but censoring occurs according to treatment strategy, so it really is the probability of adhering to the treatment you were assigned to. Stabilized weights are tricky, here we are using the marginal stabilized weight which is the probability of no censoring at each timepoint. The denominator is the conditional probability accounting for fixed, i.e. baseline, (X) and time-varying (L) covariates.

We fit a model with the outcome of censoring, including the key variables. Then we add fitted probabilities back to the outcome dataset, and calculate cumulative probabilities and weights.

```{r }
#| label: est-censor-prob
#| warning: FALSE

  # Numerator (margin probability)
  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T), data=d_treat, family=binomial()) # <1>
  d_cloned$pr_censnum = predict(d_glm_wt, newdata = d_cloned, type='response') # <1>

  # Denominator 
  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T) + X + X_t, data=d_treat, family=binomial()) # <2>
  
  d_cloned$pr_censdenom = predict(d_glm_wt, newdata = d_cloned, type='response') # <2>
```

1.  Marginal probability of no censoring
2.  Conditional probability (covariate-adjusted) for no censoring

::: callout-note
I think there is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of "censoring" where that may mean treatment initiation or some other thing.[@gaber2024]
:::

## Calculation of weights

This step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to the [Weighting](05_appendix.qmd) section for further discussion.

```{r }
#| label: cpt-weights
  setDT(d_cloned)
  
  d_cloned[, ipw := fcase(
    assign==0, 1 / (1-pr_censdenom), # <1>
    assign==0 & t_treat==time, 0,
    assign==1 & time < 12, 1, # <2>
    assign==1 & time == 12  & t_treat  < 12, 1, # <3> 
    assign==1 & time == 12  & t_treat  ==12, 1 / (pr_censdenom), # <4> 
    assign==1 & time == 12  & t_treat  >12 & outcome==0, 0, # <5> 
    assign==1 & time == 12  & t_treat  >12 & outcome==1, 1, # <6> 
    assign==1 & time > 12, 1 # <7>
  )]
  
  d_cloned[, marg_ipw := fcase(
    assign==0, (1-pr_censnum) / (1-pr_censdenom), # <8>
    assign==1 & time == 12 & t_treat   ==12, pr_censnum / (pr_censdenom), # <9>
    default = ipw
  )]
  
  d_cloned[, `:=`(ipw = cumprod(ipw),
                  marg_ipw = cumprod(marg_ipw)),
               by=list(id, assign)] # <10>
  
```

1.  (assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored
2.  (assign=1) Clones cannot artificially censor prior to grace period
3.  (assign=1) If treatment started prior to grace window ending, the clone cannot censor
4.  (assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).
5.  (assign=1) If a clone is not treated they censor at the end of the window
6.  (assign=1) In periods greater than the end of the grace window, no artificial censoring occurs (they either are treated or already censored at this point).
7. If died in last interval, do not set weight to zero.
8.  Marginal IPW is same, except numerator probability replaces 1
9.  After setting these conditions, we compute the cumulative product of the weights.

### Overall IPW Distribution  

```{r }
#| label: desc-ipw
#| echo: false
  summary(d_cloned$ipw[d_cloned$ipw!=0])
```

```{r }
#| label: desc-marg_ipw
#| echo: false
  summary(d_cloned$marg_ipw[d_cloned$marg_ipw!=0])
```

The unstabilized weights floor at 1, and we see high weights assigned to some. The marginalized weights have a mean of 1 (expected). 

### IPW Distribution at end of grace period for treatment clones

```{r }
#| label: desc-ipw-grace
#| echo: false
  summary(d_cloned$ipw[d_cloned$ipw!=0 & d_cloned$assign==1])
```
The weights at the end of the grace period are key.  

It is good to see the marginal stabilized weights have a mean 1. You can also plot this across time:

```{r }
#| label: desc-ipw-time
#| fig-cap: Distribution of Censoring Weights Across Time
#| fig-cap-location: top

  d_cloned %>%
    dplyr::filter(marg_ipw!=0) %>%
 ggplot(., aes(x = time, y = marg_ipw)) +
  geom_violin(aes(group = cut_width(time, 12)), 
               scale = "width", fill = "blue", alpha = 0.7) +
  geom_hline(aes(yintercept=1), linetype=3) +
  scale_x_continuous(breaks = seq(0,60, 12)) +
  labs(x = "Follow-up", y = "IPW") +
  theme_bw()

```

Another way to examine the weights is to look at the weighted counts of individuals at risk, and number of events pre- and post-weighting:

```{r }
#| label: weight-table
#| tbl-cap: Unweighted and Weighted Counts, N at risk (cumulative events)
#| tbl-cap-location: top
d_cloned %>%
  group_by(time, assign) %>%
  summarize(n = sum(ipw!=0), 
            n_wt = sum(ipw),
            ev = sum(outcome[ipw!=0]),
            ev_wt = sum(outcome*ipw),
            .groups = 'drop') %>%
  mutate(unwt_evn = paste0(n, ' (', ev, ')'),
         wt_evn = paste0(round(n_wt, 0), ' (', round(ev_wt, 0), ')')) %>%
  pivot_wider(id_cols = time, names_from = assign, values_from = c(unwt_evn, wt_evn)) %>%
  dplyr::filter(time %in% c(1, 12, 30, 60)) %>%
  rename(`Time` = time,
         `Unweighted, assign=0` = unwt_evn_0,
         `Unweighted, assign=1` = unwt_evn_1,
         `Weighted, assign=0` = wt_evn_0,
         `Weighted, assign=1` = wt_evn_1,
         ) %>%
  kable(align='c', digits =0) %>%
  kable_styling()
```

Like most applications of probability weights, the distribution of weights should be examined and compared to the unweighted population (size and event counts) to identify any problems.

Now with the estimated weights, it is simple to generate weighted cumulative incidences:

```{r }
#| label: est-plr-wt
#| warning: FALSE  
#| 
  d_glm_pe_1 = glm(outcome==0 ~ poly(time, 2, raw=T) + female + age + X, 
                   data=d_cloned[assign==1, ], 
                   family=binomial(), weights = ipw) # <1>

  d_glm_pe_0 = glm(outcome==0 ~ poly(time, 2, raw=T) + female + age + X, 
                   data=d_cloned[assign==0, ], 
                     family=binomial(), weights = ipw) # <1>
  
  d_cloned$pr_1 = predict(d_glm_pe_1, newdata = d_cloned, type='response') # <2>
  d_cloned$pr_0 = predict(d_glm_pe_0, newdata = d_cloned, type='response') # <2>

```

1.  Estimate for each assignment group separately. Note the `weights = ipw` argument, everything else is same as PLR model above. R `glm()` will generate a warning message because the weighted counts are "non-integer", but this is expected and not a problem.
2.  Fitted probabilities from each model

```{r }
#| label: survprob
#| 
  d_cloned[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] # <1>
  d_cloned[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] # <1>
  
  d_plrwt_est = d_cloned %>% # <2>
    group_by(time) %>% # <2>
    summarize(pr_ev_1 = mean(1-pr_cum_1), 
              pr_ev_0 = mean(1-pr_cum_0),
              .groups = 'drop') %>% # <2>
    ungroup %>% # <2>
    mutate(cid = pr_ev_1 - pr_ev_0,  # <2>
           cir = pr_ev_1 / pr_ev_0) # <2>
```

1.  Cumulative product

2.  Summarize across group, time as before.

```{r }
#| label: plot-plr-weighted
#| fig-cap: Weighted PLR Analysis
#| echo: false

  ### Plot ----
    d_gg_ci = d_plrwt_est %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = pr_ev_0), color='red', linewidth = 1) +
      geom_line(aes(y = pr_ev_1), color='blue', linewidth = 1, linetype=2) + 
      geom_vline(aes(xintercept = 12), color='black', linewidth = 0.5, linetype=3) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(0, 1)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Cumulative incidence')

    d_gg_cid = d_plrwt_est %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = cid), color='#228b22', linewidth = 1, linetype=4) + 
      geom_hline(aes(yintercept = 0), color='black', linewidth = 0.5, linetype=3) + 
      scale_y_continuous(limits = c(-0.2, 0.2)) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Risk Difference', caption = '12-month grace period')
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_cid,
                       nrow=2) 
    
    d_gg_1
```

# Final Comparison

```{r }
#| label: tbl-compare-est
#| tbl-cap-location: top
#| tbl-cap: 'Comparison of estimates by model, PLR = Pooled Logistic Regression, Wtd = with censoring weights'
#| echo: false

  d_glm_pe_1 = glm(outcome==0 ~ poly(time, 2, raw=T) + female + age + X, 
                     data=d_cloned[assign==1, ], 
                     family=binomial(), weights = marg_ipw) # <1>

  d_glm_pe_0 = glm(outcome==0 ~ poly(time, 2, raw=T) + female + age + X, 
                   data=d_cloned[assign==0, ], 
                     family=binomial(), weights = marg_ipw) # <1>
  
  d_cloned$pr_1 = predict(d_glm_pe_1, newdata = d_cloned, type='response') # <2>
  d_cloned$pr_0 = predict(d_glm_pe_0, newdata = d_cloned, type='response') # <2>

  d_cloned[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] # <1>
  d_cloned[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] # <1>
  
  d_plrstabwt_est = d_cloned %>% # <2>
    group_by(time) %>% # <2>
    summarize(pr_ev_1 = mean(1-pr_cum_1), 
              pr_ev_0 = mean(1-pr_cum_0),
              .groups = 'drop') %>% # <2>
    ungroup %>% # <2>
    mutate(cid = pr_ev_1 - pr_ev_0,  # <2>
           cir = pr_ev_1 / pr_ev_0) # <2>
  
 d_all_est = 
  bind_rows(
    select(d_km_est, time, pr_ev_0, pr_ev_1, cid, cir) %>%
        mutate(Model = 'Kaplan-Meier'),
    select(d_plr_naive_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'PLR-Naive'),
    select(d_cox_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'Cox'),
    select(d_plrwt_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'PLR-Wtd'),
    select(d_plrstabwt_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'PLR-Stab-Wtd')
  ) %>%
  dplyr::filter(time %in% c(6, 12, 36, 60))

kables(
  list(d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = pr_ev_0) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("No treatment initiation ever" = 6)),
       d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = pr_ev_1) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Treatment initiation at 12 months" = 6)),
       d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = cid) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Risk Differences" = 6)),
       d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = cir) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Relative Risks" = 6))
)) 
```

The weighted analysis observes similar effects to time-dependent cox model.

# References

::: {#refs}
:::
