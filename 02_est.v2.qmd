---
title: "Estimation"
bibliography: references.bib
---

```{r }
#| label: setup
#| echo: false
#| message: false
#| 
library(here)

source(here('scripts', 'setup.R'), echo=F)
# Set seed for reproducibility
set.seed(42)

 #increase size for bootstrapping procedure
  options(future.globals.maxSize = 4000*1024^2) 

  grid.draw.ggsurvplot = function(x) {
    survminer:::print.ggsurvplot(x, newpage=F)
  }
  
dta_c_person = readRDS(here('dta', 'dta_cloned_person.Rds'))
dta_c_panel = readRDS(here('dta', 'dta_cloned_panel.Rds'))
```

# Introduction

I recommend the following stepwise procedure in estimation:

PLR

:   Pooled Logistic Regression, a model which approximates the hazards by discretizing follow-up time.

KM

:   Kaplan-Meier, non-parametric estimator which computes the instantaneous hazard at points of follow-up. Does not allow for time-varying confounding, but the non-parametric evaluation of time-trends can be useful for diagnostics and model specification of the PLR.

1.  Estimate unweighted/unadjusted cumulative incidences with KM method
2.  Estimate a PLR model without weights

<!-- -->

i)  Compare KM vs PLR

<!-- -->

3.  Estimate censoring weights

<!-- -->

i)  Estimate PLR for treatment initiation

    A. Estimate treatment model

    B. Compare KM / PLR estimates

    C. Compute IPCW

    D. Examine IPCW weights for extreme values, non-sensical values

    E. Examine balance in covariates across time

ii) Generate "table 1" with some weighted difference statistic (e.g. wSMD)

<!-- -->

4.  Estimate weighted outcome models

<!-- -->

i)  Estimate a weighted outcome model, no covariate adjustment
ii) Estimate a weighted outcome + covariates (usually main estimate)

<!-- -->

5.  Once satisfied with stability from steps 1-4, execute bootstraps
6.  Finalize report

<!-- -->

i)  provide KM, Cox unweighted, and PLR unweighted estimates in an appendix
ii) main results reported should be the IPCW-weighted PLR analysis

::: callout-note
Pooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of [Causal Inference: What If](https://www.hsph.harvard.edu/miguel-hernan/wp-content/uploads/sites/1268/2024/04/hernanrobins_WhatIf_26apr24.pdf), which explains why the odds approximates the hazard at a given time-point *k*, as long as the hazard is small (i.e. rare event) at time *k*.
:::

# Kaplan-Meier Estimator

A reasonable starting point is to evaluate the cloned dataset without probability weighting or pooled logistic regression models. Although this estimator is "naive" in that the artificial censoring is not considered, it is useful for the following reasons:

1)  It allows examination of the overall time trends, censoring and sample size which can reveal fatal issues with the target trial emulation. For example, treatment is too rare in the grace window used, or event rate is unexpectedly high or low.

2)  The non-parametric model allows a visual examination of the cumulative incidences (or event free survival probabilities) which are modeled parametrically in the pooled logistic regression. In other words, examining the time trend can help you determine if a simple polynomial or some more complex spline function is needed. This is not definitive however, because time-varying weights may change the time trends. But in my experience the unadjusted curve provides a good starting point, and practitioners should be very skeptical of weighted analysis that shows dramatically different time trends then the unweighted analysis (e.g. more likely to be an error in coding than real).

## Data

We continue the below using the cloned dataset generated in [Data](01_syndata.v3.qmd).

```{r }
#| label: desc-dta
glimpse(dta_c_person)
```

## Estimation

```{r }
#| label: KM-est
  d_km_est = broom::tidy(
    survfit(Surv(time, event) ~ assign, data=dta_c_person)) %>% # <1>
    mutate(assign = str_extract(strata,  '(?<=assign=)\\d')
    ) %>%
    arrange(time) %>%
    select(-strata) %>%
    mutate(pr_ev = 1-estimate) %>% # <2>
    rename(pr_s = estimate) %>% # <2>
    pivot_wider(id_cols = c(time), # <3>
                names_from = assign, # <3>
                values_from = c(pr_ev, pr_s, # <3>
                                n.risk, n.event)) %>% # <3>
    mutate(cir = pr_ev_1 / pr_ev_0, # <4>
           cid = (pr_ev_1 - pr_ev_0)) # <4>
```

1.  Naive estimator, K-M estimator, assignment is by clone (not person). `t_clone` is the follow-up time for each clone, taking into account artificial censoring.
2.  `estimate` from the model is the survival probability, so probability of event is 1 - estimate
3.  Data is in long form, so one colum per group with cumulative incidence/survival
4.  Estimands, `cir` is ratio analogous to relative risk, and `cid` is analogous to risk difference

## Summarize KM estimator

```{r }
#| label: gg-km
#| echo: false
ggsurvplot(survfit(Surv(time, event) ~ assign, data=dta_c_person),
                            data=dta_c_person,
                            fun = 'event',
                            xlim = c(1, 60),
                            xlab = 'Follow-up',
                            break.time.by=6,
                            palette = cbbPalette,
                            linetype = 'strata',
                            censor=F,
                            cumevents=T,
                            conf.int=F, pval=F,
                            risk.table=T, risk.table.col = 'strata',
                            risk.table.height=0.25, 
                            ggtheme = theme_bw())
```

## Cox Proportional Hazards

The data was designed so treatment has no causal effect on outcome, but confounding is present [Data](01_syndata.v3.qmd). Adjustment for confounder, `X` identifies null effort of assignment.

```{r }
#| label: cox-mod
#| echo: FALSE
#| warning: FALSE

d_cox = coxph(Surv(time, event) ~ assign + X, data = dta_c_person) 

d_cox_est = surv_adjustedcurves(fit = d_cox, 
                                variable = "assign", 
                                data = as.data.frame(dta_c_person)) %>%
    group_by(variable, time) %>%
      dplyr::filter(row_number()==1) %>%
    ungroup %>%
    mutate(pr_ev = 1 - surv) %>%
    pivot_wider(., id_cols =c('time'), # 
                names_from = variable, # 
                names_prefix = 'pr_ev_',
                values_from = pr_ev # 
    ) %>%
    mutate(cid = pr_ev_1 - pr_ev_0, # 
           cir = pr_ev_1 / pr_ev_0) # 
    
tbl_regression(d_cox, exponentiate=T)
```

# Pooled logistic regression

Next, we proceed to essentially replicate the finding of the KM estimator with a PLR model. No weights or covariate-adjustment is applied yet. These steps are computationally intensive and so this step is important to test your code. Make sure you have a good starting parametric model for the outcome to avoid any major issues downstream when you add weights. An unadusted PLR model should closely approximate the KM curves, if it doesn't then something is wrong.

## Estimation

The PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time\^2). Choose either to estimate the outcome in a model with both clones combined in one dataset OR estimate cumulative incidences separately (two models with data limited to `assign==1` & `assign==0` respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. `time + time^2 + treat + treat*time + treat*time^2`, shorthand below is `poly(time, 2, raw=T)*assign`.

```{r }
#| label: plr-naive-est
  
  # defined above
  d_glm = glm(event==0 ~ poly(time, 2, raw=T)*assign, data=dta_c_panel, family=binomial()) # <1>
  
  d_plr_naive_est = crossing(assign = 1:0, time = 1:60)
  
  d_plr_naive_est$pr_surv = predict(d_glm, newdata = d_plr_naive_est, type = 'response') 
  d_plr_naive_est = d_plr_naive_est %>%
    group_by(assign) %>%
    mutate(pr_cumsurv = cumprod(pr_surv),
           pr_cumev = 1 - pr_cumsurv) %>% # <2>
    ungroup %>% # <3>
    pivot_wider(., id_cols =c('time'), # <3>
                names_from = assign, # <3>
                names_prefix = 'pr_ev_', # <3>
                values_from = pr_cumev # <3>
    ) %>%
    mutate(cid = pr_ev_1 - pr_ev_0, # <3>
           cir = pr_ev_1 / pr_ev_0) # <3>
```

1.  PLR model, with time\*treat interaction. Binomial family for logistic regression.
2.  The cumulative incidence is 1 - cumulative event-free survival probability. To compute, you take the cumulative product by assignment group.
3.  Reorganize data and summarize by group/time similar to above for the KM estimator.

```{r }
#| label: gg-plr-naive
#| fig-cap: Unadjusted, unweighted cumulative incidences by treatment group (PLR)
#| fig-cap-location: top
#| echo: false
#| eval: false
    d_gg_ci = d_plr_naive_est %>%
      pivot_longer(cols = c(pr_ev_0, pr_ev_1), names_to = 'assign', values_to = 'pr_ev') %>%
      ggplot(aes(x=time, y = pr_ev, color = assign)) +
      geom_line(aes(linetype=assign), linewidth=1.2) +
      scale_color_manual(labels = c('Assign=0', 'Assign=1'), values = cbbPalette) +
      scale_linetype_manual(labels = c('Assign=0', 'Assign=1'), values = c(1, 2)) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Cumulative incidence')
    
    d_gg_ci
```

At first glance, the plot looks reasonable. Direct comparison of the PLR and KM estimators is helpful for diagnosing problems in code or modeling steps.

```{r }
#| label: gg-cmp-naive-models
#| fig-cap: "Comparison of Naive PLR versus KM estimator"
#| fig-cap-location: top
#| echo: FALSE
d_ci_cmp = bind_rows(
  select(d_plr_naive_est, time, pr_ev_0, pr_ev_1, cid) %>%
    mutate(Model = 'PLR-Naive'),
  select(d_km_est, time, pr_ev_1, pr_ev_0, cid) %>%
    mutate(Model = 'KM-Naive')) %>%
    pivot_longer(cols = c(pr_ev_0, pr_ev_1), names_to = 'assign', 
                 names_prefix = 'pr_ev_', values_to = 'pr_ev')

d_gg_ci = d_ci_cmp %>%
      ggplot(aes(x=time, color = Model, Group = Model)) +
      geom_line(aes(y = pr_ev, linetype=assign), linewidth=0.9) +
      geom_point(aes(y = pr_ev, shape=assign), size=1.2) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_color_manual(values = cbbPalette) +
      theme_bw() +
      labs(x = '', y = 'Cumulative incidence') 
    
 d_gg_rr = d_ci_cmp %>%
      ggplot(aes(x=time, color = Model)) +
      geom_line(aes(y = cid), linewidth=1.1) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_color_manual(values = cbbPalette) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Risk Differences') 
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr, nrow=2, common.legend = T)
    
d_gg_1
```

So the PLR model using a simple polynomial can reasonably approximate the KM estimate. This should work because the underlying synthetic data was generated with an exponential distribution for time, but real-world data will not play so nicely. There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider the following regarding the parametric time trend:

1.  It may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.

2.  The non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.

3.  These initial steps will not guarantee a good fit after weighting is applied, it is only a first-look for diagnostic purposes.

## Comparison of Covariate-adjusted PLR with Cox model

Another approach is to compare a pooled logistic regression analysis to a time-invariant Cox model. If time-varying confounding not an issue, this should give a similar result. However, censoring weights are still necessary in final analysis even if no confounding.[@cain2010]

```{r }
#| label: glm-plr-covadj
 
# defined above
  d_glm = glm(event==0 ~ poly(time, 2, raw=T)*assign + female + age + X, # <1>
              data=dta_c_panel, family=binomial()) # <1>

  dta_c_panel$p.noevent0 <- predict(d_glm, mutate(dta_c_panel, assign=0), type="response") # <2>
  dta_c_panel$p.noevent1 <- predict(d_glm, mutate(dta_c_panel, assign=1), type="response") # <2>
  
  setDT(dta_c_panel)
    dta_c_panel[, `:=`(pr_surv_1 = cumprod(p.noevent1)), by=list(id, assign)] # <3>
    dta_c_panel[, `:=`(pr_surv_0 = cumprod(p.noevent0)), by=list(id, assign)] # <3>
  setDF(dta_c_panel)
  
  d_plr_adj_est = dta_c_panel %>% # <4>
    group_by(time) %>% # <4>
    summarize(pr_ev_1 = mean(1-pr_surv_1),
              pr_ev_0 = mean(1-pr_surv_0), 
              .groups = 'drop') %>% # <4>
    ungroup %>% # <4>
    mutate(cid = pr_ev_1 - pr_ev_0, # <4>
           cir = pr_ev_1 / pr_ev_0) # <4>
```

1.  PLR model with some covariates for adjustment. (model fit for each treatment group)
2.  Predict outcome under each assignment strategy.
3.  Estimate cumulative survival using cumulative product, within person-clone
4.  Summarize mean survival rates by time-period

::: callout-note
Note the outcome regression without weights is just for comparison and understanding, the final analysis must include probability weights for artificial censoring even if no confounding.
:::

```{r }
#| label: time-dependent-cox
#| message: FALSE
#| warning: FALSE
  d_cox = coxph(Surv(enter, exit, event) ~ assign + female + age + X, data = dta_c_panel) # <1>

  d_cox_est = surv_adjustedcurves(fit = d_cox, variable = "assign", data = dta_c_panel) %>% # <2>
      group_by(variable, time) %>% # <3>
        dplyr::filter(row_number()==1) %>% # <3>
      ungroup %>% # <3>
      mutate(pr_ev = 1 - surv) %>% # <3>
      pivot_wider(., id_cols =c('time'), # <3>
                  names_from = variable, # <3> 
                  names_prefix = 'pr_ev_', # <3>
                  values_from = pr_ev # <3> 
      ) %>% # <3>
      mutate(cid = pr_ev_1 - pr_ev_0, # <3> 
             cir = pr_ev_1 / pr_ev_0) # <3> 
```

For comparison, A time-dependent Cox model is fit.

1.  Cox model, with time1, time2 parameters which represent the start and stop time of each interval. Note that althought the cox model is time-dependent and you could adjust for `X_t` this could be problematic, because X_t also includes time-points post treatment. In our synthetic data, X_t is not a collider (Treat -> X_t <- Outcome) or on the causal path (Treat -> X_t -> Outcome). 

2.  Estimation of adjusted survival curves by treatment group, with subpopulations balanced using `conditional` method.[@acompar]
3.  Summarizing survival probabilities by time

```{r }
#| label: gg-cmp-outcadj-models
#| echo: FALSE
#| fig-label: Time-dependent Cox model versus Covariate-adjusted PLR (no weights)
#| fig-cap-location: top

d_ci_cmp = bind_rows(
  select(d_plr_adj_est, time, pr_ev_0, pr_ev_1, cid) %>%
    mutate(Model = 'PLR-OM-Adj.'),
  select(d_cox_est, time, pr_ev_1, pr_ev_0, cid) %>%
    mutate(Model = 'Cox-Adj')
  ) %>%
  pivot_longer(cols = c(pr_ev_0, pr_ev_1), names_to = 'assign', names_prefix = 'pr_ev_', values_to = 'pr_ev')

d_gg_ci = d_ci_cmp %>%
      ggplot(aes(x=time, color = Model, Group = Model)) +
      geom_line(aes(y = pr_ev, linetype=assign), linewidth=0.9) +
      geom_point(aes(y = pr_ev, shape=assign), size=1.1) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_color_manual(values = cbbPalette) +
      theme_bw() +
      labs(x = '', y = 'Cumulative incidence') 
    
 d_gg_rr = d_ci_cmp %>%
      ggplot(aes(x=time, color = Model)) +
      geom_line(aes(y = cid), linewidth=1.1) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      scale_y_continuous(limits = c(-0.1, 0.1)) +
      scale_color_manual(values = cbbPalette) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Risk Differences') 
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr, nrow=2, common.legend = T)
    
d_gg_1
```

The PLR and Cox models both appropriately find no difference in risk by treatment assignment.

# IPCW Pooled Logistic Regression {#ipcwplr}  

Once the basic procedures are setup and there is some confidence in the ability to model the cumulative incidences, weighting can begin.

::: callout-note
There is no consideration here to model training, causal DAGs or covariate selection. It is simply a toy example to show the procedure.
:::

## Build a longitudinal panel

The person- or clone-level data must be expanded to a longitudinal panel where each observation (row) is a period of follow-up.

### Censoring dataset

In our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is specific to a project's definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we can use the clones assigned to no treatment across all follow-up timepoints.

1.  This may be a little confusing, but I am taking clones where assign=0 from the `dta_c_panel` dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.
2.  We are estimating time to treatment, not outcome. The clone, assign=0 is already set up for this but for other projects this step will have to be modified if the comparator is different. 

## Estimation of weights

The probability weight, AKA inverse probability censoring weight (IPCW) or inverse probability of no loss to follow-up is estimated in this way:

$$
W^C_i = \prod^{T}_{t_0} \frac{P(C_t = 0| \overline{C}_{t-1} = 0)}{P(C_t=0 | X=x, \overline{L}_{t-1}=l_{i, t-1}, \overline{C}_{t-1}=0)}
$$

C in this case means censoring, but censoring occurs according to treatment strategy, so it really is the probability of adhering to the treatment you were assigned to. Stabilized weights are tricky, here we are using the marginal stabilized weight which is the probability of no censoring at each timepoint. The denominator is the conditional probability accounting for fixed, i.e. baseline, (X) and time-varying (L) covariates.

We fit a model with the outcome of censoring, including the key variables. Then we add fitted probabilities back to the outcome dataset, and calculate cumulative probabilities and weights.

```{r }
#| label: est-censor-prob
#| warning: FALSE
  
  # Numerator (margin probability)
  d_glm_wt = glm(treat ~ poly(time, 2, raw=T), # <1>
                 data=dta_c_panel[dta_c_panel$assign==0, ], family=binomial()) # <1>
  
  dta_c_panel$pr_censnum = predict(d_glm_wt, newdata = dta_c_panel, type='response') # <1>

  # Denominator 
  d_glm_wt = glm(treat ~ poly(time, 2, raw=T) + X + X_t, # <2>
                 data=dta_c_panel[dta_c_panel$assign==0, ], family=binomial()) # <2>
  
  dta_c_panel$pr_censdenom = predict(d_glm_wt, newdata = dta_c_panel, type='response') # <2>
```

1.  Marginal probability of no censoring. 
2.  Conditional probability (covariate-adjusted) for no censoring

::: callout-note
There is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of "censoring" where that may mean treatment initiation or some other thing.[@gaber2024]
:::

## Calculation of weights

This step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to the [Weighting](05_appendix.qmd) section for further discussion.

```{r }
#| label: cpt-weights
  setDT(dta_c_panel)
  
  dta_c_panel[, ipw := fcase(
    assign==0 & censor==0, 1 / (1-pr_censdenom), # <1>
    assign==0 & censor==1, 0,
    assign==1 & time < 12, 1, # <2>
    assign==1 & time == 12  & t_treat  < 12, 1, # <3> 
    assign==1 & time == 12  & t_treat  ==12 & censor==0, 1 / (pr_censdenom), # <4> 
    assign==1 & time == 12  & t_treat  >12 & event==0, 0, # <5> 
    assign==1 & time == 12  & t_treat  >12 & event==1, 1, # <6> 
    assign==1 & time > 12, 1 # <7>
  )]
  
  dta_c_panel[, marg_ipw := fcase( # <8>
    assign==0 & censor==0, (1-pr_censnum) / (1-pr_censdenom), # <8>
    assign==1 & time == 12 & t_treat   ==12 & censor==0, pr_censnum / (pr_censdenom), # <8>
    default = ipw # <8>
  )]
  
  dta_c_panel[, `:=`(ipw = cumprod(ipw), # <9>
                     marg_ipw = cumprod(marg_ipw)), # <9>
               by=list(id, assign)] # <9>
```

1.  (assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored
2.  (assign=1) Clones cannot artificially censor prior to grace period
3.  (assign=1) If treatment started prior to grace window ending, the clone cannot censor
4.  (assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).
5.  (assign=1) If a clone is not treated, and does not die (`event=1`) they censor at the end of the window
6. If died in last interval (`event=1`), do not set weight to zero. This is a misstep others have done, if the person dies at the end of the interval, then that is still consistent with treatment strategy (i.e. they died before end of grace window when they were supposed to recieve treatment) so they are not censor=1 and death is counted.
7. (assign=1) Set post-grace period weights = 1 for all.
8.  For a marginal IPW, numerator of 1 is replaced with marginal probability of censoring at each tiempoint.
9.  After setting these conditions, compute the cumulative product of the weights.

### Overall IPW Distribution  

#### Unstabilized weights  

```{r }
#| label: desc-ipw
#| echo: false
  summary(dta_c_panel$ipw[dta_c_panel$ipw!=0])
```

#### Marginal Stabilized weights  

```{r }
#| label: desc-marg_ipw
#| echo: false
  summary(dta_c_panel$marg_ipw[dta_c_panel$marg_ipw!=0])
```

The unstabilized weights floor at 1, and we see high weights assigned to some. The marginal weights have a mean of 1 (expected). 

### IPW Distribution at end of grace period for treatment clones

The weights at the end of the grace period are key so its good to examine them directly: 

```{r }
#| label: desc-ipw-grace
#| echo: false
  summary(dta_c_panel$ipw[dta_c_panel$ipw!=0 & dta_c_panel$assign==1 & dta_c_panel$time==12])
```

::: callout-note
Be very careful with IPW truncation with this design. If the probability of treatment is very low, then those treated at the end of the grace period will have very large weights. If you truncate at 99% for example, it could mostly just truncate the weights of those treated at the end of the grace period, and this could severely bias estimates. These persons are meant to account for all those being artificially censored in the treatment group for non-treatment and so will likely have large weights by design. 
:::

You can also plot weights across time for diagnostics:

```{r }
#| label: desc-ipw-time
#| fig-cap: Distribution of Censoring Weights Across Time
#| fig-cap-location: top

dta_c_panel %>%
    dplyr::filter(marg_ipw!=0) %>%
 ggplot(., aes(x = cut_width(time, 12), y = marg_ipw)) +
  geom_violin(aes(group = cut_width(time, 12)), 
               scale = "width", fill = cbbPalette[1], alpha = 0.5) +
  stat_summary(fun = "mean",
               geom = "point",
               color = cbbPalette[2], size=2) +
  geom_hline(aes(yintercept=1),linewidth=1.1, linetype=3) +
  scale_x_discrete(labels = seq(0, 60, 12)) +
  labs(x = "Follow-up", y = "IPW", ) +
  theme_bw()
```

Another way to examine the weights is to look at the weighted counts of individuals at risk, and number of events pre- and post-weighting:

```{r }
#| label: weight-table
#| tbl-cap: Unweighted and Weighted Counts, N at risk
#| tbl-cap-location: top
dta_c_panel %>%
  group_by(time, assign) %>%
  summarize(n = sum(ipw!=0), 
            n_wt = sum(ipw),
            .groups = 'drop') %>%
  pivot_wider(id_cols = time, names_from = assign, values_from = c(n, n_wt)) %>%
  dplyr::filter(time %in% c(1, 12, 30, 60)) %>%
  rename(`Time` = time,
         `Unweighted, assign=0` = n_0,
         `Unweighted, assign=1` = n_1,
         `Weighted, assign=0` = n_wt_0,
         `Weighted, assign=1` = n_wt_1,
         ) %>%
  kable(align='c', digits =0) %>%
  kable_styling()
```

The size of the unweighted, and weighted counts may identify problems. 

Now with the estimated weights, it is simple to generate weighted cumulative incidences:

```{r }
#| label: est-plr-wt
#| warning: FALSE  
  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), # <1>
                   data=dta_c_panel[assign==1, ], # <1>
                   family=binomial(), weights = ipw) # <1>
  # <1>
  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), # <1>
                   data=dta_c_panel[assign==0, ], # <1>
                     family=binomial(), weights = ipw) # <1>
  
  dta_c_panel$pr_1 = predict(d_glm_pe_1, newdata = dta_c_panel, # <2>
                             type='response') # <2>
  dta_c_panel$pr_0 = predict(d_glm_pe_0, newdata = dta_c_panel, # <2>
                             type='response') # <2>
```

1.  Estimate for each assignment group separately. This isn't necessary but its good practice, and if you are adjusting for other covariates may give different results. Note the `weights = ipw` argument. R `glm()` will generate a warning message because the weighted counts are "non-integer", but this is expected and not a problem.
2.  Fitted probabilities from each model

```{r }
#| label: survprob
#| 
  dta_c_panel[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] # <1>
  dta_c_panel[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] # <1>
  
  d_plrwt_est = dta_c_panel %>% # <2>
    group_by(time) %>% # <2>
    summarize(pr_ev_1 = mean(1-pr_cum_1),
              pr_ev_0 = mean(1-pr_cum_0), 
              .groups = 'drop') %>% # <2>
    ungroup %>% # <2>
    mutate(cid = pr_ev_1 - pr_ev_0, # <2>
           cir = pr_ev_1 / pr_ev_0) # <2>
```

1.  Cumulative product
2.  Summarize across group, time as before.

```{r }
#| label: gg-plr-weighted
#| fig-cap: Weighted PLR Analysis
#| fig-cap-location: top
#| echo: false
#| eval: false
    d_gg_ci = d_plrwt_est %>%
      pivot_longer(cols = c(pr_ev_0, pr_ev_1), names_to = 'assign', values_to = 'pr_ev') %>%
      ggplot(aes(x=time, y = pr_ev, color = assign)) +
      geom_line(aes(linetype=assign), linewidth=1.2) +
      scale_color_manual(labels = c('Assign=0', 'Assign=1'), values = cbbPalette) +
      scale_linetype_manual(labels = c('Assign=0', 'Assign=1'), values = c(1, 2)) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Cumulative incidence')
    
    d_gg_ci
```

# Final Comparison

```{r }
#| label: tbl-compare-est
#| tbl-cap-location: top
#| tbl-cap: 'Comparison of estimates by model, cumulative incidences, risk differences and relative risks'
#| warning: false
#| message: false
#| echo: false

  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), 
                     data=dta_c_panel[assign==1, ], 
                     family=binomial(), weights = marg_ipw) 

  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), 
                   data=dta_c_panel[assign==0, ], 
                     family=binomial(), weights = marg_ipw) 
  
  dta_c_panel$pr_1 = predict(d_glm_pe_1, newdata = dta_c_panel, 
                             type='response') 
  dta_c_panel$pr_0 = predict(d_glm_pe_0, newdata = dta_c_panel, 
                             type='response') 

  dta_c_panel[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] 
  dta_c_panel[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] 
  
  d_plrstabwt_est = dta_c_panel %>% 
    group_by(time) %>% 
    summarize(pr_ev_1 = mean(1-pr_cum_1),
              pr_ev_0 = mean(1-pr_cum_0), 
              .groups = 'drop') %>% 
    ungroup %>% # <2>
    mutate(cid = pr_ev_1 - pr_ev_0, 
           cir = pr_ev_1 / pr_ev_0) 
  
 d_all_est = 
  bind_rows(
    select(d_km_est, time, pr_ev_0, pr_ev_1, cid, cir) %>%
        mutate(Model = 'Kaplan-Meier'),
    select(d_plr_naive_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'PLR-Naive'),
    select(d_cox_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'Cox'),
    select(d_plrwt_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'PLR-Wtd'),
    select(d_plrstabwt_est, time, pr_ev_1, pr_ev_0, cid, cir) %>%
      mutate(Model = 'PLR-Stab-Wtd'),
    
  ) %>%
  dplyr::filter(time %in% c(6, 12, 36, 60))

kables(
  list(d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = pr_ev_0) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Incidence: No treatment initiation ever" = 6)),
       d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = pr_ev_1) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Incidence: Treatment initiation at 12 months" = 6)),
       d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = cid) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Risk Differences" = 6)),
       d_all_est %>%
         pivot_wider(., id_cols = time, names_from = Model, values_from = cir) %>%
         kable(digits=3, align = 'c') %>%
         kable_styling() %>%
         add_header_above(., c("Relative Risks" = 6))
))

```
Note. The IPW-PLR finds a null effect, and similar results to a time-depedent Cox model. PLR = Pooled Logistic Regression, Wtd = with censoring weights, Stab = Marginal Stabilized Weights.

# References

::: {#refs}
:::
