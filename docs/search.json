[
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "Version 0.0.1 02-19-2025",
    "section": "",
    "text": "Version 0.0.1 02-19-2025\nProject set-up"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#versions",
    "href": "about.html#versions",
    "title": "About",
    "section": "Versions",
    "text": "Versions\n\nVersion 1.0, established 04.24.2024"
  },
  {
    "objectID": "04_advanced.html",
    "href": "04_advanced.html",
    "title": "Additional Topics",
    "section": "",
    "text": "In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort, or comparison of those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals) but will provide a standardized mean difference.\nSome special considerations are needed when presenting target trial emulations.\n\n\nThe cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)"
  },
  {
    "objectID": "04_advanced.html#cloning-grace-period",
    "href": "04_advanced.html#cloning-grace-period",
    "title": "Additional Topics",
    "section": "",
    "text": "The cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)"
  },
  {
    "objectID": "04_advanced.html#evaluate-of-grace-period",
    "href": "04_advanced.html#evaluate-of-grace-period",
    "title": "Additional Topics",
    "section": "Evaluate of Grace Period",
    "text": "Evaluate of Grace Period"
  },
  {
    "objectID": "04_advanced.html#weighting-distributions",
    "href": "04_advanced.html#weighting-distributions",
    "title": "Additional Topics",
    "section": "Weighting distributions",
    "text": "Weighting distributions"
  },
  {
    "objectID": "04_advanced.html#covariate-balance-across-pre--post-weighting-during-follow-up",
    "href": "04_advanced.html#covariate-balance-across-pre--post-weighting-during-follow-up",
    "title": "Additional Topics",
    "section": "Covariate balance across pre-, post-weighting during follow-up",
    "text": "Covariate balance across pre-, post-weighting during follow-up"
  },
  {
    "objectID": "04_advanced.html#benchmarked-estimation-step",
    "href": "04_advanced.html#benchmarked-estimation-step",
    "title": "Additional Topics",
    "section": "Benchmarked Estimation Step",
    "text": "Benchmarked Estimation Step\n\nglm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n\n\nSpeeding up GLM ML procedure\nThere are two main things that can be done to speed up GLM, 1) initialize parameters based on prior estimation procedure. 2) Use efficient matrix functions and parallel computation.\n\nInitialization hack\nThis is a simple trick, either 1) run the GLM once and store est, or 2) run the GLM on a 10% sample.\n\n1d_fit = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n\n2d_fit_2 = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(), start = d_fit$coefficients)\n\n\n1\n\nGLM procedure with automatic initialization step.\n\n2\n\nGLM with start= option using coefficients from prior step.\n\n\n\n\n\n\nBLAS/LAPACK and Parallel computation\nThe parglm package provides much faster computations (but somewhat more unstable).\n\nlibrary(parglm)\n\nd_fit_3 = parglm(event_outc ~ poly(time, 2, raw=T)*assign, \n       binomial(), d_panel_outc, start = d_fit$coefficients,\n1       control = parglm.control(method = \"FAST\", nthreads = 8L))\n\n\n1\n\nparglm() function works mostly as glm(), the parglm.control() allows some additional options for parallel computing and QR decomposition.\n\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n           expr       min        lq      mean    median        uq       max\n       base GLM 1.7624406 1.9701256 2.0263911 2.0299800 2.1186653 2.1986339\n GLM with inits 0.6866930 0.7590143 0.8318813 0.8294715 0.9310696 0.9555727\n         PARGLM 0.4115152 0.4582455 0.5247244 0.5347072 0.5899022 0.5944830\n neval\n    10\n    10\n    10\n\n\nEven in a small example, parglm() outperforms 4x from base GLM. This will scale considerably with multiple cores and larger datasets as well."
  },
  {
    "objectID": "04_advanced.html#bootstrapping-procedure",
    "href": "04_advanced.html#bootstrapping-procedure",
    "title": "Additional Topics",
    "section": "Bootstrapping procedure",
    "text": "Bootstrapping procedure\nI consider bootstrapping to be the standard for this type of analysis, the statistical properties are not well-described, but some use influence-based statistics.\nThe typical bootstrap procedure resamples an entire dataset iteratively, but this can be very inefficient depending on how you set it up because it may involve holding the original dataset, and another new bootstrapped dataset in memory, also potentially a matrix in a regression optimization step. However some clever use of weighting can work around this.\n\n\n\n\n\n\nNote\n\n\n\nIn any case, the bootstrap procedure must sample at the person level to account for the cloning.\n\n\n\nInefficient Bootstrap\n\nboot_it_1 = function() {\n  \n1  d_ids = distinct(d_panel_outc, id)\n  \n  d_boot = slice_sample(d_ids, prop=1, replace=T)\n  \n2  d_panel_outc_2 = left_join(d_boot,\n                             d_panel_outc, by = join_by(id),\n3                             relationship = \"many-to-many\")\n  \n  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc_2, family=binomial())\n  \n  d_panel_outc_2$pr_ev = d_glm_pe$fitted.values \n  d_panel_outc_2[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] \n  d_res = d_panel_outc_2 %&gt;% \n    group_by(assign, time) %&gt;% \n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;% \n    ungroup %&gt;% \n    pivot_wider(., id_cols =c('time'),  \n                names_from = assign,  \n                names_prefix = 'pr_ev_', \n                values_from = pr_ev \n    ) %&gt;% \n    mutate(cid = pr_ev_1 - pr_ev_0,  \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a list of unique person IDs\n\n2\n\nSample from list with replacement\n\n3\n\nPerform left-join on resampled list back to dataset\n\n\n\n\n\n\nMore efficient bootstrap\n\nboot_it_2 = function() {\n  \n1  d_panel_outc[, freqwt:=rpois(n=1, lambda=1), by = factor(id)]\n\n  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(),\n                 weights = freqwt)\n  \n  d_panel_outc$pr_ev = d_glm_pe$fitted.values \n  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] \n  d_res = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;% \n    ungroup %&gt;% \n    pivot_wider(., id_cols =c('time'),  \n                names_from = assign,  \n                names_prefix = 'pr_ev_', \n                values_from = pr_ev \n    ) %&gt;% \n    mutate(cid = pr_ev_1 - pr_ev_0,  \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a frequency weight by group ID from a Poisson distribution with mean 1\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n                 expr      min       lq     mean   median       uq      max\n Resampling bootstrap 1.750832 1.869949 1.952947 1.931778 1.964195 2.314062\n    Poisson bootstrap 1.579040 1.595557 1.747776 1.612970 1.946814 2.294240\n neval\n    10\n    10\n\n\nThe Poisson bootstrap procedure is 20-30%, but critically this performance will scale much better for large datasets.\nIf you don’t believe this provides similar coverage estimates, here are the intervals from 50 bootstraps with both procedures:\n\n\nResampling method: \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -0.091  -0.057  -0.050  -0.052  -0.046  -0.026 \n\n\nPoisson method:\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -0.069  -0.057  -0.048  -0.048  -0.041  -0.017"
  },
  {
    "objectID": "02_est.html",
    "href": "02_est.html",
    "title": "Estimation",
    "section": "",
    "text": "I recommend the following stepwise procedure in estimation:\n\nPLR\n\nPooled Logistic Regression, a model which approximates the hazards by discretizing follow-up time. KM\n\n\nKaplan-Meier, Product-Limit, non-parametric estimator which computes the instantaneous hazard at points of follow-up. Does not allow for time-varying confounding, but the non-parametric evaluation of time-trends can be useful for diagnostics and model specification of the PLR.\n\n\n\nEstimate unweighted/unadjusted cumulative incidences with KM method\nEstimate a PLR model without weights\n\n\n\n\nCompare KM vs PLR\n\n\n\nEstimate censoring weights\n\n\n\nEstimate KM model for treatment initiation\nEstimate PLR for treatment initiation A. compare KM / PLR estimates\nestimate final treatment model A. Join treatment probabilities to outcome data B. compute IPCW\nexamine IPCW weights for extreme values, non-sensical values\nexamine balance in covariates across time\ngenerate “table 1” with some weighted difference statistic (e.g. wSMD)\n\n\n\nEstimate weighted outcome models\n\n\n\nEstimate a weighted outcome model, no covariate adjustment\nEstimate a weighted outcome + covariates (main estimate)\n\n\n\nOnce satisfied with stability from steps 1-4, execute bootstraps\nIn report, provide KM, unweighted, PLR estimates in an appendix; main results reported should be the IPCW-weighted PLR analysis with or without covariate adjustment in the outcome model (project specific decision).\n\n\n\n\n\n\n\nNote\n\n\n\nPooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of Causal Inference: What If, which explains why the odds approximates the hazard at a given time-point k, as long as the hazard is small (i.e. rare event) at time k."
  },
  {
    "objectID": "02_est.html#data",
    "href": "02_est.html#data",
    "title": "Estimation",
    "section": "Data",
    "text": "Data\nWe continue the below using the cloned dataset generated in Data.\n\nglimpse(d_cloned)\n\nRows: 20,000\nColumns: 12\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ assign     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t_clone    &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 4, 60, 2, 60, 0, 2, 60, 6, …\n$ event_outc &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ time       &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 60, 60, 60, 60, 60, 30, 60,…\n$ t_artcens  &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ treat      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,…\n$ age        &lt;dbl&gt; 88.7, 69.4, 78.6, 81.3, 79.0, 73.9, 90.1, 74.1, 95.2, 74.4,…\n$ female     &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ dx_1       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ dx_2       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,…"
  },
  {
    "objectID": "02_est.html#estimation",
    "href": "02_est.html#estimation",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\n\n  d_km_est = broom::tidy(\n1    survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n2    mutate(pr_ev = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n3    pivot_wider(id_cols = c(time),\n                names_from = assign,\n                values_from = c(pr_ev, pr_s,\n                                n.risk, n.event)) %&gt;%\n4    mutate(cir = pr_ev_1 / pr_ev_0,\n           cid = (pr_ev_1 - pr_ev_0))\n\n\n1\n\nNaive estimator, K-M estimator, assignment is by clone (not person). t_clone is the follow-up time for each clone, taking into account artificial censoring.\n\n2\n\nestimate from the model is the survival probability, so probability of event is 1 - estimate\n\n3\n\nData is in long form, so one colum per group with cumulative incidence/survival\n\n4\n\nEstimands, cir is ratio analogous to relative risk, and cid is analogous to risk difference"
  },
  {
    "objectID": "02_est.html#summarize-km-estimator",
    "href": "02_est.html#summarize-km-estimator",
    "title": "Estimation",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator\n\n\n\n\n\n\n\n\n\nLater I will show how to summarize point estimates and confidence intervals with bootstrapping."
  },
  {
    "objectID": "02_est.html#build-a-longitudinal-panel",
    "href": "02_est.html#build-a-longitudinal-panel",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nThe PLR model is a person-time procedure, modeling the probability of the event where the observation is person at follow-up time k. The KM estimator is a person-level dataset, so the dataset much first be expanded to person-time units. If there are many persons, or many observations (timepoints) per person the dataset can become quite large. I personally use the data.table package to help with computation. It is blazingly fast, so the code reflects that approach which may not be needed for your project.\n\n1    setDT(d_cloned)\n2    d_panel = d_cloned[rep(seq(.N), t_clone)]\n3    d_panel[, exit := (seq_len(.N)), by = list(id, assign)]\n    d_panel[, enter := exit-1]\n4    d_panel[, time := seq_len(.N), by = list(id, assign)]\n5    d_panel[, event_outc := if_else( t_clone &lt;= time, event_outc, 0L), by = list(id, assign)]\n    d_panel_outc = select(d_panel, id, time, event_outc, t_treat, assign, enter, exit) \n\n\n1\n\nConvert data to a DT object for faster computations\n\n2\n\nGenerate rows along the sequence of .N (number of time-points) up to the last follow-up point\n\n3\n\nFor each row, compute the starting (enter) and ending time-point (exit). This isn’t really necessary for our toy example, but could be important depending on how you are setting up the longitudinal panel and planning to model the time-trend.\n\n4\n\nThis time is the key variable, a count from 1 to the last observed follow-up point by person, clone\n\n5\n\nOutcome is = 1 in row where event occurred, so in the data a person-clone should have values of zero for event_outc up until the last observed time-point where event_outc=1 IF the event occurred at that time.\n\n\n\n\n\nglimpse(d_panel_outc)\n\nRows: 554,458\nColumns: 7\n$ id         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ time       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ event_outc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf,…\n$ assign     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ enter      &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ exit       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how the dataset has expanded from N=20000 to N=554k! This method is very memory hungry…"
  },
  {
    "objectID": "02_est.html#estimation-1",
    "href": "02_est.html#estimation-1",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\nThe PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time^2). You can choose to either estimate the outcome in a model with both clones combined in one dataset OR you can estimate cumulative incidences separately (two models with data limited to assign==1 & assign==0 respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. time + time^2 + treat + treat*time + treat*time^2, shorthand below is poly(time, 2, raw=T)*assign.\n\n  # defined above\n1  d_glm = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n  \n2  d_panel_outc$pr_ev = d_glm$fitted.values\n3  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)]\n  \n4  d_plr_naive_est = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    pivot_wider(., id_cols =c('time'),\n                names_from = assign,\n                names_prefix = 'pr_ev_',\n                values_from = pr_ev\n    ) %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nPLR model, with time*treat interaction. Binomial family for logistic regression.\n\n2\n\nIf event_outc = 1 when event occurs, the fitted values are the probability of the outcome at time k.\n\n3\n\nTo describe cumulative probabilities, you take the cumulative product of the fitted probabilities pr_ev, making sure to only compute within group and clone.\n\n4\n\nAfter you have cumulative event-free survival probability, then you can summarize by group/time as described above for the KM estimator.\n\n\n\n\n\n    d_gg_ci = d_plr_naive_est %&gt;%\n      ggplot(aes(x=time)) +\n      geom_line(aes(y = pr_ev_0), color='red') +\n      geom_line(aes(y = pr_ev_1), color='blue') + \n      scale_x_continuous(breaks = seq(0, 60, 6),\n                         limits = c(0, 60)) +\n      theme_bw() +\n      labs(x = 'Follow-up', y = 'Cumulative incidence')\n    \n    d_gg_ci\n\n\n\n\n\n\n\n\nAt first glance, plot looks reasonable. We now directly compare PLR to the KM estimator by plotting together to see if the modeling appears to be working:\n\n\n\n\n\n\n\n\n\nSo the PLR model using a simple polynomial can approximate our KM estimate reasonably well (This should work because we used synthetic data generated with an exponential distribution for time, but real-world data will not play so nicely). There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider following points:\n\nIt may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.\nThe non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.\nThese initial steps will not guarantee a good fit after weighting is applied, it is only a beginning."
  },
  {
    "objectID": "02_est.html#build-a-longitudinal-panel-1",
    "href": "02_est.html#build-a-longitudinal-panel-1",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nAs before the person- or clone-level data must be expanded to a longitudinal panel by time. Here, two datasets must be constructed, one for estimation of censoring probabilities and one for estimation of outcome probabilities.\n\nCensoring dataset\nIn our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is very specific to a project’s definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we generate a dataset that is at the person-level (no cloning):\n\nd_treat = d_cloned %&gt;%\n1    dplyr::filter(assign==0) %&gt;%\n    mutate(start=1,\n2           end = t_treat) %&gt;%\n3    group_by(id) %&gt;%\n    mutate(\n      time = t_clone,\n      outcome = if_else(time==t_treat, 1, 0)\n    ) %&gt;%\n    ungroup\n\n\n1\n\nThis may be a little confusing, but I am taking clones where assign=0 from the d_cloned dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.\n\n2\n\nWe are estimating time to treatment, not outcome\n\n3\n\nArtificial censoring time is same as treatment time unless dead first, and outcome = 1 if treated.\n\n\n\n\nThen we expand the dataset as described above:\n\n  setDT(d_treat)\n  d_panel = d_treat[rep(seq(.N), time)]\n  d_panel[, exit := (seq_len(.N)), by = list(id)]\n  d_panel[, enter := exit-1]\n  d_panel[, time := seq_len(.N), by = list(id)]\n  d_panel[, event_treat := if_else(t_treat &lt;= time, treat, 0L), by = list(id)]\n1  d_panel_treat = select(d_panel, id, time, event_treat, t_treat, enter, exit, end, age, female, dx_1, dx_2)\n\n\n1\n\nNote we keep covariates for estimation of censoring weights."
  },
  {
    "objectID": "02_est.html#estimation-2",
    "href": "02_est.html#estimation-2",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\nWe fit a model with the outcome of censoring, including the variables which are key. Then we add fitted probabilities back to the dataset, calculate cumulative probabilities of remaining uncensoring, and join those estimated probabilities to the outcome dataset.\n\n  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T) + poly(age, 2) + female + \n1                   dx_1 + dx_2, data=d_panel_treat, family=binomial())\n2  d_panel_treat$pr_treat = d_glm_wt$fitted.values\n  setDT(d_panel_treat)\n3  d_panel_treat[, cumpr_notreat := cumprod(1-pr_treat), by = .(id)]\n  \n4  d_panel_treat = select(d_panel_treat, id, time, cumpr_notreat)\n  d_panel_outc = left_join(d_panel_outc, d_panel_treat, by=c('id', 'time'))\n\n\n1\n\nEstimation of probability of censoring in PLR\n\n2\n\nAdd fitted probabilities to dataset\n\n3\n\nCalculate cumulative probability of non-treatment (treatment-free survival)\n\n4\n\nJoin probabilities back to outcome dataset\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI think there is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of “censoring” where that may mean treatment initiation or some other thing.(Gaber et al. 2024)"
  },
  {
    "objectID": "02_est.html#calculation-of-weights",
    "href": "02_est.html#calculation-of-weights",
    "title": "Estimation",
    "section": "Calculation of weights",
    "text": "Calculation of weights\nThis step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to Weighting for further discussion.\n\n  setDT(d_panel_outc)\n  \n  d_panel_outc[, ipw := fcase(\n1    assign==0, 1 / cumpr_notreat,\n2    assign==1 & time &lt; 12, 1,\n3    assign==1 & time == 12 & t_treat &lt; 12, 1,\n4    assign==1 & time==12   & t_treat   ==12, 1 / (1-cumpr_notreat),\n5    assign==1 & time==12   & t_treat    &gt;12, 0,\n6    assign==1 & time &gt; 12, 1\n  )]\n7  d_panel_outc[assign==1, ipw := cumprod(ipw), by=list(id, assign)]\n\n\n1\n\n(assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored\n\n2\n\n(assign=1) Clones cannot artifically censor prior to grace\n\n3\n\n(assign=1) If treatment started prior to grace window ending, the clone cannot censor\n\n4\n\n(assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).\n\n5\n\n(assign=1) If a clone is not treated they censor at the end of the window\n\n6\n\n(assign=1) In periods greater than the end of the grace window, no artificial censoring occurs (they either are treated or already censored at this point).\n\n7\n\nAfter setting these conditions, we compute the cumulative product of the weights.\n\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   1.201   1.125   1.219   6.019 \n\n\nNow with the estimated weights, it is simple to generate weighted cumulative incidences:\n\n1  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(), weights = ipw)\n  \n2  d_panel_outc$pr_ev = d_glm_pe$fitted.values\n  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)]\n  d_lprwt_est = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    pivot_wider(., id_cols =c('time'),\n                names_from = assign,\n                names_prefix = 'pr_ev_',\n                values_from = pr_ev\n    ) %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nNote the weights = ipw argument, everything else is same as PLR model above. R glm() will generate a warning message because the weighted counts are “non-integer”, but this is expected and not a problem.\n\n2\n\nSummarize across group, time as before."
  },
  {
    "objectID": "01_syndata.v2.html",
    "href": "01_syndata.v2.html",
    "title": "Simulate data for CCW project",
    "section": "",
    "text": "Simulation Parameters\n\n1    n = 10000\n2    age = round(rnorm(n, mean = 75, sd = 10), 1)\n3    female = sample(c(1, 0), size = n, replace = TRUE, prob = c(0.66, 0.34))\n4    treat &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.2, 0.8))\n5    dx_1 &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.05, 0.95))\n6    dx_2 &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.4, 0.6))\n7    ef_treat = 0.8\n    ef_gender = 0.9\n    ef_age = 0.98\n    ef_dx_1 = 1.2\n    ef_dx_2 = 0.8\n\n\n1\n\nUse a starting sample size of a 1000 (i.e. persons) Generate covariates:\n\n2\n\nGenerate age (normal distribution with mean=75, std=10)\n\n3\n\nFemale, 66%/34%\n\n4\n\nTreatment, 20%/80%\n\n5\n\nA rare chronic condition dx_1 5%/95%\n\n6\n\nA prevalent chronic condition dx_2 40%/60%\n\n7\n\nAssign relative effect sizes for each\n\n\n\n\n\nef_treat = 0.8\nef_gender = 0.9\nef_age = 0.98 (per unit of age)\nef_dx_1 = 1.2\nef_dx_2 = 0.8\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…\n\n\n\n\nLinear Predictor\n\n# log-hazard\n    lp_outc &lt;- log(ef_treat) * treat +\n      log(ef_gender) * female +\n1      log(ef_age) * age + log(ef_dx_1)*dx_1 + log(ef_dx_2)*dx_2\n  \n  # Simulate baseline survival times (exponential distribution)\n2    baseline_hazard &lt;- 0.05\n\n\n1\n\nCompute the linear predictor,\n\n2\n\nSimulation will use a random exponential distribution with rate of 0.05 (baseline hazard). This was selected based on trial and error.\n\n\n\n\n\n\nSimulate Event Time\n\n1    baseline_survival &lt;- rexp(n, rate = baseline_hazard)\n    \n  # Adjust survival times based on linear predictor\n2    t_outc &lt;- round(baseline_survival * exp(-lp_outc))\n\n\n1\n\nSimulate survival times using exponential distribution\n\n2\n\nAdjust the times with linear predictor and round to nearest whole number.\n\n\n\n\n\n\nSimulate Treatment Start\n\n1    lp_treat = log(0.7) * female + log(1.02) * age\n2    baseline_hazard = 0.1\n3    t_treat = if_else(treat==1, round(rexp(n, rate = baseline_hazard)* exp(-lp_treat)), Inf)\n\n\n1\n\nSimulate treatment start times based on female and age. (for weighting model); assumed dx_1 and dx_2 unrelated to treatment.\n\n2\n\nUsed a different rate, so treatment happens early relative to outcome.\n\n3\n\nGenerate treatment times and adjust for linear predictor.\n\n\n\n\n\n\nFinalize dataset\n\n  # Create dataset\n    data &lt;- data.frame(\n      age = age,\n      female = female,\n      dx_1 = dx_1,\n      dx_2 = dx_2,\n      treat = treat, \n      t_treat = t_treat, \n      t_outc = t_outc \n    ) %&gt;%\n      mutate(id = row_number(),\n1             time = pmin(60, t_outc),\n             event_outc = if_else(time==t_outc, 1L, 0L))\n\n\n1\n\nSet administrative end of follow-up at 60. No other censoring mechanism, e.g. lost to follow-up.\n\n\n\n\n\n\nSummary of survival dataset\n\nglimpse(data)\n\nRows: 10,000\nColumns: 10\n$ age        &lt;dbl&gt; 88.7, 69.4, 78.6, 81.3, 79.0, 73.9, 90.1, 74.1, 95.2, 74.4,…\n$ female     &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ dx_1       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ dx_2       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,…\n$ treat      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,…\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ t_outc     &lt;dbl&gt; 61, 85, 75, 28, 47, 49, 301, 57, 111, 75, 111, 206, 72, 30,…\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ time       &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 60, 60, 60, 60, 60, 30, 60,…\n$ event_outc &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,…\n\n\n\nsummary(survfit(Surv(time, event_outc) ~ treat, data = data), times = c(0, 12, 30, 60))\n\nCall: survfit(formula = Surv(time, event_outc) ~ treat, data = data)\n\n                treat=0 \n time n.risk n.event survival  std.err lower 95% CI upper 95% CI\n    0   7924      33    0.996 0.000723        0.994        0.997\n   12   7125     844    0.889 0.003524        0.882        0.896\n   30   5956    1145    0.745 0.004897        0.735        0.754\n   60   4465    1475    0.559 0.005578        0.548        0.570\n\n                treat=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0   2076       6    0.997 0.00118        0.995        0.999\n   12   1882     200    0.901 0.00656        0.888        0.914\n   30   1635     244    0.783 0.00904        0.766        0.801\n   60   1286     347    0.616 0.01067        0.596        0.637\n\n\n\n\n\n\n\n\nNote\n\n\n\nTreat=1 means they EVER received treatment, so additional work needed to describe a treatment window etc.\n\n\n\n\nCloning procedure\nThe cloning procedure is very project specific, tied to how the treatment strategy is defined so it is hard to give general recommendations here. For this tutorial, we describe an arbitrary window in which treatment is expected to initiate and outline strategies around this:\nTreatment Strategies:  1) Do not ever receive treatment 2) Initiate treatment within 12 time periods (days, weeks etc.) and if not then treatment will initiate on week 12.\n\n\n\n\n\n\nNote\n\n\n\nThe grace window is a funny concept when you first consider it. This does not reflect any trial I have ever heard of actually being done but may identify an interesting or important effect. It is essentially outlying a treatment “protocol” and saying what if everyone adhered to this protocol counterfactual to what was observed.\n\n\nTo clone, you make two copies of the data, and make a new artifical censoring variable, which is a censoring time for the period when clones observed data are no longer consistent with their assigned strategy.\n\n1data_cloned = bind_rows(\n                     data %&gt;%\n                       mutate(assign = 0,\n                              t_artcens = if_else(t_treat &lt; time, t_treat, Inf)\n                       ),\n2                     data %&gt;%\n                       mutate(assign = 1,\n                              t_artcens = if_else(t_treat &gt; 12, 12, Inf)\n                              )\n                       ) %&gt;%\n3  mutate(t_clone = pmin(time, t_artcens),\n         event_outc = if_else(t_clone&lt;time, 0, event_outc))\n\n\n1\n\nClones assigned to strategy 1 (No treatment)\n\n2\n\nClones assigned to strategy 2 (grace window for treatment)\n\n3\n\nUpdate failure times and events counting artificial censoring\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI set failure times to Infinite when the event is unobserved, e.g. if no treatment, time to treatment is INF"
  },
  {
    "objectID": "03_inference.html",
    "href": "03_inference.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Inferential statistics\nSeveral steps in the analysis makes the assumptions of conventional standard errors invalid. The use of cloning or repeated use of persons across sequential target trial dates complicates the statistical properties of the estimand (due to correlated data), additionally the uncertainty in estimation of probability weights should be considered. Currently, most researchers are using bootstrapping to obtain confidence limits via percentile method. It is critical that 1) the bootstrapping step occur prior to sequential repeated sampling, cloning or estimation of weights, 2) the bootstrap sampling with replacement must be at the cluster-level (person). Also consider, bootstrapping may fail to produce valid estimates in cases of matching procedures, or restricted regression procedures (i.e. LASSO or other shrinkage estimators).(Campanovo 2015, Abadie and Imbens 2008) Other bootstrapping failures may arise due to limited sample size or values on the boundary of a parameter space. Note: Poisson Bootstrap procedure Especially for large datasets, the bootstrap procedure can be computationally intensive (requiring holding multiple large matrices in memory simultaneously). Rather than sampling rows of the matrix with replacement, an alternative is to approximate the sampling with a frequency weight. If you randomly assign every observation a value drawn from a Poisson distribution with mean 1, and use this value as a frequency weight in estimators you will closely approximate the full bootstrap procedure as long as the overall sample size is &gt;100. (Hanley and Macgibbon 2006) This is very computationally efficient because you do not need to know the dataset size prior to assigning the frequency weight, and do not need to sample large matrices.\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "05_appendix.html",
    "href": "05_appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Grace windows\n\n\nWeight assignments in CCW design"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "",
    "text": "Data; Demonstrates how to simulate a dataset for use in CCW along with the cloning process. A key process is how to generate a “cloned” dataset, refer here\nEstimation; Main section which walks through each estimation step.\nInference; Examples on how to obtain confidence intervals.\n\nAdditional Topics\nAppendix; some notes on causal inference, theory that may be helpful in adapting this tutorial to a specific project.\n\nFor production notes and future efforts see: About."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe tutorial represents my gathered and organized notes from research projects and didactic training. Collaborators and mentors include: Issa Dahabreh, Kaley Hayes, Daniel Harris, Donald Miller and Andrew Zullo."
  }
]