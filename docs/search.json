[
  {
    "objectID": "qmd/surv_02_disctime.html#time-to-event",
    "href": "qmd/surv_02_disctime.html#time-to-event",
    "title": "Parametric Discrete Time Analyses",
    "section": "Time-to-event",
    "text": "Time-to-event\nIt is assumed here that the basic measurement of the outcome includes a failure time (time at which follow-up ceases) and a binary indicator for whether the event occurred or not by that time point. The measure presumes a clear origin or time zero, and knowledge of the scale or unit of time (days, years etc.). In most applied work, the event can only occur once. For those with no observed event-time, meaning the event did not occur through the last-observable time point, the observed time is some other endpoint (death, administrative end, loss-to-follow-up), these non-events are referred to as “censored” meaning follow-up ceased at that time. The observed time for a person must be the minimum of event or censoring time.",
    "crumbs": [
      "Survival Analysis Background",
      "Discrete Time Estimators"
    ]
  },
  {
    "objectID": "qmd/surv_02_disctime.html#definitions-and-notation",
    "href": "qmd/surv_02_disctime.html#definitions-and-notation",
    "title": "Parametric Discrete Time Analyses",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\nThe outcome need not be death, but often is, and individuals must be alive to experience events, censoring etc. so most of the terminology and notation refer to “survival”.\nThe letter T/t is typically used to denote time, where T may refer to total time observed, and t an individual time-period, \\(T = {1, ..., t}\\). Since time is usually discretely defined, for example as days, then \\(t\\) can be thought of as an interval, \\(t=1\\) meaning from the end of the prior interval (day 0) through the next (but not including) interval, day 2. \\(T=t\\) would mean a event or censor time occurred in interval \\(t\\). A binary indicator for whether the event occured \\(E = {0, 1}\\).\nSurvival probability, \\(S(t) = Pr(T&gt;t)\\), is the probability of survival beyond time-point t.\nCumulative incidence, \\(R(t) = Pr(T&lt;=t)\\), is the 1-probability of survival, the probability the event occurred up to and including time-point t.\nHazard, \\(h(t) = Pr(E_t=1|E_{t-1}=0)\\), the probability of an event at time t, conditional on event-free survival up to that point, sometimes described as the “instantaneuous hazard”.\nCumulative Hazard, \\(H(t)\\)\nThe terms are related, \\(S(t) = 1 – R(t)\\). \\(h(t)=R(t)/S(t)\\), \\(H(t) = -log[S(t)]\\) and \\(S(t) = e –H(t)\\).",
    "crumbs": [
      "Survival Analysis Background",
      "Discrete Time Estimators"
    ]
  },
  {
    "objectID": "qmd/surv_02_disctime.html#tmle",
    "href": "qmd/surv_02_disctime.html#tmle",
    "title": "Parametric Discrete Time Analyses",
    "section": "TMLE",
    "text": "TMLE",
    "crumbs": [
      "Survival Analysis Background",
      "Discrete Time Estimators"
    ]
  },
  {
    "objectID": "qmd/surv_02_disctime.html#survival-rf",
    "href": "qmd/surv_02_disctime.html#survival-rf",
    "title": "Parametric Discrete Time Analyses",
    "section": "Survival RF",
    "text": "Survival RF",
    "crumbs": [
      "Survival Analysis Background",
      "Discrete Time Estimators"
    ]
  },
  {
    "objectID": "qmd/surv_00_summ.html",
    "href": "qmd/surv_00_summ.html",
    "title": "Exploration of Survival Analyses, Continuous and Discrete Time",
    "section": "",
    "text": "There are many references that will explain how the product-limit estimator works, and I do not revisit these basic topics. Any standard survival analysis course will cover this in detail, and countless books, youtube lectures are available to help readers learn. However, I demonstrate how to use this simple technique in the context of a CCW analysis.",
    "crumbs": [
      "Survival Analysis Background"
    ]
  },
  {
    "objectID": "qmd/surv_00_summ.html#survival-analyses",
    "href": "qmd/surv_00_summ.html#survival-analyses",
    "title": "Exploration of Survival Analyses, Continuous and Discrete Time",
    "section": "",
    "text": "There are many references that will explain how the product-limit estimator works, and I do not revisit these basic topics. Any standard survival analysis course will cover this in detail, and countless books, youtube lectures are available to help readers learn. However, I demonstrate how to use this simple technique in the context of a CCW analysis.",
    "crumbs": [
      "Survival Analysis Background"
    ]
  },
  {
    "objectID": "qmd/surv_00_summ.html#km",
    "href": "qmd/surv_00_summ.html#km",
    "title": "Exploration of Survival Analyses, Continuous and Discrete Time",
    "section": "Kaplan-Meier Analysis",
    "text": "Kaplan-Meier Analysis\nThe Kaplan-Meier Estimator section walks through a simple example specifically for CCW analyses.",
    "crumbs": [
      "Survival Analysis Background"
    ]
  },
  {
    "objectID": "qmd/surv_00_summ.html#parametric-survival-analyses",
    "href": "qmd/surv_00_summ.html#parametric-survival-analyses",
    "title": "Exploration of Survival Analyses, Continuous and Discrete Time",
    "section": "Parametric survival analyses",
    "text": "Parametric survival analyses\nThe Discrete Time Estimator section explains a general approach for this estimator for CCW analyses.\nWhile non-parametric methods (Kaplan-Meier estimator) are commonly taught, the parametric methods tend to be given a simple overview in introductory courses. When they are taught, I have found the presentation often is framed as “this method could be done but has significant limitations”. However, parametric analyses have significant advantages as well, a text and R package goes over the topic in greater detail.(Tutz and Schmid 2016; Welchowski et al. 2022)",
    "crumbs": [
      "Survival Analysis Background"
    ]
  },
  {
    "objectID": "qmd/surv_00_summ.html#useful-readings",
    "href": "qmd/surv_00_summ.html#useful-readings",
    "title": "Exploration of Survival Analyses, Continuous and Discrete Time",
    "section": "Useful readings",
    "text": "Useful readings\n\n\nTutz, Gerhard, and Matthias Schmid. 2016. “Modeling Discrete Time-to-Event Data,” January. https://doi.org/10.1007/978-3-319-28158-2.\n\n\nWelchowski, Thomas, Moritz Berger, David Koehler, and Matthias Schmid. 2022. discSurv: Discrete Time Survival Analysis. https://CRAN.R-project.org/package=discSurv.",
    "crumbs": [
      "Survival Analysis Background"
    ]
  },
  {
    "objectID": "qmd/index.html",
    "href": "qmd/index.html",
    "title": "CCW Tutorial",
    "section": "",
    "text": "This website is provided as a guide to help researchers interested in applying target trial emulation methods, specifically use of the “clone-censor-weight” or CCW approach.\nCCW is popular in the pharmacoepidemiology field, but its execution can be tricky. A TTE need not be done use CCW, but the approach is popular due to its flexibility. This website will provide a step-by-step guide on how to use this method. I also provide some in-depth discussions of particular aspects of the method, best practices, how to overcome common obstacles etc.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/index.html#welcome",
    "href": "qmd/index.html#welcome",
    "title": "CCW Tutorial",
    "section": "",
    "text": "This website is provided as a guide to help researchers interested in applying target trial emulation methods, specifically use of the “clone-censor-weight” or CCW approach.\nCCW is popular in the pharmacoepidemiology field, but its execution can be tricky. A TTE need not be done use CCW, but the approach is popular due to its flexibility. This website will provide a step-by-step guide on how to use this method. I also provide some in-depth discussions of particular aspects of the method, best practices, how to overcome common obstacles etc.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/index.html#sections",
    "href": "qmd/index.html#sections",
    "title": "CCW Tutorial",
    "section": "Sections of this website",
    "text": "Sections of this website\n\nSurvival Analysis. While not strictly about CCW, the first section of website provides some background for the most common statistical analyses techniques that are used when doing CCW in practice. non-parametric analyses can be used for simple examples but most researchers are using pooled logistic regressions to approximate hazard ratios. I explain this in some detail and provide references to gain further understanding. Being comfortable with parametric discrete survival time analyses is crucial to properly executing more complex CCW analyses with time-varying confounders.\nClone-Censor-Weight. The primary section of this website is the stepwise guide on how to do a CCW analysis. This includes: background discussion, estimation, inference and diagnostics/validation.\nAppendix: Additional sections include a description of the synthetic data, bootstrapping, computation notes and some related methods.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/index.html#reading",
    "href": "qmd/index.html#reading",
    "title": "CCW Tutorial",
    "section": "Background reading",
    "text": "Background reading\nI provide some personal insight into the process, different approaches that can be done, and mainly focus on the analytical methods. In order to actually emulate a target trial, a researcher needs to understand the fundamentals of randomized, controlled trial (RCT) design, causal inference, probability statistics and their practical application with a statistical programming language. I do not go into detail on this.\nThe following readings are recommended for that background and to prepare for the tutorial guide.\n\nFor a full in-depth review of causal inference methods, I recommend Robins’ and Hernan’s Causal Inference: What If.\nRandomized Controlled Trials by Emily Zabor et al. provides background on RCT design. Randomization is a key feature which allows a defensible assumption of no confounding. However, another important but often overlooked advantage of RCTs is a well-defined intervention which allows clear causal contrasts to be made (e.g. enroll eligible persons and then give treatment A versus give treatment B). In observational analyses, a well-defined intervention, and the timing of assignment and assessment of eligibility for that intervention are not always aligned in a logical fashion.\nMiguel Hernan is a leading expert on TTE and provides a short description of it here: JAMA 2022. It is a two-step process; first defining the ideal RCT and how it can be emulated in the data, the second step is using TTE methods to perform that emulation (the focus of this guide).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/index.html#troubleshooting-your-own-project",
    "href": "qmd/index.html#troubleshooting-your-own-project",
    "title": "CCW Tutorial",
    "section": "Troubleshooting your own project",
    "text": "Troubleshooting your own project\nFor those having issues with their own CCW projects. I recommend that you download the synthetic data for this project, found here: Example Data. Then follow the CCW code: Clone-Censor-Weight and see if you can replicate my results. Once you can do that, compare the data, modeling and weighting steps to your own project and try to find where differences are.\nFor developments notes see About.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html",
    "href": "qmd/ccw_04_diag.html",
    "title": "Additional Topics",
    "section": "",
    "text": "The selection of a grace period is ideally based on knowledge of the treatment in real-world use. For example, if there were treatment guidelines recommending an intervention take place within a specific time-frame (6 weeks from hospital discharge, 1 hour from presentation in the ED etc.), then it makes sense to follow that recommendation in selecting a grace period. However, practical limitations may change this for a project. Even though it may be recognized that 6 weeks is recommended, maybe very few initiate that early and 12 weeks captures more treated cases.\nWhatever period is selected, a sound practice is to evaluate how different results appear when different grace periods are selected. It is simplest when the time-frame doesn’t change the estimated effect very much, but a variety of things could impact this. For example, if probability of initiation of treatment varies widely across time due to confounding or some general time-trend then selecting different grace windows could substantially change results.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#evaluation-of-grace-period",
    "href": "qmd/ccw_04_diag.html#evaluation-of-grace-period",
    "title": "Additional Topics",
    "section": "",
    "text": "The selection of a grace period is ideally based on knowledge of the treatment in real-world use. For example, if there were treatment guidelines recommending an intervention take place within a specific time-frame (6 weeks from hospital discharge, 1 hour from presentation in the ED etc.), then it makes sense to follow that recommendation in selecting a grace period. However, practical limitations may change this for a project. Even though it may be recognized that 6 weeks is recommended, maybe very few initiate that early and 12 weeks captures more treated cases.\nWhatever period is selected, a sound practice is to evaluate how different results appear when different grace periods are selected. It is simplest when the time-frame doesn’t change the estimated effect very much, but a variety of things could impact this. For example, if probability of initiation of treatment varies widely across time due to confounding or some general time-trend then selecting different grace windows could substantially change results.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#kaplan-meier-estimator",
    "href": "qmd/ccw_04_diag.html#kaplan-meier-estimator",
    "title": "Additional Topics",
    "section": "Kaplan-Meier Estimator",
    "text": "Kaplan-Meier Estimator\nA reasonable starting point is to evaluate the cloned dataset without probability weighting or pooled logistic regression models. Although this estimator is “naive” in that the artificial censoring is not considered, it is useful for the following reasons:\n\nIt allows examination of the overall time trends, censoring and sample size which can reveal fatal issues with the target trial emulation. For example, treatment is too rare in the grace window used, or event rate is unexpectedly high or low.\nThe non-parametric model allows a visual examination of the cumulative incidences (or event free survival probabilities) which are modeled parametrically in the pooled logistic regression. In other words, examining the time trend can help you determine if a simple polynomial or some more complex spline function is needed. This is not definitive however, because time-varying weights may change the time trends. But in my experience the unadjusted curve provides a good starting point, and practitioners should be very skeptical of weighted analysis that shows dramatically different time trends then the unweighted analysis (e.g. more likely to be an error in coding than real).\n\nBecause the pooled logistic regression models a person-time dataset, for large sample sizes and long follow-up periods this can require a large dataset and make estimation very time consuming.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#estimation",
    "href": "qmd/ccw_04_diag.html#estimation",
    "title": "Additional Topics",
    "section": "Estimation",
    "text": "Estimation\n\n  d_mods = dta_c_person %&gt;%\n  nest(data = -model) %&gt;%\n  mutate(est_km = map(data, \n                   ~broom::tidy(\n1    survfit(Surv(time, event) ~ assign, data=.)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n2    mutate(pr_ev = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n3    pivot_wider(id_cols = c(time),\n                names_from = assign,\n                values_from = c(pr_ev, pr_s,\n                                n.risk, n.event)) %&gt;%\n4    mutate(cir = pr_ev_1 / pr_ev_0,\n           cid = (pr_ev_1 - pr_ev_0))))\n\n\n1\n\nNaive estimator, K-M estimator, assignment is by clone (not person). t_clone is the follow-up time for each clone, taking into account artificial censoring.\n\n2\n\nestimate from the model is the survival probability, so probability of event is 1 - estimate\n\n3\n\nData is in long form, so one colum per group with cumulative incidence/survival\n\n4\n\nEstimands, cir is ratio analogous to relative risk, and cid is analogous to risk difference",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#summarize-km-estimator",
    "href": "qmd/ccw_04_diag.html#summarize-km-estimator",
    "title": "Additional Topics",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator\n\n\n\n\n\n\nA causes Y\n\n\n\n\n\n\n\nA not a cause of Y\n\n\n\n\n\n\nFigure 1. Naive Kaplan-Meier Estimator",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#cox-model",
    "href": "qmd/ccw_04_diag.html#cox-model",
    "title": "Additional Topics",
    "section": "Cox model",
    "text": "Cox model\nFor reference a cox model is also estimated, note it does not include X_t.\n\nd_mods = d_mods %&gt;%\n  mutate(est_cox = map(data, \n                     function(x) {\n                       fit = coxph(Surv(time, event) ~ assign + female + age + X, data = x)\n      surv_adjustedcurves(fit = fit, variable = \"assign\", data = as.data.frame(x)) %&gt;%\n      group_by(variable, time) %&gt;% \n        dplyr::filter(row_number()==1) %&gt;% \n      ungroup %&gt;% \n      mutate(pr_ev = 1 - surv) %&gt;% \n      pivot_wider(., id_cols =c('time'), \n                  names_from = variable, \n                  names_prefix = 'pr_ev_', \n                  values_from = pr_ev  \n      ) %&gt;% \n      mutate(cid = pr_ev_1 - pr_ev_0,  \n             cir = pr_ev_1 / pr_ev_0)}) )",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#estimation-1",
    "href": "qmd/ccw_04_diag.html#estimation-1",
    "title": "Additional Topics",
    "section": "Estimation",
    "text": "Estimation\nThe PLR model requires specification of a function of time. Many start with a quadratic polynomial (i.e. time + time^2). In my own work, I have found this insufficient, and interestingly have found that linear splines work the best. You must decide for yourself. You could compute AIC or some other model fit statistic to select it empirically. I often compare the PLR estimates to the non-parametric estimates for validity. However this doesn’t completely avoid issues, because the weighted estimates (time-varying covariates, interactions with time etc.) could conceivably diverge quite a bit from the unadjusted KM estimator. You can either estimate the outcome in a model with both clones combined in one dataset OR estimate cumulative incidences separately (two models with data limited to assign==1 & assign==0 respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and f(time), e.g. time + time^2 + treat + treat*time + treat*time^2, shorthand below is poly(time, 2, raw=T)*assign.\n\nd_mods_pnl = d_mods_pnl %&gt;%\n  mutate(est_plr_nv = \n           map(data, \n               function(x) {\n1                 d_glm = glm(event==0 ~ poly(time, 2, raw=T)*assign, data=x, family=binomial())\n                 d_plr_naive_est = crossing(assign = 1:0, time = 1:60)\n                 d_plr_naive_est$pr_surv = predict(d_glm, newdata = d_plr_naive_est, type = 'response') \n                 d_plr_naive_est %&gt;%\n                   group_by(assign) %&gt;%\n                   mutate(pr_cumsurv = cumprod(pr_surv),\n2                          pr_cumev = 1 - pr_cumsurv) %&gt;%\n3                   ungroup %&gt;%\n                   pivot_wider(., id_cols =c('time'),\n                               names_from = assign,\n                               names_prefix = 'pr_ev_',\n                               values_from = pr_cumev\n                               ) %&gt;%\n                   mutate(cid = pr_ev_1 - pr_ev_0,\n                          cir = pr_ev_1 / pr_ev_0)\n                 }))\n\n\n1\n\nPLR model, with time*treat interaction. Binomial family for logistic regression.\n\n2\n\nThe cumulative incidence is 1 - cumulative event-free survival probability. To compute, you take the cumulative product by assignment group.\n\n3\n\nReorganize data and summarize by group/time similar to above for the KM estimator.\n\n\n\n\n\n\n\nFigure 2. Unadjusted, unweighted cumulative incidences by treatment group (PLR)\n\n\n\nd_ci_cmp = bind_rows(\n  select(d_mods, model, est_km) %&gt;% unnest(cols = c(est_km)) %&gt;%\n    mutate(Est = 'KM-Naive') %&gt;%\n    select(model, Est, time, pr_ev_1, pr_ev_0, cid),\n  select(d_mods_pnl, model, est_plr_nv) %&gt;% unnest(cols = c(est_plr_nv)) %&gt;%\n    mutate(Est = 'PLR-Naive') %&gt;%\n    select(model, Est, time, pr_ev_0, pr_ev_1, cid)\n) %&gt;%\n    pivot_longer(cols = c(pr_ev_0, pr_ev_1), names_to = 'assign', \n                 names_prefix = 'pr_ev_', values_to = 'pr_ev') %&gt;%\n  nest(data = -model)\n  \nd_gg = mutate(d_ci_cmp, km_gg = map(data, function(x) {\n    d_gg_ci = x %&gt;%\n      ggplot(aes(x=time, color = Est, Group = Est)) +\n      geom_line(aes(y = pr_ev, linetype=assign), linewidth=0.9) +\n      geom_point(aes(y = pr_ev, shape=assign), size=1.2) +\n      scale_x_continuous(breaks = seq(0, 60, 6),\n                         limits = c(0, 60)) +\n      scale_color_manual(values = cbbPalette) +\n      theme_bw() +\n      labs(x = '', y = 'Cumulative incidence') \n    \nd_gg_rr = x %&gt;%\n        ggplot(aes(x=time, color = Est, Group = Est)) +\n        geom_line(aes(y = cid), linewidth=1.1) +\n        scale_x_continuous(breaks = seq(0, 60, 6),\n                           limits = c(0, 60)) +\n        scale_color_manual(values = cbbPalette) +\n        theme_bw() +\n        labs(x = 'Follow-up', y = 'Risk Differences') \n   \n   ggarrange(d_gg_ci, d_gg_rr, nrow=2, common.legend = T)\n  }))\n    \nd_gg$km_gg[[1]] # A -&gt; Y\n\nd_gg$km_gg[[2]] # X -&gt; Y\n\n\n\n\n\n\nA causes Y\n\n\n\n\n\n\n\nA not a cause of Y\n\n\n\n\n\n\nFigure 3. Comparison of Naive PLR versus KM estimator\n\n\n\nSo the PLR model using a simple polynomial approximates the KM estimator. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider the following regarding the parametric time trend:\n\nIt may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.\nThe non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.\nThese initial steps will not guarantee a good fit after weighting is applied, it is only a first-look for diagnostic purposes.\nIf the fit is not satisfying, then simply pick a different function of time. Many others to choose from, basis splines, bs::splines(time, df=6, degree=1), or you can use a mgcv() for penalized splines etc. Also, consider specifying knots at specific time-points like the end of a grace period.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_04_diag.html#comparison-of-covariate-adjusted-plr-with-cox-model",
    "href": "qmd/ccw_04_diag.html#comparison-of-covariate-adjusted-plr-with-cox-model",
    "title": "Additional Topics",
    "section": "Comparison of Covariate-adjusted PLR with Cox model",
    "text": "Comparison of Covariate-adjusted PLR with Cox model\nAnother approach is to compare a pooled logistic regression analysis to a time-invariant Cox model. If time-varying confounding not an issue, this should give a similar result. However, censoring weights are still necessary in final analysis even if no confounding.(Cain et al. 2010)\n\nd_mods_pnl = d_mods_pnl %&gt;%\n  mutate(est_plr_adj = \n           map(data, \n               function(x) {\n                 d_glm = glm(event==0 ~ poly(time, 2, raw=T)*assign + age + female + X, \n1                             data=x, family=binomial())\n                 x$p.noevent0 &lt;- predict(d_glm , x %&gt;%\n2                                      mutate(assign =1), type=\"response\")\n                 x$p.noevent1 &lt;- predict(d_glm, x %&gt;%\n                                      mutate(assign =0), type=\"response\")\n                setDT(x)\n3                  x[, `:=`(pr_surv_1 = cumprod(p.noevent1)), by=list(id, assign)]\n                  x[, `:=`(pr_surv_0 = cumprod(p.noevent0)), by=list(id, assign)]\n                setDF(x)\n  \n4                 x %&gt;%\n                   group_by(time) %&gt;%\n                   summarize(pr_ev_1 = mean(1-pr_surv_1),\n                             pr_ev_0 = mean(1-pr_surv_0), \n                             .groups = 'drop') %&gt;%\n                   ungroup %&gt;%\n                   mutate(cid = pr_ev_1 - pr_ev_0,\n                          cir = pr_ev_1 / pr_ev_0)\n                 }))\n\n\n1\n\nPLR model with some covariates for adjustment. (model fit for each treatment group)\n\n2\n\nPredict outcome under each assignment strategy.\n\n3\n\nEstimate cumulative survival using cumulative product, within person-clone\n\n4\n\nSummarize mean survival rates by time-period\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the outcome regression without weights is just for comparison and understanding, the final analysis must include probability weights for artificial censoring even if no confounding.\n\n\n\nd_mods_pnl = d_mods_pnl %&gt;%\n  mutate(est_cox_adj = \n           map(data, \n               function(x) {\n                 d_cox = coxph(Surv(enter, exit, event) ~ \n                                 assign + female + age + X, \n                               data = x)\n                 d_cox_est = surv_adjustedcurves(fit = d_cox, \n                                                 variable = \"assign\", \n                                                 data = as.data.frame(x)) %&gt;%\n                   group_by(variable, time) %&gt;%\n                     dplyr::filter(row_number()==1) %&gt;%\n                   ungroup %&gt;%\n                   mutate(pr_ev = 1 - surv) %&gt;%\n                   pivot_wider(., id_cols =c('time'),\n                               names_from = variable,\n                               names_prefix = 'pr_ev_',\n                               values_from = pr_ev) %&gt;%\n      mutate(cid = pr_ev_1 - pr_ev_0,\n             cir = pr_ev_1 / pr_ev_0)\n               }))\n\nFor comparison, A time-dependent Cox model is fit.\n\nCox model, with time1, time2 parameters which represent the start and stop time of each interval. Note that althought the cox model is time-dependent and you could adjust for X_t this could be problematic, because X_t also includes time-points post treatment. In our synthetic data, X_t is not a collider (Treat -&gt; X_t &lt;- Outcome) or on the causal path (Treat -&gt; X_t -&gt; Outcome).\nEstimation of adjusted survival curves by treatment group, with subpopulations balanced using conditional method.(“A Comparison of Different Methods to Adjust Survival Curves for Confounders - Denz - 2023 - Statistics in Medicine - Wiley Online Library,” n.d.)\nSummarizing survival probabilities by time\n\nd_ci_cmp = bind_rows(\n  select(d_mods_pnl, model, est_cox_adj) %&gt;% unnest(cols = c(est_cox_adj)) %&gt;%\n    mutate(Est = 'Cox-adj') %&gt;%\n    select(model, Est, time, pr_ev_1, pr_ev_0, cid),\n  select(d_mods_pnl, model, est_plr_adj) %&gt;% unnest(cols = c(est_plr_adj)) %&gt;%\n    mutate(Est = 'PLR-adj') %&gt;%\n    select(model, Est, time, pr_ev_0, pr_ev_1, cid)\n) %&gt;%\n    pivot_longer(cols = c(pr_ev_0, pr_ev_1), names_to = 'assign', \n                 names_prefix = 'pr_ev_', values_to = 'pr_ev') %&gt;%\n  nest(data = -model)\n  \nd_gg = mutate(d_ci_cmp, cox_gg = map(data, function(x) {\n    d_gg_ci = x %&gt;%\n      ggplot(aes(x=time, color = Est, Group = Est)) +\n      geom_line(aes(y = pr_ev, linetype=assign), linewidth=0.9) +\n      geom_point(aes(y = pr_ev, shape=assign), size=1.2) +\n      scale_x_continuous(breaks = seq(0, 60, 6),\n                         limits = c(0, 60)) +\n      scale_color_manual(values = cbbPalette) +\n      theme_bw() +\n      labs(x = '', y = 'Cumulative incidence') \n    \nd_gg_rr = x %&gt;%\n        ggplot(aes(x=time, color = Est, Group = Est)) +\n        geom_line(aes(y = cid), linewidth=1.1) +\n        scale_x_continuous(breaks = seq(0, 60, 6),\n                           limits = c(0, 60)) +\n        scale_color_manual(values = cbbPalette) +\n        theme_bw() +\n        labs(x = 'Follow-up', y = 'Risk Differences') \n   \n   ggarrange(d_gg_ci, d_gg_rr, nrow=2, common.legend = T)\n  }))\n    \nd_gg$cox_gg[[1]] # A -&gt; Y\n\nd_gg$cox_gg[[2]] # X -&gt; Y\n\n\n\n\n\n\nA causes Y\n\n\n\n\n\n\n\nA not a cause of Y\n\n\n\n\n\n\nFigure 4. Comparison of adjusted PLR versus Cox estimator",
    "crumbs": [
      "Clone-Censor-Weight",
      "Additional Topics"
    ]
  },
  {
    "objectID": "qmd/ccw_02_est.html",
    "href": "qmd/ccw_02_est.html",
    "title": "Estimation",
    "section": "",
    "text": "Example uses the cloned dataset panel generated in Data.\n\n\nRows: 104,910\nColumns: 17\n$ model     &lt;chr&gt; \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", …\n$ id        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ time      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ t_outcome &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ t_treat   &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ t_censor  &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ censor    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ event     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ assign    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ enter     &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ exit      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ female    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ X         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ X_t       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ treat     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ outcome   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nThe person- or clone-level data must be expanded to a longitudinal panel where each observation (row) is a period of follow-up.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Estimation"
    ]
  },
  {
    "objectID": "qmd/ccw_02_est.html#data",
    "href": "qmd/ccw_02_est.html#data",
    "title": "Estimation",
    "section": "",
    "text": "Example uses the cloned dataset panel generated in Data.\n\n\nRows: 104,910\nColumns: 17\n$ model     &lt;chr&gt; \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", …\n$ id        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ time      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ t_outcome &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ t_treat   &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ t_censor  &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ censor    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ event     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ assign    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ enter     &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ exit      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ female    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ X         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ X_t       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ treat     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ outcome   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nThe person- or clone-level data must be expanded to a longitudinal panel where each observation (row) is a period of follow-up.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Estimation"
    ]
  },
  {
    "objectID": "qmd/ccw_02_est.html#ipcwplr",
    "href": "qmd/ccw_02_est.html#ipcwplr",
    "title": "Estimation",
    "section": "Estimation of censoring weights",
    "text": "Estimation of censoring weights\n\n\n\n\n\n\nNote\n\n\n\nThere is no consideration here to model training, causal DAGs or covariate selection. It is simply a toy example to show the procedure.\n\n\n\nProbability of treatment initiation\nSince the example treatment strategy focuses on whether or not treatment initiates, the censoring weights can be computed from a model on the uncloned dataset where the event is treatment, and time to treatment.\nThis may be a little confusing, but since my dataset is already cloned (duplicated by assignment). I take the clones where assign=0 from the dta_c_panel dataset. These are clones assigned to not receive treatment, so their censoring time is time to treatment start and can be used for the treatment model. We are estimating time to treatment, not outcome. The clone, assign=0 is already set up for this but for other projects this step will have to be modified if the comparator is different.\n\n1  d_glm_wt = glm(treat ~ poly(time, 2, raw=T) + X + X_t + age + poly(age, 2) + female,\n                 data=dta_c_panel[dta_c_panel$assign==0, ], family=binomial())\n  \n2  dta_c_panel$pr_treat = predict(d_glm_wt, newdata = dta_c_panel, type='response')\n\n\n1\n\nlogit model, where outcome is treat event, and it is regressed on a polynomial for time, along with covariates X, X_t, age and female.\n\n2\n\nFor each person, I estimate the conditional probability of treatment initiation at each timepoint.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is some controversy in this workflow. Some do it like here by modeling the censoring probability off of a person-unique (no clones) dataset with treatment times, but others do it directly in the cloned dataset with the outcome of “censoring” where that may mean treatment initiation or some other thing.(Gaber et al. 2024)\nI am unsure which is better?!",
    "crumbs": [
      "Clone-Censor-Weight",
      "Estimation"
    ]
  },
  {
    "objectID": "qmd/ccw_02_est.html#calculation-of-weights",
    "href": "qmd/ccw_02_est.html#calculation-of-weights",
    "title": "Estimation",
    "section": "Calculation of weights",
    "text": "Calculation of weights\nThis step is highly specific to a project and must be considered carefully. I have found this step is error-prone to errors due to misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to the Weighting scheme section for further discussion.\n\n\n\n\n\n\nNote\n\n\n\nI only show the procedure here, for full diagnostics to evaluate weighting and model specification see: IPW diagnostics\n\n\n\nUnstabilized weights\n\n  setDT(dta_c_panel)\n  \n  dta_c_panel[, ipw := fcase(\n1    assign==0 & censor==0, 1 / (1-pr_treat),\n2    assign==0 & censor==1, 0,\n    assign==1 & censor==1, 0,\n3    assign==1 & time &lt; 12, 1,\n4    assign==1 & time == 12  & t_treat  &lt; 12, 1,\n5    assign==1 & time == 12  & t_treat  ==12 & censor==0, 1 / (pr_treat),\n6    assign==1 & time &gt; 12, 1\n  )]\n\n\n1\n\n(assign=0); because the model is Pr(treat=1), the no treatment clones are assigned 1 - Pr(treat=1), or the cumulative probability of no vaccination ~ Probability of remaining uncensored.\n\n2\n\nIf a clone is censored, I assigned weight=0.\n\n3\n\n(assign=1) Clones cannot artificially censor prior to grace period\n\n4\n\n(assign=1) If treatment started prior to grace window ending, the clone cannot censor\n\n5\n\n(assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).\n\n6\n\n(assign=1) Set post-grace period weights = 1 for all, since treatment must occur by period 12 and no artificial censoring happens after this point for assign==1\n\n\n\n\n\n\nStabilized weights\n\n  dta_c_panel[, marg_c0  := 1-mean(censor), by = list(assign, time)]\n\n  dta_c_panel[, marg_ipw := fcase(\n    assign==0 & censor==0, marg_c0 / (1-pr_treat),\n    assign==0 & censor==1, 0,  \n    assign==1 & censor==1, 0,  \n    assign==1 & time &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  &lt; 12, 1,  \n    assign==1 & time == 12  & t_treat  ==12 & censor==0, marg_c0 / (pr_treat),\n    assign==1 & time &gt; 12, 1 \n  )]\n\nFor the stabilized weights: 1. Compute the mean by period and assignment group. 2. Replace the numerator in the ipw calculation with the marginal probability.\n\n\nFinal weights\n\n  dta_c_panel[, `:=`(ipw = cumprod(ipw), \n                     marg_ipw = cumprod(marg_ipw)), \n               by=list(id, assign)] \n\nCompute the cumulative product within person-clone to reach final IPCW.\n\n\n\n\n\n\nNote\n\n\n\nBe very careful with IPW truncation with this design. If the probability of treatment is very low, then those treated at the end of the grace period will have very large weights. If you truncate at 99% for example, it could mostly just truncate the weights of those treated at the end of the grace period, and this could severely bias estimates. These persons are meant to account for all those being artificially censored in the treatment group for non-treatment and so will likely have large weights by design.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Estimation"
    ]
  },
  {
    "objectID": "qmd/ccw_02_est.html#outcome-model",
    "href": "qmd/ccw_02_est.html#outcome-model",
    "title": "Estimation",
    "section": "Outcome model",
    "text": "Outcome model\n\nPLR model for cumulative incidences\nWith the estimated weights, it is simple to generate weighted cumulative incidences:\n\n1  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=dta_c_panel[dta_c_panel$assign==1, ],\n                   family=binomial(), weights = ipw)\n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=dta_c_panel[dta_c_panel$assign==0, ],\n                     family=binomial(), weights = ipw)\n  \n2  dta_c_panel$pr_1 = predict(d_glm_pe_1, newdata = dta_c_panel,\n                             type='response')\n  dta_c_panel$pr_0 = predict(d_glm_pe_0, newdata = dta_c_panel,\n                             type='response')\n\n\n1\n\nEstimate probability of the event for each assignment group separately. This isn’t necessary but its good practice, and adjusting for other covariates may give different results as well. Note the weights = ipw argument. R glm() will generate a warning message because the weighted counts are “non-integer”, but this is expected and not a problem.\n\n2\n\nAssign conditional probabilities for event to each person-clone.\n\n\n\n\n\n\nEstimate cumulative incidences\n\n1  dta_c_panel[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)]\n  dta_c_panel[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)]\n  \n2  d_plrwt_est = dta_c_panel %&gt;%\n    group_by(time) %&gt;%\n    summarize(pr_ev_1 = mean(1-pr_cum_1),\n              pr_ev_0 = mean(1-pr_cum_0), \n              .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nTo estimate the cumulative event-free survival (1 - P(S) for incidence) you take the cumulative product within person-clone.\n\n2\n\nSummarize across group, time. I will generally compute both the risk difference cid and relative differences cir.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Estimation"
    ]
  },
  {
    "objectID": "qmd/ccw_02_est.html#summarize-results",
    "href": "qmd/ccw_02_est.html#summarize-results",
    "title": "Estimation",
    "section": "Summarize Results",
    "text": "Summarize Results\n\nFigure\n\n\n\n\n\nWeighted PLR Analysis\n\n\n\n\nThe results can be easily depicted in a standard graph like any survival analysis. You could also combine the lineplot with a table of the unweighted or weighted person and event counts at each point of follow-up like many do in a Kaplan-Meier analysis.\n\n\nTable\n\n\n\n\nA -&gt; Y, X -&gt; A, Y\n\n\n\n\n\n\n\n\n\nRisk Differences\n\n\n\n\n\ntime\n\n\nNaive\n\n\nIPCW\n\n\nStabilized\n\n\n\n\n\n\n6\n\n\n0.001\n\n\n-0.006\n\n\n-0.001\n\n\n\n\n12\n\n\n-0.008\n\n\n-0.013\n\n\n-0.008\n\n\n\n\n36\n\n\n-0.084\n\n\n-0.074\n\n\n-0.079\n\n\n\n\n60\n\n\n-0.178\n\n\n-0.216\n\n\n-0.201\n\n\n\n\n\n\n\n\n\n\n\nRelative Risks\n\n\n\n\n\ntime\n\n\nNaive\n\n\nIPCW\n\n\nStabilized\n\n\n\n\n\n\n6\n\n\n1.029\n\n\n0.791\n\n\n0.964\n\n\n\n\n12\n\n\n0.860\n\n\n0.770\n\n\n0.853\n\n\n\n\n36\n\n\n0.538\n\n\n0.602\n\n\n0.578\n\n\n\n\n60\n\n\n0.490\n\n\n0.403\n\n\n0.444\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX -&gt; A, Y, A does not cause Y\n\n\n\n\n\n\n\n\n\nRisk Differences\n\n\n\n\n\ntime\n\n\nNaive\n\n\nIPCW\n\n\nStabilized\n\n\n\n\n\n\n6\n\n\n0.000\n\n\n0.004\n\n\n0.000\n\n\n\n\n12\n\n\n0.000\n\n\n0.013\n\n\n0.004\n\n\n\n\n36\n\n\n0.001\n\n\n0.051\n\n\n0.026\n\n\n\n\n60\n\n\n-0.004\n\n\n0.009\n\n\n-0.002\n\n\n\n\n\n\n\n\n\n\n\nRelative Risks\n\n\n\n\n\ntime\n\n\nNaive\n\n\nIPCW\n\n\nStabilized\n\n\n\n\n\n\n6\n\n\n0.986\n\n\n1.150\n\n\n1.011\n\n\n\n\n12\n\n\n0.994\n\n\n1.215\n\n\n1.062\n\n\n\n\n36\n\n\n1.008\n\n\n1.262\n\n\n1.135\n\n\n\n\n60\n\n\n0.988\n\n\n1.026\n\n\n0.994",
    "crumbs": [
      "Clone-Censor-Weight",
      "Estimation"
    ]
  },
  {
    "objectID": "qmd/ccw_00_summ.html",
    "href": "qmd/ccw_00_summ.html",
    "title": "Clone-Censor-Weight Guide",
    "section": "",
    "text": "This is a huge topic, and the main purpose of the guide.\n\n\n\n\n\n\nNote\n\n\n\nI originally learned this as ‘clone-censor-weight’, but I am seeing ‘cloning-censoring-weighting’ also being used more frequently, I prefer the former.",
    "crumbs": [
      "Clone-Censor-Weight"
    ]
  },
  {
    "objectID": "qmd/ccw_00_summ.html#clone-censor-weight",
    "href": "qmd/ccw_00_summ.html#clone-censor-weight",
    "title": "Clone-Censor-Weight Guide",
    "section": "",
    "text": "This is a huge topic, and the main purpose of the guide.\n\n\n\n\n\n\nNote\n\n\n\nI originally learned this as ‘clone-censor-weight’, but I am seeing ‘cloning-censoring-weighting’ also being used more frequently, I prefer the former.",
    "crumbs": [
      "Clone-Censor-Weight"
    ]
  },
  {
    "objectID": "qmd/ccw_00_summ.html#sec-sections",
    "href": "qmd/ccw_00_summ.html#sec-sections",
    "title": "Clone-Censor-Weight Guide",
    "section": "Sections",
    "text": "Sections\nThe Introduction section reviews the basic target trial emulation protocol, treatment assignments and cloning. This is a key first step that establishes all the subsequent analytical choices. It is also very project specific, so what is done here may not generalize well to others.\nAfter establishing the TTE parameters, Estimator goes over the main analysis. This page goes through the procedure, but many aspects need to be interrogated for proper model specification, which is reviewed in Diagnostics. Inference gives my personal approach for bootstrapping uncertainty estimates, there are other approaches for that (See Bootstrapping). Finally, I provide some recommendations for preparing a report of results Presentation.",
    "crumbs": [
      "Clone-Censor-Weight"
    ]
  },
  {
    "objectID": "qmd/ccw_00_summ.html#stepwise-approach",
    "href": "qmd/ccw_00_summ.html#stepwise-approach",
    "title": "Clone-Censor-Weight Guide",
    "section": "Stepwise approach",
    "text": "Stepwise approach\nI recommend the this generic stepwise procedure for CCW methods:\n\nEstimate unweighted/unadjusted cumulative incidences with KM method KM estimator\nEstimate a PLR model without weights Discrete time section\n\n\n\nCompare KM vs PLR, Diagnostics\n\n\n\nEstimate censoring weights, IPCW Estimation\n\n\n\nEstimate PLR for treatment initiation Estimate\nA. Estimate treatment model\nB. Compare KM / PLR estimates\nC. Compute IPCW\nD. Examine IPCW weights for extreme values, non-sensical values\nE. Examine balance in covariates across time\nGenerate “table 1” with some weighted difference statistic (e.g. wSMD) Diagnostics\n\n\n\nEstimate weighted outcome models Estimate\n\n\n\nEstimate a weighted outcome model, no covariate adjustment\nEstimate a weighted outcome + covariates (usually main estimate)\n\n\n\nOnce satisfied with stability from steps 1-4, execute bootstraps Inference\nFinalize report\n\n\n\nprovide KM, Cox unweighted, and PLR unweighted estimates in an appendix\nmain results reported should be the IPCW-weighted PLR analysis\n\n\nPLR Discrete time section\n\nPooled Logistic Regression, a parametric model which approximates the hazards with discretized follow-up time periods.\n\nKM KM estimator\n\nKaplan-Meier, non-parametric estimator which computes the instantaneous hazard at points of follow-up. Does not allow for time-varying confounding, but the non-parametric evaluation of time-trends can be useful for diagnostics and model specification of the PLR.",
    "crumbs": [
      "Clone-Censor-Weight"
    ]
  },
  {
    "objectID": "qmd/ccw_00_summ.html#applied-examples-in-the-literature",
    "href": "qmd/ccw_00_summ.html#applied-examples-in-the-literature",
    "title": "Clone-Censor-Weight Guide",
    "section": "Applied examples in the literature:",
    "text": "Applied examples in the literature:\nMy first CCW application was an evaluation of vaccine effects on all-cause death.(McConeghy et al. 2024)\nSeveral other good examples are published.\n\n\nMcConeghy, Kevin W, Kwan Hur, Issa J Dahabreh, Rong Jiang, Lucy Pandey, Walid F Gellad, Peter Glassman, et al. 2024. “Early Mortality After the First Dose of COVID-19 Vaccination: A Target Trial Emulation.” Clin. Infect. Dis. 78 (3): 625–32.",
    "crumbs": [
      "Clone-Censor-Weight"
    ]
  },
  {
    "objectID": "qmd/app_02_boot.html",
    "href": "qmd/app_02_boot.html",
    "title": "Bootstrapping methods",
    "section": "",
    "text": "Bootstrapping procedure\nBootstrapping is the main option for this type of analysis, the statistical properties are not well-described, but some use influence-based statistics.\nThe typical bootstrap procedure resamples an entire dataset iteratively, but this can be very inefficient depending on how you set it up because it may involve holding the original dataset and the bootstrapped dataset in memory, also potentially a matrix in a regression optimization step. However some clever use of matrices and shortcuts can work around this.\n\n\n\n\n\n\nNote\n\n\n\nThe bootstrap procedure must sample at the person level to account for the cloning.\n\n\n\nInefficient Bootstrap\n\nboot_it_1 = function(x) {\n  \n1  ids = distinct(x, id)\nsampled_ids = slice_sample(ids, prop=1, replace = TRUE) %&gt;%\n  group_by(id) %&gt;%\n2    summarize(freqwt = n())\n  \n  d_boot = left_join(x, sampled_ids, by = join_by(id)) %&gt;%\n    mutate(freqwt = coalesce(freqwt, 0L))\n  \n  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), \n                     data=d_boot[d_boot$assign==1, ], \n                     family=binomial()) \n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), \n                   data=d_boot[d_boot$assign==0, ], \n                     family=binomial())\n  \n  d_boot$pr_1 = predict(d_glm_pe_1, newdata = d_boot, \n                              type='response') \n  d_boot$pr_0 = predict(d_glm_pe_0, newdata = d_boot, \n                             type='response') \n\n  setDT(d_boot)\n\n  d_boot[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  d_boot[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = d_boot %&gt;% \n    group_by(time) %&gt;% \n    summarize(pr_ev_1 = mean(1-pr_cum_1),\n              pr_ev_0 = mean(1-pr_cum_0), \n              .groups = 'drop') %&gt;% \n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0)\n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a list of unique person IDs\n\n2\n\nSample from list with replacement\n\n\n\n\n\n\nMore efficient bootstrap\nRather than sampling rows of the matrix with replacement, an alternative is to approximate the sampling with a frequency weight. If you randomly assign every observation a value drawn from a Poisson distribution with mean 1, and use this value as a frequency weight in estimators you will closely approximate the full bootstrap procedure as long as the overall sample size is &gt;100.(Hanley and MacGibbon 2006) This is very computationally efficient because you do not need to know the dataset size prior to assigning the frequency weight, and do not join or work with multiple large matrices.\n\nboot_it_2 = function(x) {\n  \n  setDT(x)\n  \n1  x[, freqwt:=rpois(n=1, lambda=1), by = factor(id)]\n\n  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), \n                     data=x[x$assign==1, ], \n                     family=binomial(), weights = freqwt) \n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), \n                   data=x[x$assign==0, ], \n                     family=binomial(), weights = freqwt) \n  \n  x$pr_1 = predict(d_glm_pe_1, newdata = x, \n                              type='response') \n  x$pr_0 = predict(d_glm_pe_0, newdata = x, \n                             type='response') \n\n  x[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  x[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = x %&gt;% \n    group_by(time) %&gt;% \n2    summarize(pr_ev_1 = weighted.mean(1-pr_cum_1, freqwt),\n              pr_ev_0 = weighted.mean(1-pr_cum_0, freqwt),\n              .groups = 'drop') %&gt;% \n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a frequency weight by group ID from a Poisson distribution with mean 1\n\n2\n\nNote the use of weighted.mean() versus mean() in other code.\n\n\n\n\n\n\nBayesian Bootstrap\nIn addition to use of weighting, a Bayesian bootstrap which assigns weights from a uniform Dirichlet prior may also avoid some issues by smoothing out the distribution of weights, where the Poisson counts may fail due to collinearity issues from dropping observations in various bootstrap iterations.(Rubin 1981)\nWe initiate one bootstrap at a time. The uniform Dirichlet is then \\(~Gamma(1, 1)\\) the same as \\(~exp(1)\\), so the function uses random draws from an exponential distribution, then normalized.\n\nboot_it_3 = function(x) {\n  \n  # this join is a little inefficient, probably faster way to do\n1  x = x %&gt;% nest(-id) %&gt;%\n  mutate(prior = rexp(n=n(), rate=1),\n         freqwt = prior / sum(prior)) %&gt;%\n    unnest \n\n  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), \n                     data=x[x$assign==1, ], \n                     family=binomial(), weights = freqwt) \n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), \n                   data=x[x$assign==0, ], \n                     family=binomial(), weights = freqwt) \n  \n  x$pr_1 = predict(d_glm_pe_1, newdata = x, \n                              type='response') \n  x$pr_0 = predict(d_glm_pe_0, newdata = x, \n                             type='response') \n\n  setDT(x)\n  x[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  x[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = x %&gt;% \n    group_by(time) %&gt;% \n    summarize(pr_ev_1 = weighted.mean(1-pr_cum_1, w=freqwt), \n              pr_ev_0 = weighted.mean(1-pr_cum_0, w=freqwt),\n              .groups = 'drop') %&gt;% \n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nDirichlet uniform prior (which is same as normalized vector drawm from exponential distribution with mean 1).\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n                 expr   min    lq  mean median    uq   max neval\n Resampling bootstrap 0.401 0.538 0.565  0.558 0.598 0.764    20\n    Poisson bootstrap 0.366 0.464 0.495  0.506 0.531 0.615    20\n   Bayesian bootstrap 0.630 1.170 1.150  1.190 1.230 1.420    20\n\n\n\n\nComparison\nIf you don’t believe this provides similar coverage estimates, here are the intervals from 500 bootstraps with both procedures:\n\nAssign number of CPU workers to job\nThe seed=T option is important because the bootstrap function uses RNG.\n\n\n\n\nDistribution of risk differences by either bootstrap methods\n\n\nMethod\nmin\nLower CI\nmean\nq50th\nUpper CI\nmax\nSD\n\n\n\n\nBayesian\n-0.242\n-0.223\n-0.178\n-0.177\n-0.143\n-0.136\n0.019\n\n\nPoisson\n-0.233\n-0.220\n-0.177\n-0.177\n-0.135\n-0.121\n0.023\n\n\nResampling\n-0.178\n-0.178\n-0.178\n-0.178\n-0.178\n-0.178\n0.000\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHanley, James A., and Brenda MacGibbon. 2006. “Creating Non-Parametric Bootstrap Samples Using Poisson Frequencies.” Computer Methods and Programs in Biomedicine 83 (1): 57–62. https://doi.org/10.1016/j.cmpb.2006.04.006.\n\n\nRubin, Donald B. 1981. “The Bayesian Bootstrap.” The Annals of Statistics 9 (1): 130–34. https://www.jstor.org/stable/2240875.",
    "crumbs": [
      "Appendix",
      "Bootstrapping methods"
    ]
  },
  {
    "objectID": "qmd/app_00_summ.html",
    "href": "qmd/app_00_summ.html",
    "title": "Appendix",
    "section": "",
    "text": "Sections\n\nSynthetic Data: Explanation of simulation data\nBootstrap: Some comparisons of bootstrap methods\nComputation efficiency: Some comparisons of bootstrap methods",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "qmd/about.html",
    "href": "qmd/about.html",
    "title": "Homepage",
    "section": "",
    "text": "The current tutorial version demonstrates a basic start to finish example of the “Clone-Censor-Weight” approach. The tutorial is a work in progress, and continually updated based on feedback, advances in science or as solutions to particular problems become known.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#acknowledgements",
    "href": "qmd/about.html#acknowledgements",
    "title": "Homepage",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe tutorial represents my gathered and organized notes from research projects and didactic training. Collaborators and mentors include: Issa Dahabreh, Kaley Hayes, Daniel Harris, Donald Miller and Andrew Zullo.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.7-01052026",
    "href": "qmd/about.html#version-0.7-01052026",
    "title": "Homepage",
    "section": "Version 0.7 01/05/2026",
    "text": "Version 0.7 01/05/2026\n\nComplete rework of website flow,\nAdded sections to further explain survival analysis methods\nReworked data to include multiple examples",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.6-07102025",
    "href": "qmd/about.html#version-0.6-07102025",
    "title": "Homepage",
    "section": "Version 0.6 07/10/2025",
    "text": "Version 0.6 07/10/2025\n\nIncluded Bayesian bootstrap\nAdded diagnostic section, and moved some of the old estimation steps to this section",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.5-05-22-2025",
    "href": "qmd/about.html#version-0.5-05-22-2025",
    "title": "Homepage",
    "section": "Version 0.5 05-22-2025",
    "text": "Version 0.5 05-22-2025\n\nTutorial appears to work as expected, some edits to text and code to be more concise",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.3-05-21-2025",
    "href": "qmd/about.html#version-0.3-05-21-2025",
    "title": "Homepage",
    "section": "Version 0.3 05-21-2025",
    "text": "Version 0.3 05-21-2025\n\nRevamped synthetic data for time-varying confounding\nAdded cox models to estimator page\nSwitched to color-blind friendly color-scheme",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.2-05-13-2025",
    "href": "qmd/about.html#version-0.2-05-13-2025",
    "title": "Homepage",
    "section": "Version 0.2 05-13-2025",
    "text": "Version 0.2 05-13-2025\n\nAdded Parallel computation step to Advanced Topics page\nSetup for the inference page (WIP)",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.1-05-12-2025",
    "href": "qmd/about.html#version-0.1-05-12-2025",
    "title": "Homepage",
    "section": "Version 0.1 05-12-2025",
    "text": "Version 0.1 05-12-2025\n\nSynthetic data set-up for observed effect by treatment due to confounding; no effect after appropriate adjustment\nEstimator file complete; additional edits for clarity and alternative approaches\nInference file TBD\nAdvanced file discusses some bootstrap and computation approaches\nAppendix TBD",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/about.html#version-0.0.1-02-19-2025",
    "href": "qmd/about.html#version-0.0.1-02-19-2025",
    "title": "Homepage",
    "section": "Version 0.0.1 02-19-2025",
    "text": "Version 0.0.1 02-19-2025\nProject set-up",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "qmd/app_01_syndata.html",
    "href": "qmd/app_01_syndata.html",
    "title": "Simulate data for CCW project",
    "section": "",
    "text": "Causal Directed Acyclic Graph\nWe assume a generic treatment, A, has no causal effect on generic outcome, Y. However, confounding is present through another random variable. The DAG includes age, X a confounder, sex defined through a binary variable F for female. X is both a baseline variable, and a time-varying confounder. Below we assign effects of these variables that will lead to identification of a causal effect between A and Y, if no controlling for the confounding is done.\n\n\n\n\n\n\nA causes Y\n\n\n\n\n\n\n\nA not a cause of Y\n\n\n\n\n\n\nFigure 1. Causal DAGs of treatment and outcome\n\n\n\nTwo basic scenarios are mapped out. 1) Where treatment A causes Y but a confounder X is present, 2) Treatment A doesn’t cause Y, but confounder is present. In a naive analysis of either (no adjustment), the estimate will be biased because treatment and outcome are d-connected.\n\n\n\n\n\n\nA causes Y\n\n\n\n\n\n\n\nA not a cause of Y\n\n\n\n\n\n\nFigure 2. D-separated treatment and outcome after adjustment\n\n\n\nAdjustment for X should lead to identification of a NULL effect of treatment on outcome in scenario 2 but not 1.\n\n\nSimulation Parameters\n\nset.seed(100)\n1n = 2000L\nfup = 61\n\n2df = tibble(id = 1:n,\n            age = round(rnorm(n, mean = 75, sd = 10), 1),\n            female = sample(c(1, 0), size = n, replace = TRUE, prob = c(0.66, 0.34)),\n            fup = fup,\n            X = rbinom(n, 1, 0.05),\n            )\n  \nsetDT(df) \n3d_panel = df[rep(seq(.N), fup)]\nd_panel[, exit := (seq_len(.N)), by = list(id)]\nd_panel[, enter := exit-1]\nd_panel[, time := seq_len(.N), by = list(id)]\n\n\n1\n\nSpecify 2000 individuals with 61 observation periods of follow-up.\n\n2\n\nSpecify covariates, age, female and X.\n\n3\n\nExpand dataset, each observation period is an interval of length = 1.\n\n\n\n\nFor baseline X, further include X_t which varies across time. A change in X_t is triggered by intermediate variable X_shift which is a random binomial draw (meant to represent a triggering event for a change in X). So for each person, there is a baseline X, then a random timepoint in follow-up where X changes.\n\nd_panel[, X_t := rbinom(.N, 1, 0.03)]\n\n\n\n\nTime to treatment and outcome is simulated in a longitudinal dataset, to allow for time-varying effect of X. Then the dataset is modified so that persons follow-up ends at the time of outcome:\n\n  set.seed(101)\n  \nd_panel_2 = d_panel %&gt;% \n1    # Treat event\n    mutate(log_odds = -4.1 + 0.0001*time + -0.00001*(time^2) + 1.2*X + 1.5*X_t,\n           p = exp(log_odds) / (1 + exp(log_odds)),\n           treat = rbinom(n(), size = 1, prob = p),\n           treat = if_else(treat==1, 1, NA_integer_)\n    ) %&gt;%\n    group_by(id) %&gt;%\n      fill(treat, .direction = 'down') %&gt;%\n    ungroup %&gt;%\n    mutate(treat = coalesce(treat, 0L))\n  \n# Dataset with no A -&gt; Y effect\n  d_xy = d_panel_2 %&gt;%\n    mutate(model = 'xy') %&gt;%\n2    # Outcome event\n    mutate(log_odds = -5 + 0.007*time + 0.0002*(time^2) + 0.6*X + 0.9*X_t + \n                                  -0.2*female + 0.07*age + -0.001*(age^2),\n           p = exp(log_odds) / (1 + exp(log_odds)),\n           outcome = rbinom(n(), size = 1, prob = p),\n           outcome = if_else(outcome==1, 1, NA_integer_)\n    ) %&gt;%\n    group_by(id) %&gt;%\n      fill(outcome, .direction = 'down') %&gt;% \n    ungroup %&gt;%\n    mutate(outcome = coalesce(outcome, 0L)) %&gt;%\n3    group_by(id) %&gt;%\n      mutate(t_outcome = coalesce(min(time[outcome==1]), Inf),\n             t_treat   = coalesce(min(time[treat==1]), Inf)) %&gt;%\n    ungroup %&gt;%\n    dplyr::filter(outcome  == 0 | time == t_outcome) %&gt;%\n    dplyr::filter(time &lt;= 60) %&gt;%\n    mutate(t_treat = if_else(t_outcome &lt; t_treat, Inf, t_treat),\n           t_treat = if_else(60 &lt; t_treat, Inf, t_treat))\n  \n# Dataset with A -&gt; Y effect\n  d_ay = d_panel_2 %&gt;%\n    mutate(model = 'ay') %&gt;%\n    # Outcome event # \n    mutate(log_odds = -5 + 0.007*time + 0.0002*(time^2) + 0.6*X + 0.9*X_t + \n4                                  -0.2*female + 0.07*age + -0.001*(age^2) + treat*-1.2,\n           p = exp(log_odds) / (1 + exp(log_odds)),\n           outcome = rbinom(n(), size = 1, prob = p),\n           outcome = if_else(outcome==1, 1, NA_integer_)\n    ) %&gt;%\n    group_by(id) %&gt;%\n      fill(outcome, .direction = 'down') %&gt;% \n    ungroup %&gt;%\n    mutate(outcome = coalesce(outcome, 0L)) %&gt;% # \n    group_by(id) %&gt;%\n      mutate(t_outcome = coalesce(min(time[outcome==1]), Inf),\n             t_treat   = coalesce(min(time[treat==1]), Inf)) %&gt;%\n    ungroup %&gt;%\n    dplyr::filter(outcome  == 0 | time == t_outcome) %&gt;%\n    dplyr::filter(time &lt;= 60) %&gt;%\n    mutate(t_treat = if_else(t_outcome &lt; t_treat, Inf, t_treat),\n           t_treat = if_else(60 &lt; t_treat, Inf, t_treat))  \n  \n  d_panel_3 = bind_rows(d_ay, d_xy)\n\n\n1\n\nSimulation of treatment event times. Treatment is only a function of time, X, and X_t.\n\n2\n\nSimulation of outcome event times. Outcome is a function of time, X, and X_t, F, Age.\n\n3\n\nModify dataset so that follow-up times coincide with new treatment and outcome variables.\n\n4\n\nAdditional term for treatment for modeling a causal effect.\n\n\n\n\n\nSimulated treatment times\n\n\n\nTime to treatment by causal model\n\n\nmodel\nay\nxy\n\n\nmin\n1\n1\n\n\nq05\n3\n4\n\n\nq25\n15\n16\n\n\nq50\n37\n39\n\n\nq75\nInf\nInf\n\n\nq95\nInf\nInf\n\n\nmax\nInf\nInf\n\n\n\n\n\n\n\nSimulated outcome times\n\n\n\nTime to treatment by causal model\n\n\nmodel\nay\nxy\n\n\nmin\n1\n1\n\n\nq05\n36\n30\n\n\nq25\nInf\n61\n\n\nq50\nInf\nInf\n\n\nq75\nInf\nInf\n\n\nq95\nInf\nInf\n\n\nmax\nInf\nInf\n\n\n\n\n\n\n\n\n\n\nFigure 5. Treatment and Outcome Incidence Across Follow-up\n\n\n\n\n\n\n\n\n\nFigure 6. Correlation plot of covariates, treatment, outcome (A-&gt;Y model)\n\n\n\n\n\n\n\nSummary of survival dataset\n\n\nRows: 205,755\nColumns: 16\n$ id        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ age       &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ female    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fup       &lt;dbl&gt; 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, …\n$ X         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exit      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ enter     &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ time      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ X_t       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ log_odds  &lt;dbl&gt; -4.9928, -4.9852, -4.9772, -4.9688, -4.9600, -4.9508, -4.941…\n$ p         &lt;dbl&gt; 0.006740887, 0.006791964, 0.006846144, 0.006903495, 0.006964…\n$ treat     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ model     &lt;chr&gt; \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", …\n$ outcome   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ t_outcome &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ t_treat   &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n\n\n\n\nCloning procedure\nThe cloning procedure is very project specific, tied to how the treatment strategy is defined so it is hard to give general recommendations here. For this tutorial, we describe an arbitrary window in which treatment is expected to initiate within 12 time periods and outline strategies around this:\nTreatment Strategies: \n\nDo not ever receive treatment\nInitiate treatment within 12 time periods (days, weeks etc.) and if not then treatment will initiate on week 12\n\n\n\n\n\n\n\nNote\n\n\n\nThe grace window is a funny concept when you first consider it. This does not reflect any trial I have ever heard of actually being done but may identify an interesting or important effect. It is essentially outlying a treatment “protocol” and saying what if everyone adhered to this protocol counterfactual to what was observed or some other intervention (i.e. no one initiates treatment).\n\n\nTo clone, you make two copies of the data, and make a new artifical censoring variable, which is a censoring time for the period when clones observed data are no longer consistent with their assigned strategy.\n\n1data_cloned = bind_rows(\n                     d_panel_3 %&gt;%\n                       mutate(assign = 0,\n                              censor = if_else(t_treat &lt;= time, 1L, 0L),\n                              event = if_else(censor==0 & t_outcome&lt;=time, 1L, 0L),\n                       ),\n2                     d_panel_3 %&gt;%\n                       mutate(assign = 1,\n                              censor = if_else(t_treat &gt; 12 & time&gt;=12, 1L, 0L),\n                              event  = if_else(censor==0 & t_outcome&lt;=time, 1L, 0L)\n                              )\n                       ) %&gt;%\n  arrange(model, id, assign, time) %&gt;%\n  group_by(model, id, assign) %&gt;%\n    mutate(t_censor = min(time[censor==1])) %&gt;%\n3    dplyr::filter(censor == 0 | time == min(time[censor==1])) %&gt;%\n  ungroup %&gt;%\n  select(model, id, time, t_outcome, t_treat, t_censor, censor, event, assign, enter, exit, everything(), -fup, -log_odds, -p)\n\n\n1\n\nClones assigned to strategy 1 (No treatment)\n\n2\n\nClones assigned to strategy 2 (grace window for treatment)\n\n3\n\nUpdate failure times and events counting for artificial censoring\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how we do not specify death or some other competing event, competing events complicate things but are common in applied work. I keep it simple here.\n\n\n\n\nSummary of simulated, cloned data-set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nay\nay\nxy\nxy\n\n\nassign\n0\n1\n0\n1\n\n\nUnique persons\n2000\n2000\n2000\n2000\n\n\nMedian (Follow-up)\n30\n12\n30\n12\n\n\nTotal treat (ever)\n1165\n1165\n1153\n1153\n\n\nTotal treat (t&lt;=12)\n388\n388\n384\n384\n\n\nTotal events\n401\n143\n418\n235\n\n\nTotal censored\n1165\n1525\n1153\n1511\n\n\n\n\n\nA person-level version of the data is also created (excluded time-varying X) for comparison:\n\ndata_cloned_p = data_cloned %&gt;%\n  group_by(model, id, assign) %&gt;%\n    mutate(t_censor = min(time[censor==1])) %&gt;%\n  ungroup %&gt;%\n  mutate(time = pmin(t_censor, t_outcome, 60),\n         event = case_when(\n           time==t_censor ~ 0, \n           time==t_outcome ~ 1,\n           T ~ 0)) %&gt;%\n  select(model, id, assign, t_treat, t_outcome, t_censor, time, event, X, female, age) %&gt;%\n  distinct()",
    "crumbs": [
      "Appendix",
      "Simulate data for CCW project"
    ]
  },
  {
    "objectID": "qmd/app_03_speed.html",
    "href": "qmd/app_03_speed.html",
    "title": "Computational methods for speed",
    "section": "",
    "text": "Because the pooled logistic regression models a person-time dataset, for large sample sizes and long follow-up periods this can require a large dataset and make estimation very time consuming.\n\n\n\nglm(event==0 ~ poly(time, 2, raw=T)*assign, data=dta_c_panel, family=binomial())\n\n\n\nThere are two main things that can be done to speed up GLM, 1) initialize parameters based on prior estimation procedure. 2) Use efficient matrix functions and parallel computation.\n\n\nThis is a simple trick, either 1) run the GLM once and store est, or 2) run the GLM on a 10% sample.\n\nd_fit = glm(event==0 ~ poly(time, 2, raw=T)*assign, \n1            data=dta_c_panel, family=binomial())\n\nd_fit_2 = glm(event==0 ~ poly(time, 2, raw=T)*assign, \n              data=dta_c_panel, family=binomial(), \n2              start = d_fit$coefficients)\n\n\n1\n\nGLM procedure with automatic initialization step.\n\n2\n\nGLM with start= option using coefficients from prior step.\n\n\n\n\n\n\n\nThe parglm package provides much faster computations (but somewhat more unstable).\n\nlibrary(parglm)\n\nd_fit_3 = parglm(event==0 ~ poly(time, 2, raw=T)*assign, \n       family=binomial(), data=dta_c_panel, \n       start = d_fit$coefficients,\n1       control = parglm.control(method = \"FAST\", nthreads = 8L))\n\n\n1\n\nparglm() function works mostly as glm(), the parglm.control() allows some additional options for parallel computing and QR decomposition.\n\n\n\n\n\n\n\n\n\n\nUnit: seconds\n           expr    min     lq  mean median     uq   max neval\n       base GLM 0.3420 0.4550 0.473  0.484 0.4990 0.565    20\n GLM with inits 0.1430 0.1510 0.182  0.155 0.1700 0.327    20\n         PARGLM 0.0914 0.0947 0.108  0.097 0.0999 0.207    20\n\n\nEven in a small example, parglm() significantly outperforms base glm(). This will scale considerably with multiple cores and larger datasets as well.\nparglm() may be more unstable (convergence issues), but should be sufficient for most problems.\n\n\n\nComparison of glm() and parglm() results\n\n\nCoefficient\nglm()\nparglm()\n\n\n\n\n(Intercept)\n5.44292\n5.44292\n\n\npoly(time, 2, raw = T)1\n-0.00999\n-0.00999\n\n\npoly(time, 2, raw = T)2\n-0.00013\n-0.00013\n\n\nassign\n-0.28224\n-0.28224\n\n\npoly(time, 2, raw = T)1:assign\n0.07837\n0.07837\n\n\npoly(time, 2, raw = T)2:assign\n-0.00104\n-0.00104\n\n\n\n\n\nParallel computation is also an option to speed up workflow. The efficiency of this depends on how the data is setup and what steps are running in parallel. It is not efficient to hold several very large datasets in memory at once so that a CPU worker can be assigned to each. However, parglm breaks up the dataset into chunks and uses parallel processing to evaluate each subset which can be very efficient.",
    "crumbs": [
      "Appendix",
      "Computational methods for speed"
    ]
  },
  {
    "objectID": "qmd/app_03_speed.html#benchmarked-estimation-step",
    "href": "qmd/app_03_speed.html#benchmarked-estimation-step",
    "title": "Computational methods for speed",
    "section": "",
    "text": "glm(event==0 ~ poly(time, 2, raw=T)*assign, data=dta_c_panel, family=binomial())\n\n\n\nThere are two main things that can be done to speed up GLM, 1) initialize parameters based on prior estimation procedure. 2) Use efficient matrix functions and parallel computation.\n\n\nThis is a simple trick, either 1) run the GLM once and store est, or 2) run the GLM on a 10% sample.\n\nd_fit = glm(event==0 ~ poly(time, 2, raw=T)*assign, \n1            data=dta_c_panel, family=binomial())\n\nd_fit_2 = glm(event==0 ~ poly(time, 2, raw=T)*assign, \n              data=dta_c_panel, family=binomial(), \n2              start = d_fit$coefficients)\n\n\n1\n\nGLM procedure with automatic initialization step.\n\n2\n\nGLM with start= option using coefficients from prior step.\n\n\n\n\n\n\n\nThe parglm package provides much faster computations (but somewhat more unstable).\n\nlibrary(parglm)\n\nd_fit_3 = parglm(event==0 ~ poly(time, 2, raw=T)*assign, \n       family=binomial(), data=dta_c_panel, \n       start = d_fit$coefficients,\n1       control = parglm.control(method = \"FAST\", nthreads = 8L))\n\n\n1\n\nparglm() function works mostly as glm(), the parglm.control() allows some additional options for parallel computing and QR decomposition.\n\n\n\n\n\n\n\n\n\n\nUnit: seconds\n           expr    min     lq  mean median     uq   max neval\n       base GLM 0.3420 0.4550 0.473  0.484 0.4990 0.565    20\n GLM with inits 0.1430 0.1510 0.182  0.155 0.1700 0.327    20\n         PARGLM 0.0914 0.0947 0.108  0.097 0.0999 0.207    20\n\n\nEven in a small example, parglm() significantly outperforms base glm(). This will scale considerably with multiple cores and larger datasets as well.\nparglm() may be more unstable (convergence issues), but should be sufficient for most problems.\n\n\n\nComparison of glm() and parglm() results\n\n\nCoefficient\nglm()\nparglm()\n\n\n\n\n(Intercept)\n5.44292\n5.44292\n\n\npoly(time, 2, raw = T)1\n-0.00999\n-0.00999\n\n\npoly(time, 2, raw = T)2\n-0.00013\n-0.00013\n\n\nassign\n-0.28224\n-0.28224\n\n\npoly(time, 2, raw = T)1:assign\n0.07837\n0.07837\n\n\npoly(time, 2, raw = T)2:assign\n-0.00104\n-0.00104\n\n\n\n\n\nParallel computation is also an option to speed up workflow. The efficiency of this depends on how the data is setup and what steps are running in parallel. It is not efficient to hold several very large datasets in memory at once so that a CPU worker can be assigned to each. However, parglm breaks up the dataset into chunks and uses parallel processing to evaluate each subset which can be very efficient.",
    "crumbs": [
      "Appendix",
      "Computational methods for speed"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html",
    "href": "qmd/ccw_01_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "For the following, I simulated a dataset with treatment, treat or A, and treatment strategy assign which is either 1) receive treatment by time period 12, or 2) never receive treatment through follow-up. The outcome Y is a function of X, age and gender (female) in model ‘xy’, and in model ‘ay’ there is a direct causal effect of A (treat) on Y. The effect is in opposite direction to the confounders, X, which is recorded as a baseline value at time zero, and X_t which varies across time.\nSo a naive estimator, not adjusting for X should identify a positive association between treatment and outcome in model ‘xy’ but little to no association in ‘ay’, while adjustment for X and X_t should find a null association in ‘xy’ but an association in ‘ay’. This helps interpret results from the code procedures below. The sample size is 1000, so associations may not be statistically significant.\nSee: Data for full explanation of procedure for generating data.\n\n\n\nThe full design of a TTE is a not a goal of this guide, but it is essential to start with a clear idea of research question, intervention/treatment, cohort to study and statistical analysis. It it is typical to present these elements in a table and compare the idealized ‘target’ trial to the emulation in the observational data.\n\n\n\n\n\nAim\n\n\nEstimate and contrast risk of death under two different treatment strategies\n\n\n\n\n\n\n\n\nTarget Trial\n\n\nEmulation\n\n\n\n\nEligibility criteria\n\n\nTime periods, Inclusion(s); Exclusion(s);\n\n\nNote differences\n\n\n\n\nTreatment\n\n\nTwo strategies: 1) Initiate treatment in 12 months, 2) Do no initiate treatment ever\n\n\n““\n\n\n\n\nAssignment\n\n\nRandomly assigned, assigned within strata etc.\n\n\nGenerally described as assumed no confounding within strata of observed covariates, and weighted estimates are as good as randomly assigned\n\n\n\n\nOutcomes\n\n\nDeath\n\n\n““\n\n\n\n\nFollow-up\n\n\nDeath, non-adherence to treatment, or 60 time periods\n\n\n\n\n\n\nCausal contrasts\n\n\nIntention to treat; per-protocol effects weighted by inverse probability of treatment adherence\n\n\nObservational analogue of per protocol effect\n\n\n\n\nStatistical Analysis\n\n\nCumulative incidences, Kaplan-Meier or Cox regression\n\n\nPooled logistic regression with probability weighting to adjust for confounder",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#case-study",
    "href": "qmd/ccw_01_intro.html#case-study",
    "title": "Introduction",
    "section": "",
    "text": "For the following, I simulated a dataset with treatment, treat or A, and treatment strategy assign which is either 1) receive treatment by time period 12, or 2) never receive treatment through follow-up. The outcome Y is a function of X, age and gender (female) in model ‘xy’, and in model ‘ay’ there is a direct causal effect of A (treat) on Y. The effect is in opposite direction to the confounders, X, which is recorded as a baseline value at time zero, and X_t which varies across time.\nSo a naive estimator, not adjusting for X should identify a positive association between treatment and outcome in model ‘xy’ but little to no association in ‘ay’, while adjustment for X and X_t should find a null association in ‘xy’ but an association in ‘ay’. This helps interpret results from the code procedures below. The sample size is 1000, so associations may not be statistically significant.\nSee: Data for full explanation of procedure for generating data.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#target-trial-specification",
    "href": "qmd/ccw_01_intro.html#target-trial-specification",
    "title": "Introduction",
    "section": "",
    "text": "The full design of a TTE is a not a goal of this guide, but it is essential to start with a clear idea of research question, intervention/treatment, cohort to study and statistical analysis. It it is typical to present these elements in a table and compare the idealized ‘target’ trial to the emulation in the observational data.\n\n\n\n\n\nAim\n\n\nEstimate and contrast risk of death under two different treatment strategies\n\n\n\n\n\n\n\n\nTarget Trial\n\n\nEmulation\n\n\n\n\nEligibility criteria\n\n\nTime periods, Inclusion(s); Exclusion(s);\n\n\nNote differences\n\n\n\n\nTreatment\n\n\nTwo strategies: 1) Initiate treatment in 12 months, 2) Do no initiate treatment ever\n\n\n““\n\n\n\n\nAssignment\n\n\nRandomly assigned, assigned within strata etc.\n\n\nGenerally described as assumed no confounding within strata of observed covariates, and weighted estimates are as good as randomly assigned\n\n\n\n\nOutcomes\n\n\nDeath\n\n\n““\n\n\n\n\nFollow-up\n\n\nDeath, non-adherence to treatment, or 60 time periods\n\n\n\n\n\n\nCausal contrasts\n\n\nIntention to treat; per-protocol effects weighted by inverse probability of treatment adherence\n\n\nObservational analogue of per protocol effect\n\n\n\n\nStatistical Analysis\n\n\nCumulative incidences, Kaplan-Meier or Cox regression\n\n\nPooled logistic regression with probability weighting to adjust for confounder",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#cloning",
    "href": "qmd/ccw_01_intro.html#cloning",
    "title": "Introduction",
    "section": "Cloning",
    "text": "Cloning\nIt is possible to specify two or more interventions where a single person’s observed data is consistent with multiple strategies at a single point in time. Under both treatment strategies if the person does not get treatment for the first 5 periods, then up to period 12 they are faithfully adherent to both treatment strategies. At t=12, if they still are untreated they would continue to follow-up under the no treatment strategy but be non-adherent to the “treatment in 12 periods” strategy. This 12 period window we outline in our strategy is referred to as a “grace period” or “grace window”. Alternatively, if the person was assigned to “no treatment” and received treatment at any point in the 12 periods they would be censored upon initiation of treatment under the 1) “no treatment strategy” but follow-up continue under the 2) “treat within 12” strategy.\nIn a target trial emulation which employs these methods, each person’s observed data may be consistent with 1+ treatment strategies in the same time period, particularly the initial period of time. Because at baseline the person’s observed data is consistent with 1+ more strategies, the typical solution is to clone the person and assign each clone to a treatment strategy, e.g. duplicate or triplicate the person’ data and follow each clone as a unique observation.\n\n\n\n\n\n\nNote\n\n\n\nThe person could also be randomly assigned to one of the interventions (instead of cloned and assigned to both), but simulations suggest this is very statistically inefficient (much less precise estimates than cloning).\n\n\nThe cloning approach has an appealing effect of eliminating confounding at baseline (time zero). This is true because the observations in each treatment group are identical at baseline. However, as each clone is followed across time, accrual of events and censoring due to non-adherence to the assigned treatment strategy leads to divergence in the distribution of covariates. The presence of confounding by indication or causal effects of treatment will determine how these distributions change",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#natural-censoring",
    "href": "qmd/ccw_01_intro.html#natural-censoring",
    "title": "Introduction",
    "section": "Natural Censoring",
    "text": "Natural Censoring\nTo distinguish from the artificial case above, I refer to loss to follow-up as natural censoring. In this simple example, the outcome is death but in many applied examples death would be a competing event and individuals would censor on death. This would be accounted for either with a competing risk regression, Fine and Gray model etc.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#simple-example-probability-weighting-with-a-grace-period",
    "href": "qmd/ccw_01_intro.html#simple-example-probability-weighting-with-a-grace-period",
    "title": "Introduction",
    "section": "Simple example: Probability weighting with a grace period",
    "text": "Simple example: Probability weighting with a grace period\nAssume three persons are assigned to a treatment strategy as follows: Get the treatment within 2 time periods, if you don’t get it by the second period you are censored for non-adherence. The goal of weighting is to create a pseudo-population that can estimate the effect in a counterfactual world where all those assigned to treatment take treatment by the end of the grace period, and all those assigned to not take treatment do not take it. In other words, we are estimating the causal effects of treatment in a trial where there is perfect adherence to study assignment, also known as a “per protocol” effect.\n\n\n\n\n\n\nNote\n\n\n\nObserved treatments:\nA: treated at last period\nB: never treated\nC: treated at first period\n\n\n\n\n\nSimple Weighting Example\n\n\nPerson\nPeriod\nTreat\nCensor\n\n\n\n\nA\n1\n0\n0\n\n\nA\n2\n1\n0\n\n\nB\n1\n0\n0\n\n\nB\n2\n0\n1\n\n\nC\n1\n1\n0\n\n\nC\n2\n0\n0\n\n\n\n\n\nThree persons have observed data in our trial emulation. Persons A & C get the treatment and are not censored, Person B doesn’t get the treatment and is censored.\nNow adding censoring probabilities Pr(C=1) to the table:\n\n\n\nSimple Weighting Example - Censoring Probabilities\n\n\nPerson\nPeriod\nTreat\nCensor\nPr(C=1)\n\n\n\n\nA\n1\n0\n0\n0\n\n\nA\n2\n1\n0\n1/2\n\n\nB\n1\n0\n0\n0\n\n\nB\n2\n0\n1\n1/2\n\n\nC\n1\n1\n0\n0\n\n\nC\n2\n0\n0\n-\n\n\n\n\n\nBecause people have 2 periods to get treatment, the Pr(C=1) in period 1 is 0 for all persons.\n\n\n\n\n\n\nNote\n\n\n\nThis is a key point that can be confusing at first glance. But it comes down to what the treatment strategy is explicitly saying. In this case, the person has 2 time-periods to get treatment, so not getting treatment in the first period is still adherent. Therefore noone assigned to treatment is artificially censored in the first period\n\n\nThe Pr(C=1)= ½ for person A and B.\n\n\n\n\n\n\nNote\n\n\n\nWhy is it 1/2 even though there are three people (i.e. not ⅓)? This is because person C already received the treatment in period 1. They are immutably uncensored from that point forward. So the only two who can censor are persons A and B, since person B censors in period 2, the Pr(C=1) = ½.\n\n\nThe probability of not censoring, Pr(C=0) is simply 1 - Pr(C=1).\n\n\n\nSimple Weighting Example - Censoring Probabilities\n\n\nPerson\nPeriod\nTreat\nCensor\nPr(C=1)\nPr(C=0)\n\n\n\n\nA\n1\n0\n0\n0\n1\n\n\nA\n2\n1\n0\n1/2\n1/2\n\n\nB\n1\n0\n0\n0\n1\n\n\nB\n2\n0\n1\n1/2\n1/2\n\n\nC\n1\n1\n0\n0\n1\n\n\nC\n2\n0\n0\n-\n1\n\n\n\n\n\nThe unstabilized inverse probability weight is 1 / Pr(C=0); in simple terms people who are likely to censor but dont are assigned higher weights and count for more in the analysis.\n\n\n\nSimple Weighting Example - IPW\n\n\nPerson\nPeriod\nTreat\nCensor\nPr(C=1)\nPr(C=0)\nIPW\n\n\n\n\nA\n1\n0\n0\n0\n1\n1\n\n\nA\n2\n1\n0\n1/2\n1/2\n2\n\n\nB\n1\n0\n0\n0\n1\n1\n\n\nB\n2\n0\n1\n1/2\n1/2\n0\n\n\nC\n1\n1\n0\n0\n1\n1\n\n\nC\n2\n0\n0\n-\n1\n1\n\n\n\n\n\nPerson A is given a weight of 2, making up for the loss of censored person B. Person C is given a weight of 1 because they received treatment in the first period, and so couldn’t censor at the end of the grace period. Three persons, and the sum of the weights in both periods is 3.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#post-grace-period",
    "href": "qmd/ccw_01_intro.html#post-grace-period",
    "title": "Introduction",
    "section": "Post grace period",
    "text": "Post grace period\nHow does one carry the IPW weight forward after the grace period? This depends on the treatment strategy. If all that is required is initiation of treatment within the grace period, then there is no reason for censoring after the grace period. So the IPCW continues as a fixed–time-invariant–weight. However, if there were rules like “maintain treatment”, or “don’t initiate some other treatment”, then these would be applied still and so the computations should reflect those scenarios.\nIn this simple scheme, assign a weight of 1 for each time period after grace, and compute a cumulative product of the weights for each timepoint up until the end of follow-up. So person B, IPW=2, person C, IPW=1, for each subsequent time-point.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_01_intro.html#summary-of-weighting-scheme",
    "href": "qmd/ccw_01_intro.html#summary-of-weighting-scheme",
    "title": "Introduction",
    "section": "Summary of weighting scheme",
    "text": "Summary of weighting scheme\nExamples of IPW by different treatment strategies:\n\n\n\n\n\n\n\n\n\n\nTime period\nNo Intervention\nNo Treatment Ever\nNo Treatment within grace period\nTreatment by grace period\n\n\n\n\n** Prior to last grace period **\n1 / 1\n1 / P(C = 0 | X, L, T)\n1 / P(C = 0 | X, L, T)\n1 / 1\n\n\n** End of grace period **\n1 / 1\n1 / P(C = 0 | X, L, T)\n1 / P(C = 0 | X, L, T)\n1 / P(C = 0 | X, L, T)\n\n\n** After grace period**\n1 / 1\n1 / P(C = 0 | X, L, T)\n1 / 1\n1 / 1",
    "crumbs": [
      "Clone-Censor-Weight",
      "Introduction"
    ]
  },
  {
    "objectID": "qmd/ccw_03_inf.html",
    "href": "qmd/ccw_03_inf.html",
    "title": "Inference",
    "section": "",
    "text": "Several steps in the analysis make the assumptions of conventional standard errors invalid. Cloning or repeated use of persons across sequential target trial dates complicates the statistical properties of the estimand (due to correlated data), additionally the added uncertainty in estimation of probability weights must be accounted for.\nCurrently, most researchers are using bootstrapping to obtain confidence limits via percentile method. It is critical that: 1. The bootstrapping step occur prior to sequential repeated sampling, cloning or estimation of weights 2. The bootstrap sampling with replacement must be at the cluster-level (person).\nAlso consider, bootstrapping may fail to produce valid estimates in cases of matching procedures, or penalized regression procedures.[Camponovo (2015)](Abadie and Imbens 2008) Other bootstrapping failures may arise due to limited sample size or values on the boundary of a parameter space.\n\n\n\n\n\n\n\n\nNote\n\n\n\nA Bayesian bootstrap procedure is used here, see:Bootstrap methods for some notes on this.\n\n\nBootstrapping is fairly straightforward, but for specific projects the procedure may need to be modified because of computing resources. Here we execute the entire procedure in one step, but it may make sense for your project to generate a single bootstrapped data first, then run each estimation step sequentially and store the results in a file before executing the next one. This would make sense if 1) one bootstrap takes a very long time, 2) there is a decent chance that the procedure may be interrupted (like a cloud server cuts off user etc.).\nThe function generates a continuous frequency weight from a random exponent distribution with rate=1, also known as Bayesian bootstrap. This weight is used through the IPW estimation and outcome model steps.\n\nboot_ipw = function(x, point=F) {\n  \n setDT(x)\n  \n  i_list = distinct(x, id) %&gt;%\n1  mutate(prior = rexp(n=n(), rate=1),\n         freqwt = prior / sum(prior))\n\n  \n  x = inner_join(x, i_list, by = 'id')\n\n  if (point) x$freqwt = 1 # Option to get non-bootstrapped est\n  \n  d_glm_wt = glm(treat ~ poly(time, 2, raw=T) + X + X_t + age + poly(age, 2) + female, \n                 data=x[x$assign==0, ], family=binomial(),\n2                 weights = freqwt)\n  \nx$pr_treat = predict(d_glm_wt, newdata = x, type='response') \n  \nsetDT(x)\n  \nx[, ipw := fcase(\n    assign==0 & censor==0, 1 / (1-pr_treat), \n    assign==0 & censor==1, 0, \n    assign==1 & censor==1, 0, \n    assign==1 & time &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  ==12 & censor==0, 1 / (pr_treat), \n    assign==1 & time &gt; 12, 1 \n  )]\n  \nx[, marg_c0  := 1-mean(censor), by = list(assign, time)]\n\nx[, marg_ipw := fcase(\n    assign==0 & censor==0, marg_c0 / (1-pr_treat), \n    assign==0 & censor==1, 0,  \n    assign==1 & censor==1, 0,  \n    assign==1 & time &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  &lt; 12, 1,  \n    assign==1 & time == 12  & t_treat  ==12 & censor==0, marg_c0 / (pr_treat), \n    assign==1 & time &gt; 12, 1 \n  )]\n  \n  x[, `:=`(ipw = cumprod(ipw), \n                     marg_ipw = cumprod(marg_ipw)), \n               by=list(id, assign)] \n\n3  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=x[x$assign==1, ],\n                   family=binomial(), weights = ipw*freqwt)\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=x[x$assign==0, ],\n                     family=binomial(), weights = ipw*freqwt)\n  \n  x$pr_1 = predict(d_glm_pe_1, newdata = x, \n                             type='response') \n  x$pr_0 = predict(d_glm_pe_0, newdata = x, \n                             type='response') \n  \n  x[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  x[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = x %&gt;% \n    group_by(time) %&gt;% \n4    summarize(pr_ev_1 = weighted.mean(1-pr_cum_1, freqwt),\n              pr_ev_0 = weighted.mean(1-pr_cum_0, freqwt),\n              .groups = 'drop') %&gt;%\n    ungroup %&gt;% \n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res)\n}\n\n\n1\n\nThe Bayesian bootstrap is a continuous measure of the number of times (versus a count) that observation would have appeared in a dataset resampled with equal size and replacement (i.e. on average would appear 1 time).\n\n2\n\nNote the freqwt used in the glm() procedure.\n\n3\n\nNote the freqwt*ipw used in the glm() procedure.\n\n4\n\nNote the use of weighted.mean() and freqwt.\n\n\n\n\nThe function can be run iteratively, with replicate() or purrr/furrr or many other options.\n\nlibrary(furrr)\nboots = 20\n1plan(multisession, workers = 10)\n\n2d_pe = boot_ipw(dta_c_panel, point=T)\n\nd_boot = future_map(1:boots, \n                        function(x) boot_ipw(dta_c_panel), \n3                        .options = furrr_options(seed=T))\n\n\n1\n\nSet 10 CPU workers for parallel computation\n\n2\n\nObtain point estimates first (no bootstrapping)\n\n3\n\nRun bootstraps in parallel, note the furrr_options(seed=T) is important because of the RNG functions and is needed to ensure reproducibility and some funny quirks of generating random numbers in parallel. See furrr for explanation.\n\n\n\n\nAfter the bootstraps, summarizing is straightforward as well:\n\n1lci_q = 0.025\nuci_q = 0.975\n\nd_summ = d_boot %&gt;%\n  bind_rows(., .id = 'boot') %&gt;%\n  group_by(time) %&gt;%\n  summarize(n_boots = n(),\n            pr_ev_1_lci = quantile(pr_ev_1, lci_q),\n            pr_ev_1_uci = quantile(pr_ev_1, uci_q),\n            pr_ev_0_lci = quantile(pr_ev_0, lci_q),\n            pr_ev_0_uci = quantile(pr_ev_0, uci_q),\n            cid_lci = quantile(cid, lci_q),\n            cid_uci = quantile(cid, uci_q),\n            cir_lci = quantile(cir, lci_q),\n            cir_uci = quantile(cir, uci_q)\n            ) %&gt;%\n2  inner_join(., d_pe, by='time')\n\n\n1\n\npercentile intervals are specified at the conventional points (95%). However any of the typical bootstrapped confidence procedures can be done at this point.\n\n2\n\nJoining the computed intervals back with point estimates.\n\n\n\n\n\n\n\nIPW-adjusted cumulative incidences, differences and relative risks\n\n\ntime\nAssign=1\nAssign=0\nCID\nRR\n\n\n\n\n12.00\n0.04 (0.04, 0.05)\n0.06 (0.05, 0.06)\n-0.01 (-0.02, -0.01)\n0.77 (0.64, 0.88)\n\n\n24.00\n0.08 (0.06, 0.11)\n0.12 (0.11, 0.13)\n-0.04 (-0.06, -0.01)\n0.70 (0.47, 0.90)\n\n\n60.00\n0.15 (0.09, 0.23)\n0.36 (0.34, 0.38)\n-0.22 (-0.28, -0.14)\n0.40 (0.25, 0.61)\n\n\n\n\n\n\n\n\nUnder development…",
    "crumbs": [
      "Clone-Censor-Weight",
      "Inference"
    ]
  },
  {
    "objectID": "qmd/ccw_03_inf.html#bootstrapping",
    "href": "qmd/ccw_03_inf.html#bootstrapping",
    "title": "Inference",
    "section": "",
    "text": "Note\n\n\n\nA Bayesian bootstrap procedure is used here, see:Bootstrap methods for some notes on this.\n\n\nBootstrapping is fairly straightforward, but for specific projects the procedure may need to be modified because of computing resources. Here we execute the entire procedure in one step, but it may make sense for your project to generate a single bootstrapped data first, then run each estimation step sequentially and store the results in a file before executing the next one. This would make sense if 1) one bootstrap takes a very long time, 2) there is a decent chance that the procedure may be interrupted (like a cloud server cuts off user etc.).\nThe function generates a continuous frequency weight from a random exponent distribution with rate=1, also known as Bayesian bootstrap. This weight is used through the IPW estimation and outcome model steps.\n\nboot_ipw = function(x, point=F) {\n  \n setDT(x)\n  \n  i_list = distinct(x, id) %&gt;%\n1  mutate(prior = rexp(n=n(), rate=1),\n         freqwt = prior / sum(prior))\n\n  \n  x = inner_join(x, i_list, by = 'id')\n\n  if (point) x$freqwt = 1 # Option to get non-bootstrapped est\n  \n  d_glm_wt = glm(treat ~ poly(time, 2, raw=T) + X + X_t + age + poly(age, 2) + female, \n                 data=x[x$assign==0, ], family=binomial(),\n2                 weights = freqwt)\n  \nx$pr_treat = predict(d_glm_wt, newdata = x, type='response') \n  \nsetDT(x)\n  \nx[, ipw := fcase(\n    assign==0 & censor==0, 1 / (1-pr_treat), \n    assign==0 & censor==1, 0, \n    assign==1 & censor==1, 0, \n    assign==1 & time &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  ==12 & censor==0, 1 / (pr_treat), \n    assign==1 & time &gt; 12, 1 \n  )]\n  \nx[, marg_c0  := 1-mean(censor), by = list(assign, time)]\n\nx[, marg_ipw := fcase(\n    assign==0 & censor==0, marg_c0 / (1-pr_treat), \n    assign==0 & censor==1, 0,  \n    assign==1 & censor==1, 0,  \n    assign==1 & time &lt; 12, 1, \n    assign==1 & time == 12  & t_treat  &lt; 12, 1,  \n    assign==1 & time == 12  & t_treat  ==12 & censor==0, marg_c0 / (pr_treat), \n    assign==1 & time &gt; 12, 1 \n  )]\n  \n  x[, `:=`(ipw = cumprod(ipw), \n                     marg_ipw = cumprod(marg_ipw)), \n               by=list(id, assign)] \n\n3  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=x[x$assign==1, ],\n                   family=binomial(), weights = ipw*freqwt)\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=x[x$assign==0, ],\n                     family=binomial(), weights = ipw*freqwt)\n  \n  x$pr_1 = predict(d_glm_pe_1, newdata = x, \n                             type='response') \n  x$pr_0 = predict(d_glm_pe_0, newdata = x, \n                             type='response') \n  \n  x[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  x[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = x %&gt;% \n    group_by(time) %&gt;% \n4    summarize(pr_ev_1 = weighted.mean(1-pr_cum_1, freqwt),\n              pr_ev_0 = weighted.mean(1-pr_cum_0, freqwt),\n              .groups = 'drop') %&gt;%\n    ungroup %&gt;% \n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res)\n}\n\n\n1\n\nThe Bayesian bootstrap is a continuous measure of the number of times (versus a count) that observation would have appeared in a dataset resampled with equal size and replacement (i.e. on average would appear 1 time).\n\n2\n\nNote the freqwt used in the glm() procedure.\n\n3\n\nNote the freqwt*ipw used in the glm() procedure.\n\n4\n\nNote the use of weighted.mean() and freqwt.\n\n\n\n\nThe function can be run iteratively, with replicate() or purrr/furrr or many other options.\n\nlibrary(furrr)\nboots = 20\n1plan(multisession, workers = 10)\n\n2d_pe = boot_ipw(dta_c_panel, point=T)\n\nd_boot = future_map(1:boots, \n                        function(x) boot_ipw(dta_c_panel), \n3                        .options = furrr_options(seed=T))\n\n\n1\n\nSet 10 CPU workers for parallel computation\n\n2\n\nObtain point estimates first (no bootstrapping)\n\n3\n\nRun bootstraps in parallel, note the furrr_options(seed=T) is important because of the RNG functions and is needed to ensure reproducibility and some funny quirks of generating random numbers in parallel. See furrr for explanation.\n\n\n\n\nAfter the bootstraps, summarizing is straightforward as well:\n\n1lci_q = 0.025\nuci_q = 0.975\n\nd_summ = d_boot %&gt;%\n  bind_rows(., .id = 'boot') %&gt;%\n  group_by(time) %&gt;%\n  summarize(n_boots = n(),\n            pr_ev_1_lci = quantile(pr_ev_1, lci_q),\n            pr_ev_1_uci = quantile(pr_ev_1, uci_q),\n            pr_ev_0_lci = quantile(pr_ev_0, lci_q),\n            pr_ev_0_uci = quantile(pr_ev_0, uci_q),\n            cid_lci = quantile(cid, lci_q),\n            cid_uci = quantile(cid, uci_q),\n            cir_lci = quantile(cir, lci_q),\n            cir_uci = quantile(cir, uci_q)\n            ) %&gt;%\n2  inner_join(., d_pe, by='time')\n\n\n1\n\npercentile intervals are specified at the conventional points (95%). However any of the typical bootstrapped confidence procedures can be done at this point.\n\n2\n\nJoining the computed intervals back with point estimates.\n\n\n\n\n\n\n\nIPW-adjusted cumulative incidences, differences and relative risks\n\n\ntime\nAssign=1\nAssign=0\nCID\nRR\n\n\n\n\n12.00\n0.04 (0.04, 0.05)\n0.06 (0.05, 0.06)\n-0.01 (-0.02, -0.01)\n0.77 (0.64, 0.88)\n\n\n24.00\n0.08 (0.06, 0.11)\n0.12 (0.11, 0.13)\n-0.04 (-0.06, -0.01)\n0.70 (0.47, 0.90)\n\n\n60.00\n0.15 (0.09, 0.23)\n0.36 (0.34, 0.38)\n-0.22 (-0.28, -0.14)\n0.40 (0.25, 0.61)",
    "crumbs": [
      "Clone-Censor-Weight",
      "Inference"
    ]
  },
  {
    "objectID": "qmd/ccw_03_inf.html#influence-statistics",
    "href": "qmd/ccw_03_inf.html#influence-statistics",
    "title": "Inference",
    "section": "",
    "text": "Under development…",
    "crumbs": [
      "Clone-Censor-Weight",
      "Inference"
    ]
  },
  {
    "objectID": "qmd/ccw_05_rep.html",
    "href": "qmd/ccw_05_rep.html",
    "title": "Presentation",
    "section": "",
    "text": "In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort, or comparison of those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals) but will provide a standardized mean difference.\nSome special considerations are needed when presenting target trial emulations.\n\n\nThe cloning presents an interesting problem for a “Table 1” because the groups are identical at baseline. Artificial censoring may change the distribution of covariates over time if censoring is correlated with these covariates. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time point. The end of the grace period is a good standard choice. It makes sense to present the time-invariant covariates as well as time-varying covariates.\nAssume a simple example where the design of the study includes a baseline period, “time zero” and two treatment strategies, with a grace period or interval where one strategy allows some time for treatment to occur.There being two main periods: 1) Baseline time zero, 2) End of the grace period.\nHere is one approach:\n\nPresent the baseline characteristics for everyone (prior to cloning) at time zero.\nPresent the characteristics for both treatment groups at the end of the grace period.\n\nThis would lead to a “Table 1” with three columns.\n\n\n\nMany R packages will construct a “Table 1” for users. I manually code below for transparency.\n\n# Many packages can do this for you, I write out manually for transparency\n\nd_cbase = dta_c_panel %&gt;%\n  # Take one obs for the baseline column\n  dplyr::filter(time==1 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Sex, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_c0 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==0 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n  \nd_c1 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Sex, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_tab1 = bind_cols(Variables = c('N', 'X, mean (SD)', 'Xt, mean (SD)', 'Age, years (SD)', 'Sex, female'),\n          `Baseline` = d_cbase, `Assign=0` = d_c0, `Assign=1` = d_c1) \n\n\n\n\nTable 1. Example of presentation of characteristics of a CCW Cohort\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\n\n\n\n\nN\n2,000\n1,525\n383\n\n\nX, mean (SD)\n0.05 (0.21)\n0.03 (0.18)\n0.11 (0.32)\n\n\nXt, mean (SD)\n0.02 (0.15)\n0.03 (0.18)\n0.03 (0.17)\n\n\nAge, years (SD)\n75 (10)\n75 (9)\n75 (10)\n\n\nSex, female\n1,324 (66.2)\n1,003 (65.8)\n263 (68.7)\n\n\n\n\n\n\n\n\nMost researchers would provide the naive differences between groups, and then present some adjusted difference. Most commonly the standardized mean differences (aSMD) or a P-value from a univariate test (not recommended).\nSo you could add one column for the SMD to the “Table 1”, but there are also other approaches such as reporting Mahalanobis distance.\n\n\n\nd_smd = dta_c_panel %&gt;%\n  dplyr::filter(time==12 & censor==0) %&gt;%\n  summarize(`X, mean (SD)` = (mean(X[assign==1]) - mean(X[assign==0])) / sd(X),\n            `Xt, mean (SD)` = (mean(X_t[assign==1]) - mean(X_t[assign==0])) / sd(X_t),\n            `Age, years (SD)`  = (mean(age[assign==1]) - mean(age[assign==0])) / sd(age),\n            `Sex, female` = (mean(female[assign==1]) - mean(female[assign==0])) / sd(female)\n         ) %&gt;%\n  t() %&gt;%\n  as_tibble(rownames = 'Variable') %&gt;%\n  rename(`aSMD` = V1)\n\nd_tab1 = left_join(d_tab1, d_smd, by=c('Variables' = 'Variable'))\n\n\n# Compute pooled covariance matrix\ncov_pooled &lt;- (var(dta_c_panel[dta_c_panel$assign==1, c('X', 'X_t', 'age', 'female')]) + \n                 var(dta_c_panel[dta_c_panel$assign==0, c('X', 'X_t', 'age', 'female')])) / 2\n\n# Compute Mahalanobis distance\nmahal_dist &lt;- mahalanobis(colMeans(dta_c_panel[dta_c_panel$assign==1, \n                                               c('X', 'X_t', 'age', 'female')]), \n                          center = colMeans(dta_c_panel[dta_c_panel$assign==0, \n                                                        c('X', 'X_t', 'age', 'female')]), \n                          cov = cov_pooled)\n\n\n\n\n\n\n\nTable 1 + aSMD (unweighted)\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\n\n\n\n\nN\n2,000\n1,525\n383\nNA\n\n\nX, mean (SD)\n0.05 (0.21)\n0.03 (0.18)\n0.11 (0.32)\n0.359\n\n\nXt, mean (SD)\n0.02 (0.15)\n0.03 (0.18)\n0.03 (0.17)\n-0.030\n\n\nAge, years (SD)\n75 (10)\n75 (9)\n75 (10)\n0.020\n\n\nSex, female\n1,324 (66.2)\n1,003 (65.8)\n263 (68.7)\n0.061\n\n\n\n\n\nNote: Mahalanobis distance between groups, Naive: 0.049\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee Estimators for full description of weight estimation\n\n\n\n\n\n\n\n\n\n\n\nTable 1 + aSMD (weighted)\n\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\nIPW aSMD\n\n\n\n\nN\n2,000\n1,525\n383\nNA\nNA\n\n\nX, mean (SD)\n0.05 (0.21)\n0.03 (0.18)\n0.11 (0.32)\n0.359\n0.070\n\n\nXt, mean (SD)\n0.02 (0.15)\n0.03 (0.18)\n0.03 (0.17)\n-0.030\n-0.080\n\n\nAge, years (SD)\n75 (10)\n75 (9)\n75 (10)\n0.020\n0.102\n\n\nSex, female\n1,324 (66.2)\n1,003 (65.8)\n263 (68.7)\n0.061\n0.245\n\n\n\n\n\nNote. Mahalanobis distance between groups, Naive: 0.049, IPW: 0.082\n\n\n\n\nThese differences can also be presented in a modified “Love” plot, with panels for each period of follow-up. It doesn’t really make sense to do one at baseline because the differences will be small/none. But we can pick a few follow-up periods to evaluate:\n\n# aSMD Figure ----\n  i_SMD_grps = c(12, 24, 60)\n\n  d_time = dta_c_panel %&gt;%\n    dplyr::filter(time %in% c(i_SMD_grps) & censor==0) %&gt;%\n    nest(.by = time) \n  \n  d_time$SMD_naive = map(d_time$data, ~f_cpt_dist(.))\n  d_time$SMD_ipw = map(d_time$data, ~f_cpt_dist_wt(.))\n  \n  gg_bal = select(d_time, time, SMD_naive, SMD_ipw) %&gt;%\n    unnest() %&gt;%\n    select(-Variable1) %&gt;%\n    rename(`Naive` = SMD,\n           `IPW` = SMD1) %&gt;%\n      pivot_longer(cols = c('Naive', 'IPW'), \n                   names_to = 'Model', values_to = 'SMD') %&gt;% \n      mutate(flag = if_else(abs(SMD)&gt;0.1, cbbPalette[1], cbbPalette[2])) %&gt;%\n      ggplot(., aes(y = Variable, x = SMD, group = Model)) +\n      geom_point(aes(shape=Model, color = flag), size=1.8) +\n      geom_vline(xintercept = 0, linetype=2) +\n      geom_vline(xintercept = 0.2, linetype=2, color=cbbPalette[3]) +\n      scale_color_identity() + \n      facet_grid(cols = vars(time)) +\n      theme_classic() +\n      labs(x = 'SMD', y = 'Variable',\n           caption = 'M-dist = Mahalanobis distance for overall groups covariate means')\n  \n  gg_bal\n\n\n\n\nFigure 1: Baseline and Time-varying Differences by Treatment Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can see the weighted between group differences for X and X_t are significantly reduced by the IPW. However, caution is advised in interpretation of the differences. If the treatment/exposure of interest has a causal relationship with the outcome (or some other censoring mechanism besides the artificial censoring the weights account for), then you might expect there to be differences between groups like gender and age. This is because the groups are conditional on remaining alive and uncensored at each time-point, which may occur after treatment/exposure. If other covariates also cause the outcome, you would expect distributions to diverge over time.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Presentation"
    ]
  },
  {
    "objectID": "qmd/ccw_05_rep.html#cloning-grace-period",
    "href": "qmd/ccw_05_rep.html#cloning-grace-period",
    "title": "Presentation",
    "section": "",
    "text": "The cloning presents an interesting problem for a “Table 1” because the groups are identical at baseline. Artificial censoring may change the distribution of covariates over time if censoring is correlated with these covariates. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time point. The end of the grace period is a good standard choice. It makes sense to present the time-invariant covariates as well as time-varying covariates.\nAssume a simple example where the design of the study includes a baseline period, “time zero” and two treatment strategies, with a grace period or interval where one strategy allows some time for treatment to occur.There being two main periods: 1) Baseline time zero, 2) End of the grace period.\nHere is one approach:\n\nPresent the baseline characteristics for everyone (prior to cloning) at time zero.\nPresent the characteristics for both treatment groups at the end of the grace period.\n\nThis would lead to a “Table 1” with three columns.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Presentation"
    ]
  },
  {
    "objectID": "qmd/ccw_05_rep.html#table-1",
    "href": "qmd/ccw_05_rep.html#table-1",
    "title": "Presentation",
    "section": "",
    "text": "Many R packages will construct a “Table 1” for users. I manually code below for transparency.\n\n# Many packages can do this for you, I write out manually for transparency\n\nd_cbase = dta_c_panel %&gt;%\n  # Take one obs for the baseline column\n  dplyr::filter(time==1 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Sex, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_c0 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==0 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n  \nd_c1 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Sex, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_tab1 = bind_cols(Variables = c('N', 'X, mean (SD)', 'Xt, mean (SD)', 'Age, years (SD)', 'Sex, female'),\n          `Baseline` = d_cbase, `Assign=0` = d_c0, `Assign=1` = d_c1) \n\n\n\n\nTable 1. Example of presentation of characteristics of a CCW Cohort\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\n\n\n\n\nN\n2,000\n1,525\n383\n\n\nX, mean (SD)\n0.05 (0.21)\n0.03 (0.18)\n0.11 (0.32)\n\n\nXt, mean (SD)\n0.02 (0.15)\n0.03 (0.18)\n0.03 (0.17)\n\n\nAge, years (SD)\n75 (10)\n75 (9)\n75 (10)\n\n\nSex, female\n1,324 (66.2)\n1,003 (65.8)\n263 (68.7)",
    "crumbs": [
      "Clone-Censor-Weight",
      "Presentation"
    ]
  },
  {
    "objectID": "qmd/ccw_05_rep.html#presentation-of-weighted-differences",
    "href": "qmd/ccw_05_rep.html#presentation-of-weighted-differences",
    "title": "Presentation",
    "section": "",
    "text": "Most researchers would provide the naive differences between groups, and then present some adjusted difference. Most commonly the standardized mean differences (aSMD) or a P-value from a univariate test (not recommended).\nSo you could add one column for the SMD to the “Table 1”, but there are also other approaches such as reporting Mahalanobis distance.\n\n\n\nd_smd = dta_c_panel %&gt;%\n  dplyr::filter(time==12 & censor==0) %&gt;%\n  summarize(`X, mean (SD)` = (mean(X[assign==1]) - mean(X[assign==0])) / sd(X),\n            `Xt, mean (SD)` = (mean(X_t[assign==1]) - mean(X_t[assign==0])) / sd(X_t),\n            `Age, years (SD)`  = (mean(age[assign==1]) - mean(age[assign==0])) / sd(age),\n            `Sex, female` = (mean(female[assign==1]) - mean(female[assign==0])) / sd(female)\n         ) %&gt;%\n  t() %&gt;%\n  as_tibble(rownames = 'Variable') %&gt;%\n  rename(`aSMD` = V1)\n\nd_tab1 = left_join(d_tab1, d_smd, by=c('Variables' = 'Variable'))\n\n\n# Compute pooled covariance matrix\ncov_pooled &lt;- (var(dta_c_panel[dta_c_panel$assign==1, c('X', 'X_t', 'age', 'female')]) + \n                 var(dta_c_panel[dta_c_panel$assign==0, c('X', 'X_t', 'age', 'female')])) / 2\n\n# Compute Mahalanobis distance\nmahal_dist &lt;- mahalanobis(colMeans(dta_c_panel[dta_c_panel$assign==1, \n                                               c('X', 'X_t', 'age', 'female')]), \n                          center = colMeans(dta_c_panel[dta_c_panel$assign==0, \n                                                        c('X', 'X_t', 'age', 'female')]), \n                          cov = cov_pooled)\n\n\n\n\n\n\n\nTable 1 + aSMD (unweighted)\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\n\n\n\n\nN\n2,000\n1,525\n383\nNA\n\n\nX, mean (SD)\n0.05 (0.21)\n0.03 (0.18)\n0.11 (0.32)\n0.359\n\n\nXt, mean (SD)\n0.02 (0.15)\n0.03 (0.18)\n0.03 (0.17)\n-0.030\n\n\nAge, years (SD)\n75 (10)\n75 (9)\n75 (10)\n0.020\n\n\nSex, female\n1,324 (66.2)\n1,003 (65.8)\n263 (68.7)\n0.061\n\n\n\n\n\nNote: Mahalanobis distance between groups, Naive: 0.049\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee Estimators for full description of weight estimation\n\n\n\n\n\n\n\n\n\n\n\nTable 1 + aSMD (weighted)\n\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\nIPW aSMD\n\n\n\n\nN\n2,000\n1,525\n383\nNA\nNA\n\n\nX, mean (SD)\n0.05 (0.21)\n0.03 (0.18)\n0.11 (0.32)\n0.359\n0.070\n\n\nXt, mean (SD)\n0.02 (0.15)\n0.03 (0.18)\n0.03 (0.17)\n-0.030\n-0.080\n\n\nAge, years (SD)\n75 (10)\n75 (9)\n75 (10)\n0.020\n0.102\n\n\nSex, female\n1,324 (66.2)\n1,003 (65.8)\n263 (68.7)\n0.061\n0.245\n\n\n\n\n\nNote. Mahalanobis distance between groups, Naive: 0.049, IPW: 0.082",
    "crumbs": [
      "Clone-Censor-Weight",
      "Presentation"
    ]
  },
  {
    "objectID": "qmd/ccw_05_rep.html#figure-of-differences-across-time",
    "href": "qmd/ccw_05_rep.html#figure-of-differences-across-time",
    "title": "Presentation",
    "section": "",
    "text": "These differences can also be presented in a modified “Love” plot, with panels for each period of follow-up. It doesn’t really make sense to do one at baseline because the differences will be small/none. But we can pick a few follow-up periods to evaluate:\n\n# aSMD Figure ----\n  i_SMD_grps = c(12, 24, 60)\n\n  d_time = dta_c_panel %&gt;%\n    dplyr::filter(time %in% c(i_SMD_grps) & censor==0) %&gt;%\n    nest(.by = time) \n  \n  d_time$SMD_naive = map(d_time$data, ~f_cpt_dist(.))\n  d_time$SMD_ipw = map(d_time$data, ~f_cpt_dist_wt(.))\n  \n  gg_bal = select(d_time, time, SMD_naive, SMD_ipw) %&gt;%\n    unnest() %&gt;%\n    select(-Variable1) %&gt;%\n    rename(`Naive` = SMD,\n           `IPW` = SMD1) %&gt;%\n      pivot_longer(cols = c('Naive', 'IPW'), \n                   names_to = 'Model', values_to = 'SMD') %&gt;% \n      mutate(flag = if_else(abs(SMD)&gt;0.1, cbbPalette[1], cbbPalette[2])) %&gt;%\n      ggplot(., aes(y = Variable, x = SMD, group = Model)) +\n      geom_point(aes(shape=Model, color = flag), size=1.8) +\n      geom_vline(xintercept = 0, linetype=2) +\n      geom_vline(xintercept = 0.2, linetype=2, color=cbbPalette[3]) +\n      scale_color_identity() + \n      facet_grid(cols = vars(time)) +\n      theme_classic() +\n      labs(x = 'SMD', y = 'Variable',\n           caption = 'M-dist = Mahalanobis distance for overall groups covariate means')\n  \n  gg_bal\n\n\n\n\nFigure 1: Baseline and Time-varying Differences by Treatment Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can see the weighted between group differences for X and X_t are significantly reduced by the IPW. However, caution is advised in interpretation of the differences. If the treatment/exposure of interest has a causal relationship with the outcome (or some other censoring mechanism besides the artificial censoring the weights account for), then you might expect there to be differences between groups like gender and age. This is because the groups are conditional on remaining alive and uncensored at each time-point, which may occur after treatment/exposure. If other covariates also cause the outcome, you would expect distributions to diverge over time.",
    "crumbs": [
      "Clone-Censor-Weight",
      "Presentation"
    ]
  },
  {
    "objectID": "qmd/NEWS.html",
    "href": "qmd/NEWS.html",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "",
    "text": "Complete rework of website flow,\nAdded sections to further explain survival analysis methods\nReworked data to include multiple examples"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.7-01052026",
    "href": "qmd/NEWS.html#version-0.7-01052026",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "",
    "text": "Complete rework of website flow,\nAdded sections to further explain survival analysis methods\nReworked data to include multiple examples"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.6-07102025",
    "href": "qmd/NEWS.html#version-0.6-07102025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.6 07/10/2025",
    "text": "Version 0.6 07/10/2025\n\nIncluded Bayesian bootstrap\nAdded diagnostic section, and moved some of the old estimation steps to this section"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.5-05-22-2025",
    "href": "qmd/NEWS.html#version-0.5-05-22-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.5 05-22-2025",
    "text": "Version 0.5 05-22-2025\n\nTutorial appears to work as expected, some edits to text and code to be more concise"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.3-05-21-2025",
    "href": "qmd/NEWS.html#version-0.3-05-21-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.3 05-21-2025",
    "text": "Version 0.3 05-21-2025\n\nRevamped synthetic data for time-varying confounding\nAdded cox models to estimator page\nSwitched to color-blind friendly color-scheme"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.2-05-13-2025",
    "href": "qmd/NEWS.html#version-0.2-05-13-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.2 05-13-2025",
    "text": "Version 0.2 05-13-2025\n\nAdded Parallel computation step to Advanced Topics page\nSetup for the inference page (WIP)"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.1-05-12-2025",
    "href": "qmd/NEWS.html#version-0.1-05-12-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.1 05-12-2025",
    "text": "Version 0.1 05-12-2025\n\nSynthetic data set-up for observed effect by treatment due to confounding; no effect after appropriate adjustment\nEstimator file complete; additional edits for clarity and alternative approaches\nInference file TBD\nAdvanced file discusses some bootstrap and computation approaches\nAppendix TBD"
  },
  {
    "objectID": "qmd/NEWS.html#version-0.0.1-02-19-2025",
    "href": "qmd/NEWS.html#version-0.0.1-02-19-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.0.1 02-19-2025",
    "text": "Version 0.0.1 02-19-2025\nProject set-up"
  },
  {
    "objectID": "qmd/surv_01_kaplan.html",
    "href": "qmd/surv_01_kaplan.html",
    "title": "Kaplan-Meier Estimator",
    "section": "",
    "text": "A brief example of applying a simple Kaplan-Meier estimator to CCW data.",
    "crumbs": [
      "Survival Analysis Background",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "qmd/surv_01_kaplan.html#data",
    "href": "qmd/surv_01_kaplan.html#data",
    "title": "Kaplan-Meier Estimator",
    "section": "Data",
    "text": "Data\n\n\nRows: 4,000\nColumns: 11\n$ model     &lt;chr&gt; \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", \"ay\", …\n$ id        &lt;int&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10…\n$ assign    &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, …\n$ t_treat   &lt;dbl&gt; Inf, Inf, 3, 3, 44, 44, 22, 22, 48, 48, Inf, Inf, 10, 10, In…\n$ t_outcome &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n$ t_censor  &lt;dbl&gt; Inf, 12, 3, Inf, 44, 12, 22, 12, 48, 12, Inf, 12, 10, Inf, I…\n$ time      &lt;dbl&gt; 60, 12, 3, 60, 44, 12, 22, 12, 48, 12, 60, 12, 10, 60, 54, 1…\n$ event     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ X         &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ female    &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, …\n$ age       &lt;dbl&gt; 70.0, 70.0, 76.3, 76.3, 74.2, 74.2, 83.9, 83.9, 76.2, 76.2, …\n\n\nThis example uses model ‘ay’, meaning A causes Y but confounding exists (Data). Assign = {0, 1} , where 1 means initiate treatment by period 12, and so many observations have a censoring time of 12 if they did not initiate treatment. Each person id has two observations, one under each assignment strategy. We will disregard X, female and age covariates at this step. Note that this dataset is not expanded to person-period, that is unnecessary in this case.",
    "crumbs": [
      "Survival Analysis Background",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "qmd/surv_01_kaplan.html#kaplan-meier-estimator",
    "href": "qmd/surv_01_kaplan.html#kaplan-meier-estimator",
    "title": "Kaplan-Meier Estimator",
    "section": "Kaplan-Meier Estimator",
    "text": "Kaplan-Meier Estimator\nThe Kaplan-Meier (KM) estimator will estimate the instantaneous hazard (event rate for some time among those at risk up to that time). I generally will estimate the cumulative incidence for many of my applied projects.\n\nDefinitions and Notation\nThe outcome need not be death, but often is, and individuals must be alive to experience events, censoring etc. so most of the terminology and notation refer to “survival”.\nThe letter T/t is typically used to denote time, where T may refer to total time observed, and t an individual time-period, \\(T = {1, ..., t}\\). Since time is usually discretely defined, for example as days, then \\(t\\) can be thought of as an interval, \\(t=1\\) meaning from the end of the prior interval (day 0) through the next (but not including) interval, day 2. \\(T=t\\) would mean a event or censor time occurred in interval \\(t\\). A binary indicator for whether the event occured \\(E = {0, 1}\\).\nSurvival probability, \\(S(t) = Pr(T&gt;t)\\), is the probability of survival (event-free) beyond time-point t.\nCumulative incidence, \\(R(t) = Pr(T&lt;=t)\\), is the 1-probability of survival, the probability the event occurred up to and including time-point t.\nThe Kaplan-Meier estimator is the product of the individual survival probabilities across T.\n\\(S(t) = \\prod_{t_i &lt;= t}(1-/\\frac{d_i}{n_i})\\)\nThe terms are related to hazard function, \\(S(t) = 1 – R(t)\\). \\(h(t)=R(t)/S(t)\\), \\(H(t) = -log[S(t)]\\) and \\(S(t) = e –H(t)\\).",
    "crumbs": [
      "Survival Analysis Background",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "qmd/surv_01_kaplan.html#estimation",
    "href": "qmd/surv_01_kaplan.html#estimation",
    "title": "Kaplan-Meier Estimator",
    "section": "Estimation",
    "text": "Estimation\nThe KM estimator is available in the base R survival package, the function is survfit(). I complete the estimator, and then do some post-estimation steps to compute the risk differences and relative differences between groups.\n\n  d_mods = broom::tidy(\n1    survfit(Surv(time, event) ~ assign, data=dta_c_person)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n2    mutate(pr_ev = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n3    pivot_wider(id_cols = c(time),\n                names_from = assign,\n                values_from = c(pr_ev, pr_s,\n                                n.risk, n.event)) %&gt;%\n4    mutate(cir = pr_ev_1 / pr_ev_0,\n           cid = (pr_ev_1 - pr_ev_0))\n\n\n1\n\nNaive estimator, K-M estimator, assignment is by clone (not person). t_clone is the follow-up time for each clone, taking into account artificial censoring.\n\n2\n\nestimate from the model is the survival probability, so probability of event is 1 - estimate\n\n3\n\nData is in long form, so one column per group with cumulative incidence/survival\n\n4\n\nEstimands, cir is ratio analogous to relative risk, and cid is analogous to risk difference",
    "crumbs": [
      "Survival Analysis Background",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "qmd/surv_01_kaplan.html#summarize-km-estimator",
    "href": "qmd/surv_01_kaplan.html#summarize-km-estimator",
    "title": "Kaplan-Meier Estimator",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator\nThe KM estimator is usually presented in figures of cumulative incidence or event-free survival probability. Often the figure is combined with tables to illustrate follow-up, events or censoring across time.\n\nd_gg = ggsurvplot(survfit(Surv(time, event) ~ assign, data=dta_c_person),\n                            data=dta_c_person,\n                            fun = 'event',\n                            xlim = c(1, 60),\n                            xlab = 'Follow-up',\n                            break.time.by=6,\n                            palette = cbbPalette,\n                            linetype = 'strata',\n                            censor=F,\n                            cumevents=T,\n                            conf.int=F, pval=F,\n                            risk.table=T, risk.table.col = 'strata',\n                            risk.table.height=0.25, \n                            ggtheme = theme_bw())\n\nd_gg\n\n\n\n\nA causes Y\n\n\n\nFigure 1. Naive Kaplan-Meier Estimator\n\nDescription. Top panel - cumulative event incidence, stratified by strategy assignment. Middle panel - the number at risk (alive, uncensored) at the start of each follow-up period. Bottom panel - Cumulative number of events.",
    "crumbs": [
      "Survival Analysis Background",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "qmd/surv_01_kaplan.html#why-cant-we-use-this-estimator-for-ccw",
    "href": "qmd/surv_01_kaplan.html#why-cant-we-use-this-estimator-for-ccw",
    "title": "Kaplan-Meier Estimator",
    "section": "Why can’t we use this estimator for CCW?",
    "text": "Why can’t we use this estimator for CCW?\nThe primary issue is that because this non-parametric estimator treats time as a continuous random variable, you cannot account for what happens within intervals/periods of time. You must either 1) conduct time-dependent analyses, where you expand the dataset and record start/stop times for each interval, or 2) move on to parametric discrete time analyses, where you also model intervals of time. This is an important step for CCW analyses, because most applied examples are describing a protocol or treatment intervention which may be implemented across a period of time (‘grace period’), and so important confounders could exist between the baseline period and initiation of treatment. The KM estimator cannot account for this.",
    "crumbs": [
      "Survival Analysis Background",
      "Kaplan-Meier Estimator"
    ]
  }
]