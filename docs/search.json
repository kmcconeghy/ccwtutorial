[
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "Version 0.0.1 02-19-2025",
    "section": "",
    "text": "Version 0.0.1 02-19-2025\nProject set-up"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#versions",
    "href": "about.html#versions",
    "title": "About",
    "section": "Versions",
    "text": "Versions\n\nVersion 1.0, established 04.24.2024"
  },
  {
    "objectID": "04_bsnotes.html",
    "href": "04_bsnotes.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Inferential statistics\nSeveral steps in the analysis makes the assumptions of conventional standard errors invalid. The use of cloning or repeated use of persons across sequential target trial dates complicates the statistical properties of the estimand (due to correlated data), additionally the uncertainty in estimation of probability weights should be considered. Currently, most researchers are using bootstrapping to obtain confidence limits via percentile method. It is critical that 1) the bootstrapping step occur prior to sequential repeated sampling, cloning or estimation of weights, 2) the bootstrap sampling with replacement must be at the cluster-level (person). Also consider, bootstrapping may fail to produce valid estimates in cases of matching procedures, or restricted regression procedures (i.e. LASSO or other shrinkage estimators).(Campanovo 2015, Abadie and Imbens 2008) Other bootstrapping failures may arise due to limited sample size or values on the boundary of a parameter space. Note: Poisson Bootstrap procedure Especially for large datasets, the bootstrap procedure can be computationally intensive (requiring holding multiple large matrices in memory simultaneously). Rather than sampling rows of the matrix with replacement, an alternative is to approximate the sampling with a frequency weight. If you randomly assign every observation a value drawn from a Poisson distribution with mean 1, and use this value as a frequency weight in estimators you will closely approximate the full bootstrap procedure as long as the overall sample size is &gt;100. (Hanley and Macgibbon 2006) This is very computationally efficient because you do not need to know the dataset size prior to assigning the frequency weight, and do not need to sample large matrices.\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "02_est.html",
    "href": "02_est.html",
    "title": "Estimation",
    "section": "",
    "text": "I recommend the following stepwise procedure in estimation:\n(PLR = pooled logistic regression; KM = Kaplan-Meier estimator).\n\nEstimate unweighted/unadjusted cumulative incidences with KM method\nEstimate a PLR model without weights\n\n\n\n\nCompare KM vs PLR, estimates should be similar\n\n\n\nEstimate censoring weights\n\n\n\nEstimate KM model for treatment initiation\nEstimate PLR for treatment initiation\ncompare estimates, should be similar\nestimate full weighted model, compute IPCW\nexamine IPCW weights for extreme values, non-sensical values\nexamine balance in covariates across time\ngenerate “table 1” with some weighted difference statistic (e.g. wSMD)\n\n\n\nEstimate weighted outcome models\n\n\n\nEstimate a weighted outcome model, no covariate adjustment\nEstimate a weighted outcome + covariates (main estimate)\n\n\n\nOnce satisfied with steps 1-4, execute bootstraps\nIn report, provide KM, unweighted, PLR estimates in an appendix; main results reported should be the IPCW-weighted PLR analysis with or without covariate adjustment in the outcome model (project specific decision).\n\n\n\n\n\n\n\nNote\n\n\n\nPooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of Causal Inference: What If, which explains why the odds approximates the hazard at a given time-point k, as long as the hazard is small (i.e. rare event) at time k."
  },
  {
    "objectID": "02_est.html#data",
    "href": "02_est.html#data",
    "title": "Estimation",
    "section": "Data",
    "text": "Data\nWe continue the below using the cloned dataset generated in Data.\n\nglimpse(d_cloned)\n\nRows: 20,000\nColumns: 12\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ assign     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t_clone    &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 4, 60, 2, 60, 0, 2, 60, 6, …\n$ event_outc &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ time       &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 60, 60, 60, 60, 60, 30, 60,…\n$ t_artcens  &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ treat      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,…\n$ age        &lt;dbl&gt; 88.7, 69.4, 78.6, 81.3, 79.0, 73.9, 90.1, 74.1, 95.2, 74.4,…\n$ female     &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ dx_1       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ dx_2       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,…"
  },
  {
    "objectID": "02_est.html#estimation",
    "href": "02_est.html#estimation",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\n\n  d_surv_pe = broom::tidy(\n1    survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n2    mutate(pr_e = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n3    pivot_wider(id_cols = c(time),\n                names_from = assign,\n                values_from = c(pr_e, pr_s,\n                                n.risk, n.event)) %&gt;%\n4    mutate(cir = pr_e_1 / pr_e_0,\n           cid = (pr_e_1 - pr_e_0))\n\n\n1\n\nNaive estimator, K-M estimator, assign is by clone (not person). t_clone is the follow-up time for each clone, taking into account artificial censoring.\n\n2\n\nestimate from the model is the survival probability, so probability of event is 1 - estimate\n\n3\n\nData is in long form, reshape so one colum per group with cumulative incidence/survival\n\n4\n\nEstimands, cir is ratio analogous to relative risk, and cid is analogous to risk difference"
  },
  {
    "objectID": "02_est.html#summarize-km-estimator",
    "href": "02_est.html#summarize-km-estimator",
    "title": "Estimation",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator\n\nggsurvplot(survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned),\n                            data=d_cloned,\n                            fun = 'event',\n                            xlim = c(0, 60),\n                            xlab = 'Follow-up',\n                            break.time.by=6,\n                            palette = c('red', 'blue'),\n                            linetype = 'strata',\n                            censor=F,\n                            cumevents=T,\n                            conf.int=F, pval=F,\n                            risk.table=T, risk.table.col = 'strata',\n                            risk.table.height=0.25, \n                            ggtheme = theme_classic())\n\n\n\n\n\n\n\n\nLater I will show how to summarize point estimates and confidence intervals with bootstrapping."
  },
  {
    "objectID": "02_est.html#build-a-longitudinal-panel",
    "href": "02_est.html#build-a-longitudinal-panel",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nThe PLR model is a person-time procedure, modeling the probability of the event where the observation is person at follow-up time k. The KM estimator is a person-level dataset, so the dataset much first be expanded to person-time units. If there are many persons, or many observations (timepoints) per person the dataset can become quite large. I personally use the data.table package to help with computation. It is blazingly fast, so the code reflects that approach which may not be needed for your project.\n\n1    setDT(d_cloned)\n2    d_panel = d_cloned[rep(seq(.N), t_clone)]\n3    d_panel[, exit := (seq_len(.N)), by = list(id, assign)]\n    d_panel[, enter := exit-1]\n4    d_panel[, time := seq_len(.N), by = list(id, assign)]\n5    d_panel[, event_outc := if_else( t_clone &lt;= time, event_outc, 0L), by = list(id, assign)]\n    d_panel_outc = select(d_panel, id, time, event_outc, t_treat, assign, enter, exit) \n\n\n1\n\nConvert data to a DT object for faster computations\n\n2\n\nGenerate rows along the sequence of .N (number of time-points) up to the last follow-up point\n\n3\n\nFor each row, compute the starting (enter) and ending time-point (exit). This isn’t really necessary for our toy example, but could be important depending on how you are setting up the longitudinal panel and planning to model the time-trend.\n\n4\n\nThis time is the key variable, a count from 1 to the last observed follow-up point by person, clone\n\n5\n\nOutcome is = 1 in row where event occurred, so in the data a person-clone should have values of zero for event_outc up until the last observed time-point where event_outc=1 IF the event occurred at that time.\n\n\n\n\n\nglimpse(d_panel_outc)\n\nRows: 554,458\nColumns: 7\n$ id         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ time       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ event_outc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf,…\n$ assign     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ enter      &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ exit       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how the dataset has expanded from N=20000 to N=554k! This method is very memory hungry…"
  },
  {
    "objectID": "02_est.html#estimation-1",
    "href": "02_est.html#estimation-1",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\nThe PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time^2). You can choose to either estimate the outcome in a model with both clones combined in one dataset OR you can estimate cumulative incidences separately (two models with data limited to assign==1 & assign==0 respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. time + time^2 + treat + treat*time + treat*time^2, shorthand below is poly(time, 2, raw=T)*assign.\n\n  # defined above\n1  d_glm = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n  \n2  d_panel_outc$pr_ev = d_glm$fitted.values\n3  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)]\n  \n4  d_res = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    pivot_wider(., id_cols =c('time'),\n                names_from = assign,\n                names_prefix = 'pr_ev_',\n                values_from = pr_ev\n    ) %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nPLR model, with time*treat interaction. Binomial family for logistic regression.\n\n2\n\nIf event_outc = 1 when event occurs, the fitted values are the probability of the outcome at time k.\n\n3\n\nTo describe cumulative probabilities, you take the cumulative product of the fitted probabilities pr_ev, making sure to only compute within group and clone.\n\n4\n\nAfter you have cumulative event-free survival probability, then you can summarize by group/time as described above for the KM estimator.\n\n\n\n\n\n    d_gg_ci = d_res %&gt;%\n      ggplot(aes(x=time)) +\n      geom_line(aes(y = pr_ev_0), color='red') +\n      geom_line(aes(y = pr_ev_1), color='blue') + \n      scale_x_continuous(breaks = seq(0, 60, 6),\n                         limits = c(0, 60)) +\n      theme_bw() +\n      labs(x = 'Follow-up', y = 'Cumulative incidence')\n    \n    d_gg_ci\n\n\n\n\n\n\n\n\nAt first glance, plot looks reasonable. We now directly compare PLR to the KM estimator by plotting together to see if the modeling appears to be working:\n\n\n\n\n\n\n\n\n\nSo the PLR model using a simple polynomial can approximate our KM estimate reasonably well (This should work because we used synthetic data generated with an exponential distribution for time, but real-world data will not play so nicely). There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider following points:\n\nIt may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.\nThe non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.\nThese initial steps will not guarantee a good fit after weighting is applied, it is only a beginning."
  },
  {
    "objectID": "02_est.html#build-a-longitudinal-panel-1",
    "href": "02_est.html#build-a-longitudinal-panel-1",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nAs before the person- or clone-level data must be expanded to a longitudinal panel by time. Here, two datasets must be constructed, one for estimation of censoring probabilities and one for estimation of outcome probabilities.\n\nCensoring dataset\nIn our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is very specific to a project’s definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we generate a dataset that is at the person-level (no cloning):\n\nd_treat = d_cloned %&gt;%\n1    dplyr::filter(assign==0) %&gt;%\n    mutate(start=1,\n2           end = t_treat) %&gt;%\n3    group_by(id) %&gt;%\n    mutate(\n      time = t_clone,\n      outcome = if_else(time==t_treat, 1, 0)\n    ) %&gt;%\n    ungroup\n\n\n1\n\nThis may be a little confusing, but I am taking clones where assign=0 from the d_cloned dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.\n\n2\n\nWe are estimating time to treatment, not outcome\n\n3\n\nArtificial censoring time is same as treatment time unless dead first, and outcome = 1 if treated.\n\n\n\n\nThen we expand the dataset as described above:\n\n  setDT(d_treat)\n  d_panel = d_treat[rep(seq(.N), time)]\n  d_panel[, exit := (seq_len(.N)), by = list(id)]\n  d_panel[, enter := exit-1]\n  d_panel[, time := seq_len(.N), by = list(id)]\n  d_panel[, event_treat := if_else(t_treat &lt;= time, treat, 0L), by = list(id)]\n1  d_panel_treat = select(d_panel, id, time, event_treat, t_treat, enter, exit, end, age, female, dx_1, dx_2)\n\n\n1\n\nNote we keep covariates for estimation of censoring weights."
  },
  {
    "objectID": "02_est.html#estimation-2",
    "href": "02_est.html#estimation-2",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\nWe fit a model with the outcome of censoring, including the variables which are key. Then we add fitted probabilities back to the dataset, calculate cumulative probabilities of remaining uncensoring, and join those estimated probabilities to the outcome dataset.\n\n1  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T) + poly(age, 2) + female + dx_1 + dx_2, data=d_panel_treat, family=binomial())\n2  d_panel_treat$pr_treat = d_glm_wt$fitted.values\n  setDT(d_panel_treat)\n3  d_panel_treat[, cumpr_notreat := cumprod(1-pr_treat), by = .(id)]\n  \n4  d_panel_treat = select(d_panel_treat, id, time, cumpr_notreat)\n  d_panel_outc = left_join(d_panel_outc, d_panel_treat, by=c('id', 'time'))\n\n\n1\n\nEstimation of probability of censoring in PLR\n\n2\n\nAdd fitted probabilities to dataset\n\n3\n\nCalculate cumulative probability of non-treatment (treatment-free survival)\n\n4\n\nJoin probabilities back to outcome dataset\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI think there is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of “censoring” where that may mean treatment initiation or some other thing.(Gaber et al. 2024)"
  },
  {
    "objectID": "02_est.html#calculation-of-weights",
    "href": "02_est.html#calculation-of-weights",
    "title": "Estimation",
    "section": "Calculation of weights",
    "text": "Calculation of weights\nThis step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to Weighting for further discussion.\n\n  setDT(d_panel_outc)\n  \n  d_panel_outc[, ipw := fcase(\n1    assign==0, 1 / cumpr_notreat,\n2    assign==1 & time &lt; 12, 1,\n3    assign==1 & time == 12 & t_treat &lt; 12, 1,\n4    assign==1 & time==12   & t_treat   ==12, 1 / (1-cumpr_notreat),\n5    assign==1 & time==12   & t_treat    &gt;12, 0,\n6    assign==1 & time &gt; 12, 1\n  )]\n7  d_panel_outc[assign==1, ipw := cumprod(ipw), by=list(id, assign)]\n\n\n1\n\n(assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored\n\n2\n\n(assign=1) Clones cannot artifically censor prior to grace\n\n3\n\n(assign=1) If treatment started prior to grace window ending, the clone cannot censor\n\n4\n\n(assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).\n\n5\n\n(assign=1) If a clone is not treated they censor at the end of the window\n\n6\n\n(assign=1) In periods greater than the end of the grace window, no artificial censoring occurs (they either are treated or already censored at this point).\n\n7\n\nAfter setting these conditions, we compute the cumulative product of the weights.\n\n\n\n\n\n  summary(d_panel_outc$ipw)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   1.201   1.125   1.219   6.019 \n\n\nNow with the estimated weights, it is simple to generate weighted cumulative incidences:\n\n1  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(), weights = ipw)\n  \n2  d_panel_outc$pr_ev = d_glm_pe$fitted.values\n  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)]\n  d_res = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    pivot_wider(., id_cols =c('time'),\n                names_from = assign,\n                names_prefix = 'pr_ev_',\n                values_from = pr_ev\n    ) %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nNote the weights = ipw argument, everything else is same as PLR model above. R glm() will generate a warning message because the weighted counts are “non-integer”, but this is expected and not a problem.\n\n2\n\nSummarize across group, time as before.\n\n\n\n\n\n  ### Plot ----\n    d_gg_ci = d_res %&gt;%\n      ggplot(aes(x=time)) +\n      geom_line(aes(y = pr_ev_0), color='red') +\n      geom_line(aes(y = pr_ev_1), color='blue') + \n      scale_x_continuous(breaks = seq(0, 60, 6),\n                         limits = c(0, 60)) +\n      theme_bw() +\n      labs(x = 'Follow-up', y = 'Cumulative incidence')\n\n    d_gg_rr = d_res %&gt;%\n      ggplot(aes(x=time)) +\n      geom_line(aes(y = cir), color='green') + \n      scale_y_continuous(limits = c(0.5, 1.1)) + \n      scale_x_continuous(breaks = seq(0, 60, 6),\n                         limits = c(0, 60)) +\n      theme_bw() +\n      labs(x = 'Follow-up', y = 'Relative Risk')\n    \n    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr,\n                       nrow=2)\n    \n    d_gg_1\n\n\n\n\n\n\n\n\n\n\nGaber, Charles E., Armen A. Ghazarian, Paula D. Strassle, Tatiane B. Ribeiro, Maribel Salas, Camille Maringe, Xabier Garcia-Albeniz, Richard Wyss, Wei Du, and Jennifer L. Lund. 2024. “De-Mystifying the Clone-Censor-Weight Method for Causal Research Using Observational Data: A Primer for Cancer Researchers.” Cancer Medicine 13 (23): e70461. https://doi.org/10.1002/cam4.70461."
  },
  {
    "objectID": "01_syndata.v2.html",
    "href": "01_syndata.v2.html",
    "title": "Simulate data for CCW project",
    "section": "",
    "text": "Simulation Parameters\n\n1    n = 10000\n2    age = round(rnorm(n, mean = 75, sd = 10), 1)\n3    female = sample(c(1, 0), size = n, replace = TRUE, prob = c(0.66, 0.34))\n4    treat &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.2, 0.8))\n5    dx_1 &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.05, 0.95))\n6    dx_2 &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.4, 0.6))\n7    ef_treat = 0.8\n    ef_gender = 0.9\n    ef_age = 0.98\n    ef_dx_1 = 1.2\n    ef_dx_2 = 0.8\n\n\n1\n\nUse a starting sample size of a 1000 (i.e. persons) Generate covariates:\n\n2\n\nGenerate age (normal distribution with mean=75, std=10)\n\n3\n\nFemale, 66%/34%\n\n4\n\nTreatment, 20%/80%\n\n5\n\nA rare chronic condition dx_1 5%/95%\n\n6\n\nA prevalent chronic condition dx_2 40%/60%\n\n7\n\nAssign relative effect sizes for each\n\n\n\n\n\nef_treat = 0.8\nef_gender = 0.9\nef_age = 0.98 (per unit of age)\nef_dx_1 = 1.2\nef_dx_2 = 0.8\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…\n\n\n\n\nLinear Predictor\n\n# log-hazard\n    lp_outc &lt;- log(ef_treat) * treat +\n      log(ef_gender) * female +\n1      log(ef_age) * age + log(ef_dx_1)*dx_1 + log(ef_dx_2)*dx_2\n  \n  # Simulate baseline survival times (exponential distribution)\n2    baseline_hazard &lt;- 0.05\n\n\n1\n\nCompute the linear predictor,\n\n2\n\nSimulation will use a random exponential distribution with rate of 0.05 (baseline hazard). This was selected based on trial and error.\n\n\n\n\n\n\nSimulate Event Time\n\n1    baseline_survival &lt;- rexp(n, rate = baseline_hazard)\n    \n  # Adjust survival times based on linear predictor\n2    t_outc &lt;- round(baseline_survival * exp(-lp_outc))\n\n\n1\n\nSimulate survival times using exponential distribution\n\n2\n\nAdjust the times with linear predictor and round to nearest whole number.\n\n\n\n\n\n\nSimulate Treatment Start\n\n1    lp_treat = log(0.7) * female + log(1.02) * age\n2    baseline_hazard = 0.1\n3    t_treat = if_else(treat==1, round(rexp(n, rate = baseline_hazard)* exp(-lp_treat)), Inf)\n\n\n1\n\nSimulate treatment start times based on female and age. (for weighting model); assumed dx_1 and dx_2 unrelated to treatment.\n\n2\n\nUsed a different rate, so treatment happens early relative to outcome.\n\n3\n\nGenerate treatment times and adjust for linear predictor.\n\n\n\n\n\n\nFinalize dataset\n\n  # Create dataset\n    data &lt;- data.frame(\n      age = age,\n      female = female,\n      dx_1 = dx_1,\n      dx_2 = dx_2,\n      treat = treat, \n      t_treat = t_treat, \n      t_outc = t_outc \n    ) %&gt;%\n      mutate(id = row_number(),\n1             time = pmin(60, t_outc),\n             event_outc = if_else(time==t_outc, 1L, 0L))\n\n\n1\n\nSet administrative end of follow-up at 60. No other censoring mechanism, e.g. lost to follow-up.\n\n\n\n\n\n\nSummary of survival dataset\n\nglimpse(data)\n\nRows: 10,000\nColumns: 10\n$ age        &lt;dbl&gt; 88.7, 69.4, 78.6, 81.3, 79.0, 73.9, 90.1, 74.1, 95.2, 74.4,…\n$ female     &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ dx_1       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ dx_2       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,…\n$ treat      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,…\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ t_outc     &lt;dbl&gt; 61, 85, 75, 28, 47, 49, 301, 57, 111, 75, 111, 206, 72, 30,…\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ time       &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 60, 60, 60, 60, 60, 30, 60,…\n$ event_outc &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,…\n\n\n\nsummary(survfit(Surv(time, event_outc) ~ treat, data = data), times = c(0, 12, 30, 60))\n\nCall: survfit(formula = Surv(time, event_outc) ~ treat, data = data)\n\n                treat=0 \n time n.risk n.event survival  std.err lower 95% CI upper 95% CI\n    0   7924      33    0.996 0.000723        0.994        0.997\n   12   7125     844    0.889 0.003524        0.882        0.896\n   30   5956    1145    0.745 0.004897        0.735        0.754\n   60   4465    1475    0.559 0.005578        0.548        0.570\n\n                treat=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0   2076       6    0.997 0.00118        0.995        0.999\n   12   1882     200    0.901 0.00656        0.888        0.914\n   30   1635     244    0.783 0.00904        0.766        0.801\n   60   1286     347    0.616 0.01067        0.596        0.637\n\n\n\n\n\n\n\n\nNote\n\n\n\nTreat=1 means they EVER received treatment, so additional work needed to describe a treatment window etc.\n\n\n\n\nCloning procedure\nThe cloning procedure is very project specific, tied to how the treatment strategy is defined so it is hard to give general recommendations here. For this tutorial, we describe an arbitrary window in which treatment is expected to initiate and outline strategies around this:\nTreatment Strategies:  1) Do not ever receive treatment 2) Initiate treatment within 12 time periods (days, weeks etc.) and if not then treatment will initiate on week 12.\n\n\n\n\n\n\nNote\n\n\n\nThe grace window is a funny concept when you first consider it. This does not reflect any trial I have ever heard of actually being done but may identify an interesting or important effect. It is essentially outlying a treatment “protocol” and saying what if everyone adhered to this protocol counterfactual to what was observed.\n\n\nTo clone, you make two copies of the data, and make a new artifical censoring variable, which is a censoring time for the period when clones observed data are no longer consistent with their assigned strategy.\n\n1data_cloned = bind_rows(\n                     data %&gt;%\n                       mutate(assign = 0,\n                              t_artcens = if_else(t_treat &lt; time, t_treat, Inf)\n                       ),\n2                     data %&gt;%\n                       mutate(assign = 1,\n                              t_artcens = if_else(t_treat &gt; 12, 12, Inf)\n                              )\n                       ) %&gt;%\n3  mutate(t_clone = pmin(time, t_artcens),\n         event_outc = if_else(t_clone&lt;time, 0, event_outc))\n\n\n1\n\nClones assigned to strategy 1 (No treatment)\n\n2\n\nClones assigned to strategy 2 (grace window for treatment)\n\n3\n\nUpdate failure times and events counting artificial censoring\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI set failure times to Infinite when the event is unobserved, e.g. if no treatment, time to treatment is INF"
  },
  {
    "objectID": "03_tab1exs.html",
    "href": "03_tab1exs.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Description of cohort, “Table 1” In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort. Often researchers compare those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals). At most I will provide a standardized mean difference. This follows some template like below: Basic “Table 1” set up often found in research manuscripts:\nTreatment No treatment Age - - Gender - - Chronic condition - - … - -\nSome special considerations are needed when presenting target trial emulations. Sequential target trials If conducting sequential daily target trial emulations, even though there may be only 100 unique persons in your study, because you repeatedly use some as controls, the number of observations in the control arm could be substantially larger. Assume 100 unique people analyzed across 2 sequential emulations, and on average 10% get treatment on any given sequence. You can expect ([100 * 0.1] + [90 * 0.1]) = 19 treated, and (90 + 81) = 171 control observations. So how do you present this in the “Table 1”? One approach is to present every observation. Since baseline is indexed to each sequence this can be important if the baseline information changes across sequences (e.g. a diagnosis is present for one sequence but not the other). However, this can be misleading (inflating sample size) or confusing to the reader especially if they are unfamiliar with emulation methods. It is important to report the number of unique persons and follow-up time if relevant as well as the total number of observations. Other possibilities include randomly selecting one observation per person within each group but analytical methods will lose precision. In the working example, 9 people used as controls in the first sequence, were included in the treatment arm for the second sequence. Present all observations Treatment N=19 (19 unique people) No treatment N=171 (90 unique people) Randomly select one observation per person Treatment N=19 No treatment N=90\nCloning & Grace Period The cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then two columns describing the intervention groups at the end of the grace period. Additionally, it makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.) Basic “Table 1” set up for a clone-censor-weight target trial emulation:\nBaseline (N=100) No treatment at 6 months (N=50) Treatment within 6 months (N=50) Age - - - Gender - - - Hypertension\n\n\n\n\nMonth prior to index\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "03_tab1exs.html#baseline",
    "href": "03_tab1exs.html#baseline",
    "title": "Reporting Results",
    "section": "",
    "text": "Month prior to index\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "04_wtnotes.html",
    "href": "04_wtnotes.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Application of inverse-probability weighting:\nConsider a target trial emulation assigning those eligible for vaccination with Shingrix (RZV) in a nursing home to one of two treatment strategies:\nVaccination with a RZV within 6 months; No vaccination through follow-up\nThe outcome is an incident dementia diagnosis, and individuals censor for non-adherence to treatment strategy, or death\nFor practicality, we use month units of time, and we assume censoring due to death is non-informative and unaccounted for in the analysis.\nEach person is cloned and assigned to both strategies. For example, consider three persons again:\nPerson A Person B Person C Vaccinated in month 3 Unvaccinated through follow-up Vaccinated in month 6\nArtificial censoring: abbrev. HZV (vaccination within 9 months), no HZV (none ever)\nPerson A Person B Person C\nVaccinated in month 3 Unvaccinated through follow-up Vaccinated in month 6\nCensoring Censoring Censoring Period HZV No HZV HZV No HZV HZV No HZV 1 0 0 0 0 0 0 2 0 0 0 0 0 0 3 0 1 0 0 0 0 4 0 - 0 0 0 0 5 0 - 0 0 0 0 6 0 - 1 0 0 1\nThe probability of censoring is intrinsically tied to the probability of initiating treatment because we censor based on adherence to treatment strategy, it might be more appropriate to think of the weighting as “adherence weighting”.\nNo HZV strategy:\nFor the No HZV strategy, the probability of censoring is equal to the probability of initiating treatment (because you are censored if you get the vaccine!). Where V is vaccination, Pr(C) = Pr(Vt=i=1|Vt=i-1=0). Of course, 1 - Pr(C) is the probability of not censoring. The inverse probability of remaining uncensored is 1 / (1 - Pr[C]) at each timepoint.\nTime periods i, from 1 to end of followup, the IPW is 1 / (1 - Pr[C]) = 1 / (1 -Pr[Vt=i=1|Vt=i-1=0])\nPeriod Probability of censoring Probability of not censoring IPW i = 1,…I, where I is end of follow-up Pr(C) = Pr(Vt=i=1|Vt=i-1=0) 1 - Pr(C) 1 / (1 - Pr[C])\nHZV strategy:\nWhen assigned to vaccination within 6 months, the probability of censoring is 0 for each period up until the end of the grace period (G).\nPeriod Probability of censoring Probability of not censoring IPW i= 1, …, 1-G 0 1 1 / 1\nThis is because we have explicitly stated we are allowing a grace period of time (6 months) before people must be vaccinated.\nAt the end of the grace period, the probability of censoring is the probability of not being vaccinated, conditional on not yet receiving the vaccine.\nPeriod Probability of censoring Probability of not censoring IPW I = 1, …, 1-G 0 1 1 / 1 I = G Pr(C) = Pr(Vt = G=0|, Vt=G-1=0) 1 - Pr(C) 1 / (Pr[C])\nSo individuals who are vaccinated in the last period of the grace window are very important, because they are going to be assigned a weight according to the likelihood of receiving vaccination in that period. If the probability of vaccination in the last period given no prior vaccination is very small, 1 - Pr(Vt = G=0|, Vt=G-1=0), then those persons will be assigned very large weights. E.g. if 1% of the sample gets the vaccine at month 6, then the mean of the weights computed for them will be approximately 100 because 1/0.01 = 100. A key consideration is whether folks getting vaccinated in the last period meet causal assumptions of consistency, positivity etc. and you have good data to model the probability of initiation in this later period (time-varying confounders).\nNote: Conceptually, what we are doing here is emulating a trial where there is perfect adherence to vaccine strategy, but we allow a period of time (grace) before people must be vaccinated. If anyone doesn’t get the vaccine then we essentially force them to receive it at the end of the grace period. This is a somewhat extreme hypothetical, that is commented on by Jamie Robins in the appendix of the Cain article. There is some speculation that alternative weighting strategies could be applied which spread out “forced” vaccination across the grace period, but I have yet to see them in practice.\nIf a person receives the vaccine prior to month 6, they cannot be censored because they are fully adherent to treatment no matter what happens from that period forward. So if vaccinated prior to the grace period, the weight is 1.\nIn both cases however, the key estimand is the probability of vaccination at each point of follow-up. This is estimated in the full sample prior to cloning.\nSo a model must be fit which estimates probability of vaccination at time period i, given no vaccination in prior time period, baseline covariates (X), time-varying covariates (L), for each point of follow-up.\nPr(Vt=i=1 | Vt=i-1=0, X, Lt=i-1)\nOnce you have that basic probability is it easy to compute the weighting at each timepoint:\nBack to our cloned panel:\nPerson A Person B Person C\nVaccinated in month 3 Unvaccinated through follow-up Vaccinated in month 6\nIPW IPW IPW Period HZV No HZV HZV No HZV HZV No HZV 1 1 1 / Pr(Vt=i=0) 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 2 1 1 / Pr(Vt=i=0) 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 3 1 - 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 4 1 - 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 5 1 - 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 6 1* - - 1 / Pr(Vt=i=0) 1 / (1 - Pr[Vt=i=0])** -\n*Person A is vaccinated at month 3, so their probability of censoring is now 0, 1 - 0 = 1, 1/1 = IPW=1\n**Person C is vaccinated in the last period, so their probability of censoring is the conditional probability of not being vaccinated in the 6th period, given no vaccination up to that point. In other words their probability of remaining uncensored, is the probability of initiating treatment in that last period. So 1 / (1 - Pr[C]) is their IPW for this period. They are being weighted to estimate causal effects in a pseudopopulation where person B (who had been censored due to no vaccination) had also been vaccinated at timepoint 6 (a perfectly adherent, “per-procol” estimand).\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "",
    "text": "Data; Demonstrates how to simulate a dataset for use in CCW along with the cloning process. A key process is how to generate a “cloned” dataset, refer here\nEstimation; Main section which walks through each estimation step.\n\nFor production notes and future efforts see: About."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe tutorial represents my gathered and organized notes from research projects and didactic training. Collaborators and mentors include: Issa Dahabreh, Kaley Hayes, Daniel Harris, Donald Miller and Andrew Zullo."
  }
]