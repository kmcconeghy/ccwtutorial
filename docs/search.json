[
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "",
    "text": "Project set-up"
  },
  {
    "objectID": "NEWS.html#version-0.0.1-02-19-2025",
    "href": "NEWS.html#version-0.0.1-02-19-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "",
    "text": "Project set-up"
  },
  {
    "objectID": "NEWS.html#version-0.1-05-12-2025",
    "href": "NEWS.html#version-0.1-05-12-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.1 05-12-2025",
    "text": "Version 0.1 05-12-2025\n\nSynthetic data set-up for observed effect by treatment due to confounding; no effect after appropriate adjustment\nEstimator file complete; additional edits for clarity and alternative approaches\nInference file TBD\nAdvanced file discusses some bootstrap and computation approaches\nAppendix TBD"
  },
  {
    "objectID": "NEWS.html#version-0.2-05-13-2025",
    "href": "NEWS.html#version-0.2-05-13-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.2 05-13-2025",
    "text": "Version 0.2 05-13-2025\n\nAdded Parallel computation step to Advanced Topics page\nSetup for the inference page (WIP)"
  },
  {
    "objectID": "NEWS.html#version-0.3-05-21-2025",
    "href": "NEWS.html#version-0.3-05-21-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.3 05-21-2025",
    "text": "Version 0.3 05-21-2025\n\nRevamped synthetic data for time-varying confounding\nAdded cox models to estimator page\nswitched to color-blind friendly color-scheme"
  },
  {
    "objectID": "NEWS.html#version-0.5-05-22-2025",
    "href": "NEWS.html#version-0.5-05-22-2025",
    "title": "Tutorial on Clone-Censor-Weight",
    "section": "Version 0.5 05-22-2025",
    "text": "Version 0.5 05-22-2025\n\nTutorial appears to work as expected, some edits to text and code to be more concise"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#version-0.0.1-02-19-2025",
    "href": "about.html#version-0.0.1-02-19-2025",
    "title": "About",
    "section": "Version 0.0.1 02-19-2025",
    "text": "Version 0.0.1 02-19-2025\nProject set-up"
  },
  {
    "objectID": "about.html#version-0.1-05-12-2025",
    "href": "about.html#version-0.1-05-12-2025",
    "title": "About",
    "section": "Version 0.1 05-12-2025",
    "text": "Version 0.1 05-12-2025\n\nSynthetic data set-up for observed effect by treatment due to confounding; no effect after appropriate adjustment\nEstimator file complete; additional edits for clarity and alternative approaches\nInference file TBD\nAdvanced file discusses some bootstrap and computation approaches\nAppendix TBD"
  },
  {
    "objectID": "about.html#version-0.2-05-13-2025",
    "href": "about.html#version-0.2-05-13-2025",
    "title": "About",
    "section": "Version 0.2 05-13-2025",
    "text": "Version 0.2 05-13-2025\n\nAdded Parallel computation step to Advanced Topics page\nSetup for the inference page (WIP)"
  },
  {
    "objectID": "about.html#version-0.3-05-21-2025",
    "href": "about.html#version-0.3-05-21-2025",
    "title": "About",
    "section": "Version 0.3 05-21-2025",
    "text": "Version 0.3 05-21-2025\n\nRevamped synthetic data for time-varying confounding\nAdded cox models to estimator page\nswitched to color-blind friendly color-scheme"
  },
  {
    "objectID": "about.html#version-0.5-05-22-2025",
    "href": "about.html#version-0.5-05-22-2025",
    "title": "About",
    "section": "Version 0.5 05-22-2025",
    "text": "Version 0.5 05-22-2025\n\nTutorial appears to work as expected, some edits to text and code to be more concise"
  },
  {
    "objectID": "04_advanced.html",
    "href": "04_advanced.html",
    "title": "Additional Topics",
    "section": "",
    "text": "In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort, or comparison of those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals) but will provide a standardized mean difference.\nSome special considerations are needed when presenting target trial emulations.\n\n\nThe cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)"
  },
  {
    "objectID": "04_advanced.html#cloning-grace-period",
    "href": "04_advanced.html#cloning-grace-period",
    "title": "Additional Topics",
    "section": "",
    "text": "The cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)"
  },
  {
    "objectID": "04_advanced.html#evaluation-of-grace-period",
    "href": "04_advanced.html#evaluation-of-grace-period",
    "title": "Additional Topics",
    "section": "Evaluation of Grace Period",
    "text": "Evaluation of Grace Period"
  },
  {
    "objectID": "04_advanced.html#weighting-distributions",
    "href": "04_advanced.html#weighting-distributions",
    "title": "Additional Topics",
    "section": "Weighting distributions",
    "text": "Weighting distributions"
  },
  {
    "objectID": "04_advanced.html#covariate-balance-across-pre--post-weighting-during-follow-up",
    "href": "04_advanced.html#covariate-balance-across-pre--post-weighting-during-follow-up",
    "title": "Additional Topics",
    "section": "Covariate balance across pre-, post-weighting during follow-up",
    "text": "Covariate balance across pre-, post-weighting during follow-up"
  },
  {
    "objectID": "04_advanced.html#benchmarked-estimation-step",
    "href": "04_advanced.html#benchmarked-estimation-step",
    "title": "Additional Topics",
    "section": "Benchmarked Estimation Step",
    "text": "Benchmarked Estimation Step\n\nglm(event==0 ~ poly(time, 2, raw=T)*assign, data=dta_c_panel, family=binomial())\n\n\nSpeeding up GLM ML procedure\nThere are two main things that can be done to speed up GLM, 1) initialize parameters based on prior estimation procedure. 2) Use efficient matrix functions and parallel computation.\n\nInitialization hack\nThis is a simple trick, either 1) run the GLM once and store est, or 2) run the GLM on a 10% sample.\n\nd_fit = glm(event==0 ~ poly(time, 2, raw=T)*assign, \n1            data=dta_c_panel, family=binomial())\n\nd_fit_2 = glm(event==0 ~ poly(time, 2, raw=T)*assign, \n              data=dta_c_panel, family=binomial(), \n2              start = d_fit$coefficients)\n\n\n1\n\nGLM procedure with automatic initialization step.\n\n2\n\nGLM with start= option using coefficients from prior step.\n\n\n\n\n\n\nBLAS/LAPACK and Parallel computation\nThe parglm package provides much faster computations (but somewhat more unstable).\n\nlibrary(parglm)\n\nd_fit_3 = parglm(event==0 ~ poly(time, 2, raw=T)*assign, \n       family=binomial(), data=dta_c_panel, \n       start = d_fit$coefficients,\n1       control = parglm.control(method = \"FAST\", nthreads = 8L))\n\n\n1\n\nparglm() function works mostly as glm(), the parglm.control() allows some additional options for parallel computing and QR decomposition.\n\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n           expr   min    lq  mean median    uq   max neval\n       base GLM 2.380 2.400 2.490  2.460 2.460 2.750     5\n GLM with inits 0.815 0.855 0.962  1.020 1.030 1.080     5\n         PARGLM 0.468 0.476 0.520  0.483 0.489 0.684     5\n\n\nEven in a small example, parglm() significantly outperforms base glm(). This will scale considerably with multiple cores and larger datasets as well.\nparglm() may be more unstable (convergence issues), but should be sufficient for most problems.\n\n\n\nComparison of glm() and parglm() results\n\n\nCoefficient\nglm()\nparglm()\n\n\n\n\n(Intercept)\n5.16165\n5.16165\n\n\npoly(time, 2, raw = T)1\n-0.00463\n-0.00463\n\n\npoly(time, 2, raw = T)2\n-0.00015\n-0.00015\n\n\nassign\n0.11568\n0.11568\n\n\npoly(time, 2, raw = T)1:assign\n-0.01575\n-0.01575\n\n\npoly(time, 2, raw = T)2:assign\n0.00013\n0.00013"
  },
  {
    "objectID": "04_advanced.html#bootstrapping-procedure",
    "href": "04_advanced.html#bootstrapping-procedure",
    "title": "Additional Topics",
    "section": "Bootstrapping procedure",
    "text": "Bootstrapping procedure\nI consider bootstrapping to be the standard for this type of analysis, the statistical properties are not well-described, but some use influence-based statistics.\nThe typical bootstrap procedure resamples an entire dataset iteratively, but this can be very inefficient depending on how you set it up because it may involve holding the original dataset, and another new bootstrapped dataset in memory, also potentially a matrix in a regression optimization step. However some clever use of matrices and shortcuts can work around this.\n\n\n\n\n\n\nNote\n\n\n\nThe bootstrap procedure must sample at the person level to account for the cloning.\n\n\n\nInefficient Bootstrap\n\nboot_it_1 = function(x) {\n  \n  setDT(x)\n  \n1  d_ids = distinct(x, id)\n  \n2  d_boot = slice_sample(d_ids, prop=1, replace=T)\n  \n3  d_panel_outc_2 = left_join(d_boot,\n                             x, by = join_by(id),\n                             relationship = \"many-to-many\")\n  \n  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), \n                     data=d_panel_outc_2[d_panel_outc_2$assign==1, ], \n                     family=binomial()) \n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), \n                   data=d_panel_outc_2[d_panel_outc_2$assign==0, ], \n                     family=binomial())\n  \n  d_panel_outc_2$pr_1 = predict(d_glm_pe_1, newdata = d_panel_outc_2, \n                              type='response') \n  d_panel_outc_2$pr_0 = predict(d_glm_pe_0, newdata = d_panel_outc_2, \n                             type='response') \n\n  d_panel_outc_2[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  d_panel_outc_2[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = d_panel_outc_2 %&gt;% \n    group_by(time) %&gt;% \n    summarize(pr_ev_1 = mean(1-pr_cum_1),\n              pr_ev_0 = mean(1-pr_cum_0), \n              .groups = 'drop') %&gt;% \n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0)\n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a list of unique person IDs\n\n2\n\nSample from list with replacement\n\n3\n\nPerform left-join on resampled list back to dataset\n\n\n\n\n\n\nMore efficient bootstrap\nRather than sampling rows of the matrix with replacement, an alternative is to approximate the sampling with a frequency weight. If you randomly assign every observation a value drawn from a Poisson distribution with mean 1, and use this value as a frequency weight in estimators you will closely approximate the full bootstrap procedure as long as the overall sample size is &gt;100.(Hanley and MacGibbon 2006) This is very computationally efficient because you do not need to know the dataset size prior to assigning the frequency weight, and do not join or work with multiple large matrices.\n\nboot_it_2 = function(x) {\n  \n  setDT(x)\n  \n1  x[, freqwt:=rpois(n=1, lambda=1), by = factor(id)]\n\n  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T), \n                     data=x[x$assign==1, ], \n                     family=binomial(), weights = freqwt) \n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T), \n                   data=x[x$assign==0, ], \n                     family=binomial(), weights = freqwt) \n  \n  x$pr_1 = predict(d_glm_pe_1, newdata = x, \n                              type='response') \n  x$pr_0 = predict(d_glm_pe_0, newdata = x, \n                             type='response') \n\n  x[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)] \n  x[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)] \n  \n  d_res = x %&gt;% \n    group_by(time) %&gt;% \n2    summarize(pr_ev_1 = weighted.mean(1-pr_cum_1, freqwt),\n              pr_ev_0 = weighted.mean(1-pr_cum_0, freqwt),\n              .groups = 'drop') %&gt;% \n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0, \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a frequency weight by group ID from a Poisson distribution with mean 1\n\n2\n\nNote the use of weighted.mean() versus mean() in other code.\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n                 expr  min   lq mean median   uq  max neval\n Resampling bootstrap 2.39 2.46 2.57   2.50 2.72 2.75     5\n    Poisson bootstrap 2.34 2.36 2.38   2.36 2.41 2.44     5\n\n\n\n\nParallel computation\nThe next thing to work on is parallel computation steps. The efficiency depends on how the data is setup and what steps are running in parallel, it is not efficient to hold several very large datasets in memory at once so that a CPU worker can be assigned to each.\nIf you don’t believe this provides similar coverage estimates, here are the intervals from 50 bootstraps with both procedures:\nHere is a simple example for a Dell Laptop using the Furrr package with 10 workers:\n\nlibrary(furrr)\nboots = 100\n1plan(multisession, workers = 10)\n\nresampling_res = future_map_dbl(1:boots, \n                                function(x) boot_it_1(dta_c_panel), \n2                                .options = furrr_options(seed=T))\n\npoisson_res = future_map_dbl(1:boots, \n                                function(x) boot_it_2(dta_c_panel), \n                                .options = furrr_options(seed=T))\n\n\n1\n\nAssign number of CPU workers to job\n\n2\n\nThe seed=T option is important because the bootstrap function uses RNG.\n\n\n\n\n\n\n\npaste0(‘Distribution of risk difference by assignment by either bootstrap methods,’, boots, ’ bootstraps)\n\n\nMethod\nmin\nLower CI\nmean\nq50th\nUpper CI\nmax\nSD\n\n\n\n\nPoisson\n0.0405\n0.0496\n0.0738\n0.075\n0.0970\n0.0995\n0.0119\n\n\nResampling\n0.0471\n0.0522\n0.0790\n0.081\n0.1014\n0.1125\n0.0131"
  },
  {
    "objectID": "02_est.v2.html",
    "href": "02_est.v2.html",
    "title": "Estimation",
    "section": "",
    "text": "I recommend the following stepwise procedure in estimation:\n\nPLR\n\nPooled Logistic Regression, a model which approximates the hazards by discretizing follow-up time.\n\nKM\n\nKaplan-Meier, non-parametric estimator which computes the instantaneous hazard at points of follow-up. Does not allow for time-varying confounding, but the non-parametric evaluation of time-trends can be useful for diagnostics and model specification of the PLR.\n\n\n\nEstimate unweighted/unadjusted cumulative incidences with KM method\nEstimate a PLR model without weights\n\n\n\nCompare KM vs PLR\n\n\n\nEstimate censoring weights\n\n\n\nEstimate PLR for treatment initiation\nA. Estimate treatment model\nB. Compare KM / PLR estimates\nC. Compute IPCW\nD. Examine IPCW weights for extreme values, non-sensical values\nE. Examine balance in covariates across time\nGenerate “table 1” with some weighted difference statistic (e.g. wSMD)\n\n\n\nEstimate weighted outcome models\n\n\n\nEstimate a weighted outcome model, no covariate adjustment\nEstimate a weighted outcome + covariates (usually main estimate)\n\n\n\nOnce satisfied with stability from steps 1-4, execute bootstraps\nFinalize report\n\n\n\nprovide KM, Cox unweighted, and PLR unweighted estimates in an appendix\nmain results reported should be the IPCW-weighted PLR analysis\n\n\n\n\n\n\n\nNote\n\n\n\nPooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of Causal Inference: What If, which explains why the odds approximates the hazard at a given time-point k, as long as the hazard is small (i.e. rare event) at time k."
  },
  {
    "objectID": "02_est.v2.html#data",
    "href": "02_est.v2.html#data",
    "title": "Estimation",
    "section": "Data",
    "text": "Data\nWe continue the below using the cloned dataset generated in Data.\n\nglimpse(dta_c_person)\n\nRows: 20,000\nColumns: 10\n$ id        &lt;int&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10…\n$ assign    &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, …\n$ t_treat   &lt;dbl&gt; Inf, Inf, 20, 20, Inf, Inf, 22, 22, 16, 16, Inf, Inf, 24, 24…\n$ t_outcome &lt;dbl&gt; 46, 46, Inf, Inf, 77, 77, Inf, Inf, 73, 73, 87, 87, 60, 60, …\n$ t_censor  &lt;dbl&gt; Inf, 12, 20, 12, Inf, 12, 22, 12, 16, 12, Inf, 12, 24, 12, 1…\n$ time      &lt;dbl&gt; 46, 12, 20, 12, 60, 12, 22, 12, 16, 12, 60, 12, 24, 12, 14, …\n$ event     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ X         &lt;dbl&gt; -0.7311102, -0.7311102, -2.5249386, -2.5249386, -0.6470915, …\n$ female    &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ age       &lt;dbl&gt; 70.0, 70.0, 76.3, 76.3, 74.2, 74.2, 83.9, 83.9, 76.2, 76.2, …"
  },
  {
    "objectID": "02_est.v2.html#estimation",
    "href": "02_est.v2.html#estimation",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\n\n  d_km_est = broom::tidy(\n1    survfit(Surv(time, event) ~ assign, data=dta_c_person)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n2    mutate(pr_ev = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n3    pivot_wider(id_cols = c(time),\n                names_from = assign,\n                values_from = c(pr_ev, pr_s,\n                                n.risk, n.event)) %&gt;%\n4    mutate(cir = pr_ev_1 / pr_ev_0,\n           cid = (pr_ev_1 - pr_ev_0))\n\n\n1\n\nNaive estimator, K-M estimator, assignment is by clone (not person). t_clone is the follow-up time for each clone, taking into account artificial censoring.\n\n2\n\nestimate from the model is the survival probability, so probability of event is 1 - estimate\n\n3\n\nData is in long form, so one colum per group with cumulative incidence/survival\n\n4\n\nEstimands, cir is ratio analogous to relative risk, and cid is analogous to risk difference"
  },
  {
    "objectID": "02_est.v2.html#summarize-km-estimator",
    "href": "02_est.v2.html#summarize-km-estimator",
    "title": "Estimation",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator"
  },
  {
    "objectID": "02_est.v2.html#cox-proportional-hazards",
    "href": "02_est.v2.html#cox-proportional-hazards",
    "title": "Estimation",
    "section": "Cox Proportional Hazards",
    "text": "Cox Proportional Hazards\nThe data was designed so treatment has no causal effect on outcome, but confounding is present Data. Adjustment for confounder, X identifies null effort of assignment.\n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\nassign\n0.96\n0.90, 1.03\n0.3\n\n\nX\n1.90\n1.84, 1.97\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio"
  },
  {
    "objectID": "02_est.v2.html#estimation-1",
    "href": "02_est.v2.html#estimation-1",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\nThe PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time^2). Choose either to estimate the outcome in a model with both clones combined in one dataset OR estimate cumulative incidences separately (two models with data limited to assign==1 & assign==0 respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. time + time^2 + treat + treat*time + treat*time^2, shorthand below is poly(time, 2, raw=T)*assign.\n\n  # defined above\n1  d_glm = glm(event==0 ~ poly(time, 2, raw=T)*assign, data=dta_c_panel, family=binomial())\n  \n  d_plr_naive_est = crossing(assign = 1:0, time = 1:60)\n  \n  d_plr_naive_est$pr_surv = predict(d_glm, newdata = d_plr_naive_est, type = 'response') \n  d_plr_naive_est = d_plr_naive_est %&gt;%\n    group_by(assign) %&gt;%\n    mutate(pr_cumsurv = cumprod(pr_surv),\n2           pr_cumev = 1 - pr_cumsurv) %&gt;%\n3    ungroup %&gt;%\n    pivot_wider(., id_cols =c('time'),\n                names_from = assign,\n                names_prefix = 'pr_ev_',\n                values_from = pr_cumev\n    ) %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nPLR model, with time*treat interaction. Binomial family for logistic regression.\n\n2\n\nThe cumulative incidence is 1 - cumulative event-free survival probability. To compute, you take the cumulative product by assignment group.\n\n3\n\nReorganize data and summarize by group/time similar to above for the KM estimator.\n\n\n\n\n\n\n\nAt first glance, the plot looks reasonable. Direct comparison of the PLR and KM estimators is helpful for diagnosing problems in code or modeling steps.\n\n\n\n\n\nComparison of Naive PLR versus KM estimator\n\n\n\n\nSo the PLR model using a simple polynomial can reasonably approximate the KM estimate. This should work because the underlying synthetic data was generated with an exponential distribution for time, but real-world data will not play so nicely. There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider the following regarding the parametric time trend:\n\nIt may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.\nThe non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.\nThese initial steps will not guarantee a good fit after weighting is applied, it is only a first-look for diagnostic purposes."
  },
  {
    "objectID": "02_est.v2.html#comparison-of-covariate-adjusted-plr-with-cox-model",
    "href": "02_est.v2.html#comparison-of-covariate-adjusted-plr-with-cox-model",
    "title": "Estimation",
    "section": "Comparison of Covariate-adjusted PLR with Cox model",
    "text": "Comparison of Covariate-adjusted PLR with Cox model\nAnother approach is to compare a pooled logistic regression analysis to a time-invariant Cox model. If time-varying confounding not an issue, this should give a similar result. However, censoring weights are still necessary in final analysis even if no confounding.(Cain et al. 2010)\n\n# defined above\n1  d_glm = glm(event==0 ~ poly(time, 2, raw=T)*assign + female + age + X,\n              data=dta_c_panel, family=binomial())\n\n2  dta_c_panel$p.noevent0 &lt;- predict(d_glm, mutate(dta_c_panel, assign=0), type=\"response\")\n  dta_c_panel$p.noevent1 &lt;- predict(d_glm, mutate(dta_c_panel, assign=1), type=\"response\")\n  \n  setDT(dta_c_panel)\n3    dta_c_panel[, `:=`(pr_surv_1 = cumprod(p.noevent1)), by=list(id, assign)]\n    dta_c_panel[, `:=`(pr_surv_0 = cumprod(p.noevent0)), by=list(id, assign)]\n  setDF(dta_c_panel)\n  \n4  d_plr_adj_est = dta_c_panel %&gt;%\n    group_by(time) %&gt;%\n    summarize(pr_ev_1 = mean(1-pr_surv_1),\n              pr_ev_0 = mean(1-pr_surv_0), \n              .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nPLR model with some covariates for adjustment. (model fit for each treatment group)\n\n2\n\nPredict outcome under each assignment strategy.\n\n3\n\nEstimate cumulative survival using cumulative product, within person-clone\n\n4\n\nSummarize mean survival rates by time-period\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the outcome regression without weights is just for comparison and understanding, the final analysis must include probability weights for artificial censoring even if no confounding.\n\n\n\n  d_cox = coxph(Surv(enter, exit, event) ~ assign + female + age + X, data = dta_c_panel)\n\n  d_cox_est = surv_adjustedcurves(fit = d_cox, variable = \"assign\", data = dta_c_panel) %&gt;%\n      group_by(variable, time) %&gt;%\n        dplyr::filter(row_number()==1) %&gt;%\n      ungroup %&gt;%\n      mutate(pr_ev = 1 - surv) %&gt;%\n      pivot_wider(., id_cols =c('time'),\n                  names_from = variable,\n                  names_prefix = 'pr_ev_',\n                  values_from = pr_ev\n      ) %&gt;%\n      mutate(cid = pr_ev_1 - pr_ev_0,\n             cir = pr_ev_1 / pr_ev_0)\n\nFor comparison, A time-dependent Cox model is fit.\n\nCox model, with time1, time2 parameters which represent the start and stop time of each interval. Note that althought the cox model is time-dependent and you could adjust for X_t this could be problematic, because X_t also includes time-points post treatment. In our synthetic data, X_t is not a collider (Treat -&gt; X_t &lt;- Outcome) or on the causal path (Treat -&gt; X_t -&gt; Outcome).\nEstimation of adjusted survival curves by treatment group, with subpopulations balanced using conditional method.(“A Comparison of Different Methods to Adjust Survival Curves for Confounders - Denz - 2023 - Statistics in Medicine - Wiley Online Library,” n.d.)\nSummarizing survival probabilities by time\n\n\n\n\n\n\n\n\n\n\nThe PLR and Cox models both appropriately find no difference in risk by treatment assignment."
  },
  {
    "objectID": "02_est.v2.html#build-a-longitudinal-panel",
    "href": "02_est.v2.html#build-a-longitudinal-panel",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nThe person- or clone-level data must be expanded to a longitudinal panel where each observation (row) is a period of follow-up.\n\nCensoring dataset\nIn our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is specific to a project’s definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we can use the clones assigned to no treatment across all follow-up timepoints.\n\nThis may be a little confusing, but I am taking clones where assign=0 from the dta_c_panel dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.\nWe are estimating time to treatment, not outcome. The clone, assign=0 is already set up for this but for other projects this step will have to be modified if the comparator is different."
  },
  {
    "objectID": "02_est.v2.html#estimation-of-weights",
    "href": "02_est.v2.html#estimation-of-weights",
    "title": "Estimation",
    "section": "Estimation of weights",
    "text": "Estimation of weights\nThe probability weight, AKA inverse probability censoring weight (IPCW) or inverse probability of no loss to follow-up is estimated in this way:\n\\[\nW^C_i = \\prod^{T}_{t_0} \\frac{P(C_t = 0| \\overline{C}_{t-1} = 0)}{P(C_t=0 | X=x, \\overline{L}_{t-1}=l_{i, t-1}, \\overline{C}_{t-1}=0)}\n\\]\nC in this case means censoring, but censoring occurs according to treatment strategy, so it really is the probability of adhering to the treatment you were assigned to. Stabilized weights are tricky, here we are using the marginal stabilized weight which is the probability of no censoring at each timepoint. The denominator is the conditional probability accounting for fixed, i.e. baseline, (X) and time-varying (L) covariates.\nWe fit a model with the outcome of censoring, including the key variables. Then we add fitted probabilities back to the outcome dataset, and calculate cumulative probabilities and weights.\n\n  # Numerator (margin probability)\n1  d_glm_wt = glm(treat ~ poly(time, 2, raw=T),\n                 data=dta_c_panel[dta_c_panel$assign==0, ], family=binomial())\n  \n  dta_c_panel$pr_censnum = predict(d_glm_wt, newdata = dta_c_panel, type='response')\n\n  # Denominator \n2  d_glm_wt = glm(treat ~ poly(time, 2, raw=T) + X + X_t,\n                 data=dta_c_panel[dta_c_panel$assign==0, ], family=binomial())\n  \n  dta_c_panel$pr_censdenom = predict(d_glm_wt, newdata = dta_c_panel, type='response')\n\n\n1\n\nMarginal probability of no censoring.\n\n2\n\nConditional probability (covariate-adjusted) for no censoring\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of “censoring” where that may mean treatment initiation or some other thing.(Gaber et al. 2024)"
  },
  {
    "objectID": "02_est.v2.html#calculation-of-weights",
    "href": "02_est.v2.html#calculation-of-weights",
    "title": "Estimation",
    "section": "Calculation of weights",
    "text": "Calculation of weights\nThis step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to the Weighting section for further discussion.\n\n  setDT(dta_c_panel)\n  \n  dta_c_panel[, ipw := fcase(\n1    assign==0 & censor==0, 1 / (1-pr_censdenom),\n    assign==0 & censor==1, 0,\n2    assign==1 & time &lt; 12, 1,\n3    assign==1 & time == 12  & t_treat  &lt; 12, 1,\n4    assign==1 & time == 12  & t_treat  ==12 & censor==0, 1 / (pr_censdenom),\n5    assign==1 & time == 12  & t_treat  &gt;12 & event==0, 0,\n6    assign==1 & time == 12  & t_treat  &gt;12 & event==1, 1,\n7    assign==1 & time &gt; 12, 1\n  )]\n  \n8  dta_c_panel[, marg_ipw := fcase(\n    assign==0 & censor==0, (1-pr_censnum) / (1-pr_censdenom),\n    assign==1 & time == 12 & t_treat   ==12 & censor==0, pr_censnum / (pr_censdenom),\n    default = ipw\n  )]\n  \n9  dta_c_panel[, `:=`(ipw = cumprod(ipw),\n                     marg_ipw = cumprod(marg_ipw)),\n               by=list(id, assign)]\n\n\n1\n\n(assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored\n\n2\n\n(assign=1) Clones cannot artificially censor prior to grace period\n\n3\n\n(assign=1) If treatment started prior to grace window ending, the clone cannot censor\n\n4\n\n(assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).\n\n5\n\n(assign=1) If a clone is not treated, and does not die (event=1) they censor at the end of the window\n\n6\n\nIf died in last interval (event=1), do not set weight to zero. This is a misstep others have done, if the person dies at the end of the interval, then that is still consistent with treatment strategy (i.e. they died before end of grace window when they were supposed to recieve treatment) so they are not censor=1 and death is counted.\n\n7\n\n(assign=1) Set post-grace period weights = 1 for all.\n\n8\n\nFor a marginal IPW, numerator of 1 is replaced with marginal probability of censoring at each tiempoint.\n\n9\n\nAfter setting these conditions, compute the cumulative product of the weights.\n\n\n\n\n\nOverall IPW Distribution\n\nUnstabilized weights\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.148   1.922   1.496 159.935 \n\n\n\n\nMarginal Stabilized weights\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3632  0.9531  1.0000  0.9923  1.0006  5.2056 \n\n\nThe unstabilized weights floor at 1, and we see high weights assigned to some. The marginal weights have a mean of 1 (expected).\n\n\n\nIPW Distribution at end of grace period for treatment clones\nThe weights at the end of the grace period are key so its good to examine them directly:\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   5.234   1.000 159.935 \n\n\n\n\n\n\n\n\nNote\n\n\n\nBe very careful with IPW truncation with this design. If the probability of treatment is very low, then those treated at the end of the grace period will have very large weights. If you truncate at 99% for example, it could mostly just truncate the weights of those treated at the end of the grace period, and this could severely bias estimates. These persons are meant to account for all those being artificially censored in the treatment group for non-treatment and so will likely have large weights by design.\n\n\nYou can also plot weights across time for diagnostics:\n\ndta_c_panel %&gt;%\n    dplyr::filter(marg_ipw!=0) %&gt;%\n ggplot(., aes(x = cut_width(time, 12), y = marg_ipw)) +\n  geom_violin(aes(group = cut_width(time, 12)), \n               scale = \"width\", fill = cbbPalette[1], alpha = 0.5) +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               color = cbbPalette[2], size=2) +\n  geom_hline(aes(yintercept=1),linewidth=1.1, linetype=3) +\n  scale_x_discrete(labels = seq(0, 60, 12)) +\n  labs(x = \"Follow-up\", y = \"IPW\", ) +\n  theme_bw()\n\n\n\n\nDistribution of Censoring Weights Across Time\n\n\n\n\nAnother way to examine the weights is to look at the weighted counts of individuals at risk, and number of events pre- and post-weighting:\n\ndta_c_panel %&gt;%\n  group_by(time, assign) %&gt;%\n  summarize(n = sum(ipw!=0), \n            n_wt = sum(ipw),\n            .groups = 'drop') %&gt;%\n  pivot_wider(id_cols = time, names_from = assign, values_from = c(n, n_wt)) %&gt;%\n  dplyr::filter(time %in% c(1, 12, 30, 60)) %&gt;%\n  rename(`Time` = time,\n         `Unweighted, assign=0` = n_0,\n         `Unweighted, assign=1` = n_1,\n         `Weighted, assign=0` = n_wt_0,\n         `Weighted, assign=1` = n_wt_1,\n         ) %&gt;%\n  kable(align='c', digits =0) %&gt;%\n  kable_styling()\n\n\nUnweighted and Weighted Counts, N at risk\n\n\nTime\nUnweighted, assign=0\nUnweighted, assign=1\nWeighted, assign=0\nWeighted, assign=1\n\n\n\n\n1\n9829\n10000\n9999\n10000\n\n\n12\n7652\n1712\n9344\n8961\n\n\n30\n5094\n1469\n8207\n7862\n\n\n60\n2469\n971\n5830\n5564\n\n\n\n\n\n\n\nThe size of the unweighted, and weighted counts may identify problems.\nNow with the estimated weights, it is simple to generate weighted cumulative incidences:\n\n1  d_glm_pe_1 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=dta_c_panel[assign==1, ],\n                   family=binomial(), weights = ipw)\n\n  d_glm_pe_0 = glm(event==0 ~ poly(time, 2, raw=T),\n                   data=dta_c_panel[assign==0, ],\n                     family=binomial(), weights = ipw)\n  \n2  dta_c_panel$pr_1 = predict(d_glm_pe_1, newdata = dta_c_panel,\n                             type='response')\n  dta_c_panel$pr_0 = predict(d_glm_pe_0, newdata = dta_c_panel,\n                             type='response')\n\n\n1\n\nEstimate for each assignment group separately. This isn’t necessary but its good practice, and if you are adjusting for other covariates may give different results. Note the weights = ipw argument. R glm() will generate a warning message because the weighted counts are “non-integer”, but this is expected and not a problem.\n\n2\n\nFitted probabilities from each model\n\n\n\n\n\n1  dta_c_panel[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)]\n  dta_c_panel[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)]\n  \n2  d_plrwt_est = dta_c_panel %&gt;%\n    group_by(time) %&gt;%\n    summarize(pr_ev_1 = mean(1-pr_cum_1),\n              pr_ev_0 = mean(1-pr_cum_0), \n              .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nCumulative product\n\n2\n\nSummarize across group, time as before."
  },
  {
    "objectID": "01_syndata.v3.html",
    "href": "01_syndata.v3.html",
    "title": "Simulate data for CCW project",
    "section": "",
    "text": "Causal Directed Acyclic Graph\nWe assume a generic treatment, A, has no causal effect on generic outcome, Y. However, confounding is present through another random variable. The DAG includes age, X a confounder, gender defined through a binary variable F for female. X is both a baseline variable, and a time-varying confounder. Below we assign effects of these variables that will lead to identification of a causal effect between A and Y, if no controlling for the confounding is done.\n\n\n\n\n\nFigure 1. Causal DAG of treatment and outcome\n\n\n\n\nIf a naive analysis (no adjustment) was performed the estimate will be biased, because treatment and outcome are d-connected.\n\n\n\n\n\nFigure 2. D-connected treatment and outcome\n\n\n\n\n\n\n\n\n\nFigure 3. D-separated treatment and outcome after adjustment\n\n\n\n\nAdjustment for X should lead to identification of a NULL effect of treatment on outcome.\n\n\nSimulation Parameters\n\nset.seed(100)\n1n = 10000L\nfup = 90\n\ndf = tibble(id = 1:n, \n2            age = round(rnorm(n, mean = 75, sd = 10), 1),\n            female = sample(c(1, 0), size = n, replace = TRUE, prob = c(0.66, 0.34)),\n            fup = fup,\n            X = rnorm(n, 0, sd=1),\n            X_shift = rexp(n, rate=0.005))\n  \nsetDT(df) \n3d_panel = df[rep(seq(.N), fup)]\nd_panel[, exit := (seq_len(.N)), by = list(id)]\nd_panel[, enter := exit-1]\nd_panel[, time := seq_len(.N), by = list(id)]\n\n\n1\n\nSpecify 10000 individuals with 90 observation periods of follow-up.\n\n2\n\nSpecify covariates, age, female and X.\n\n3\n\nExpand dataset, each observation period is an interval of length = 1.\n\n\n\n\nFor X, further include X_t which varies across time. A change in X_t is triggered by intermediate variable X_shift which is a random draw from an exponential distribution (meant to represent a time to change in X). So for each person, there is a baseline X, then a random timepoint in follow-up where X changes.\n\nd_panel[, X_t := fifelse(X_shift &lt;= time, X+rnorm(0, n=1), X), by = list(id)] \n\n\n\n\n\n\nFigure 4. Distribution of Time-varying Confounder\n\n\n\n\nTime to treatment and outcome is simulated in a longitudinal dataset, to allow for time-varying effect of X. Then the dataset is modified so that persons follow-up ends at the time of outcome:\n\n  set.seed(101)\n  \n  d_panel_2 = d_panel %&gt;% \n1    # Treat event\n    mutate(log_odds = -4.1 + 0.0001*time + -0.00001*(time^2) + 0.2*X + 0.2*X_t,\n           p = exp(log_odds) / (1 + exp(log_odds)),\n           treat = rbinom(n(), size = 1, prob = p),\n           treat = if_else(treat==1, 1, NA_integer_)\n    ) %&gt;%\n    group_by(id) %&gt;%\n      fill(treat, .direction = 'down') %&gt;%\n    ungroup %&gt;%\n    mutate(treat = coalesce(treat, 0L)) %&gt;%\n2    # Outcome event\n    mutate(log_odds = -5 + 0.009*time + 0.0003*(time^2) + 0.4*X + 0.3*X_t + \n                                  -0.2*female + 0.07*age + -0.001*(age^2),\n           p = exp(log_odds) / (1 + exp(log_odds)),\n           outcome = rbinom(n(), size = 1, prob = p),\n           outcome = if_else(outcome==1, 1, NA_integer_)\n    ) %&gt;%\n    group_by(id) %&gt;%\n      fill(outcome, .direction = 'down') %&gt;% \n    ungroup %&gt;%\n    mutate(outcome = coalesce(outcome, 0L)) %&gt;%\n3    group_by(id) %&gt;%\n      mutate(t_outcome = coalesce(min(time[outcome==1]), Inf),\n             t_treat   = coalesce(min(time[treat==1]), Inf)) %&gt;%\n    ungroup %&gt;%\n    dplyr::filter(outcome  == 0 | time == t_outcome) %&gt;%\n    dplyr::filter(time &lt;= 60) %&gt;%\n    mutate(t_treat = if_else(t_outcome &lt; t_treat, Inf, t_treat),\n           t_treat = if_else(60 &lt; t_treat, Inf, t_treat))\n\n\n1\n\nSimulation of treatment event times. Treatment is only a function of time, X, and X_t.\n\n2\n\nSimulation of outcome event times. Outcome is a function of time, X, and X_t, F, Age.\n\n3\n\nModify dataset so that follow-up times coincide with new treatment and outcome variables.\n\n\n\n\n\nSimulated treatment times\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1      18      48     Inf     Inf     Inf \n\n\n\n\nSimulated outcome times\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1      57      76     Inf     Inf     Inf \n\n\n\n\n\n\n\nFigure 5. Treatment and Outcome Incidence Across Follow-up\n\n\n\n\n\n\n\n\n\nFigure 6. Correlation plot of covariates, treatment, outcome\n\n\n\n\nSimple correlation statistics support the simulated dataset has target associations.\n\n\n\nSummary of survival dataset\n\n\nRows: 484,775\nColumns: 16\n$ id        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ age       &lt;dbl&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ female    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fup       &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, …\n$ X         &lt;dbl&gt; -0.7311102, -0.7311102, -0.7311102, -0.7311102, -0.7311102, …\n$ X_shift   &lt;dbl&gt; 37.27515, 37.27515, 37.27515, 37.27515, 37.27515, 37.27515, …\n$ exit      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ enter     &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ time      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ X_t       &lt;dbl&gt; -0.7311102, -0.7311102, -0.7311102, -0.7311102, -0.7311102, …\n$ log_odds  &lt;dbl&gt; -5.502477, -5.492577, -5.482077, -5.470977, -5.459277, -5.44…\n$ p         &lt;dbl&gt; 0.004060109, 0.004100338, 0.004143439, 0.004189494, 0.004238…\n$ treat     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ outcome   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ t_outcome &lt;dbl&gt; 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, …\n$ t_treat   &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, …\n\n\n\n\nCloning procedure\nThe cloning procedure is very project specific, tied to how the treatment strategy is defined so it is hard to give general recommendations here. For this tutorial, we describe an arbitrary window in which treatment is expected to initiate and outline strategies around this:\nTreatment Strategies: \n\nDo not ever receive treatment\nInitiate treatment within 12 time periods (days, weeks etc.) and if not then treatment will initiate on week 12\n\n\n\n\n\n\n\nNote\n\n\n\nThe grace window is a funny concept when you first consider it. This does not reflect any trial I have ever heard of actually being done but may identify an interesting or important effect. It is essentially outlying a treatment “protocol” and saying what if everyone adhered to this protocol counterfactual to what was observed.\n\n\nTo clone, you make two copies of the data, and make a new artifical censoring variable, which is a censoring time for the period when clones observed data are no longer consistent with their assigned strategy.\n\n1data_cloned = bind_rows(\n                     d_panel_2 %&gt;%\n                       mutate(assign = 0,\n                              censor = if_else(t_treat &lt;= time, 1L, 0L),\n                              event = if_else(censor==0 & t_outcome&lt;=time, 1L, 0L),\n                       ),\n2                     d_panel_2 %&gt;%\n                       mutate(assign = 1,\n                              censor = if_else(t_treat &gt; 12 & time&gt;=12, 1L, 0L),\n                              event  = if_else(censor==0 & t_outcome&lt;=time, 1L, 0L)\n                              )\n                       ) %&gt;%\n  arrange(id, assign, time) %&gt;%\n  group_by(id, assign) %&gt;%\n  mutate(t_censor = min(time[censor==1])) %&gt;%\n3  dplyr::filter(censor == 0 | time == min(time[censor==1])) %&gt;%\n  ungroup %&gt;%\n  select(id, time, t_outcome, t_treat, t_censor, censor, event, assign, enter, exit, everything(), -fup, -X_shift, -log_odds, -p)\n\n\n1\n\nClones assigned to strategy 1 (No treatment)\n\n2\n\nClones assigned to strategy 2 (grace window for treatment)\n\n3\n\nUpdate failure times and events counting for artificial censoring\n\n\n\n\n\n\nSummary of simulated, cloned data-set\n\n\n\n\n\n\n\n\n\n\n\n\nassign\n0\n1\n\n\nUnique persons\n10000\n10000\n\n\nMedian (Follow-up)\n30\n12\n\n\nTotal treat (ever)\n5149\n5149\n\n\nTotal treat (t&lt;=12)\n1775\n1775\n\n\nTotal events\n2422\n1382\n\n\nTotal censored\n5149\n7652\n\n\n\n\n\n\n\nA person-level version of the data is also created (excluded time-varying X) for comparison:\n\ndata_cloned_p = data_cloned %&gt;%\n  group_by(id, assign) %&gt;%\n    mutate(t_censor = min(time[censor==1])) %&gt;%\n  ungroup %&gt;%\n  mutate(time = pmin(t_censor, t_outcome, 60),\n         event = case_when(\n           time==t_censor ~ 0, \n           time==t_outcome ~ 1,\n           T ~ 0)) %&gt;%\n  select(id, assign, t_treat, t_outcome, t_censor, time, event, X, female, age) %&gt;%\n  distinct()"
  },
  {
    "objectID": "03_inference.html",
    "href": "03_inference.html",
    "title": "Inference",
    "section": "",
    "text": "In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort, or comparison of those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals) but will provide a standardized mean difference.\nSome special considerations are needed when presenting target trial emulations.\n\n\nThe cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)\nAssume a simple example where the design of the study includes a baseline period, “time zero” and two treatment strategies, with a grace period or interval where one strategy allows some time for treatment to occur.There being two main periods: 1) Baseline time zero, 2) End of the grace period.\nHere is one approach:\n\nPresent the baseline characteristics for everyone (prior to cloning) at time zero.\nPresent the characteristics for both treatment groups at the end of the grace period.\n\nThis would lead to a “Table 1” with three columns.\n\n\n\nMany R packages will construct a “Table 1” for users. I manually code below for transparency.\n\n# Many packages can do this for you, I write out manually for transparency\n\nd_cbase = dta_c_panel %&gt;%\n  # Take one obs for the baseline column\n  dplyr::filter(time==1 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_c0 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==0 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n  \nd_c1 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_tab1 = bind_cols(Variables = c('N', 'X, mean (SD)', 'Xt, mean (SD)', 'Age (years)', 'Gender, female'),\n          `Baseline` = d_cbase, `Assign=0` = d_c0, `Assign=1` = d_c1) \n\n\n\n\nTable 1. Example of presentation of characteristics of a CCW Cohort\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\n\n\n\n\nN\n10,000\n7,652\n1,712\n\n\nX, mean (SD)\n0 (0.99)\n-0.12 (0.96)\n0.26 (0.97)\n\n\nXt, mean (SD)\n0 (0.99)\n-0.12 (1)\n0.26 (0.98)\n\n\nAge (years)\n75 (9)\n75 (9)\n75 (9)\n\n\nGender, female\n6,657 (66.6)\n5,098 (66.6)\n1,160 (67.8)\n\n\n\n\n\n\n\n\n\n\nMost researchers would provide the naive differences between groups, and then present some adjusted difference. Most commonly the standardized mean differences (aSMD) or a P-value from a univariate test (not recommended).\nSo you could add one column for the SMD to the “Table 1”, but there are also other approaches such as reporting Mahalanobis distance.\n\n\n\nd_smd = dta_c_panel %&gt;%\n  dplyr::filter(time==12 & censor==0) %&gt;%\n  summarize(`X, mean (SD)` = (mean(X[assign==1]) - mean(X[assign==0])) / sd(X),\n            `Xt, mean (SD)` = (mean(X_t[assign==1]) - mean(X_t[assign==0])) / sd(X_t),\n            `Age, years (SD)`  = (mean(age[assign==1]) - mean(age[assign==0])) / sd(age),\n            `Gender, female` = (mean(female[assign==1]) - mean(female[assign==0])) / sd(female)\n         ) %&gt;%\n  t() %&gt;%\n  as_tibble(rownames = 'Variable') %&gt;%\n  rename(`aSMD` = V1)\n\nd_tab1 = left_join(d_tab1, d_smd, by=c('Variables' = 'Variable'))\n\n\n# Compute pooled covariance matrix\ncov_pooled &lt;- (var(dta_c_panel[dta_c_panel$assign==1, c('X', 'X_t', 'age', 'female')]) + \n                 var(dta_c_panel[dta_c_panel$assign==0, c('X', 'X_t', 'age', 'female')])) / 2\n\n# Compute Mahalanobis distance\nmahal_dist &lt;- mahalanobis(colMeans(dta_c_panel[dta_c_panel$assign==1, \n                                               c('X', 'X_t', 'age', 'female')]), \n                          center = colMeans(dta_c_panel[dta_c_panel$assign==0, \n                                                        c('X', 'X_t', 'age', 'female')]), \n                          cov = cov_pooled)\n\n\n\n\n\n\n\nTable 1 + aSMD (unweighted)\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\n\n\n\n\nN\n10,000\n7,652\n1,712\nNA\n\n\nX, mean (SD)\n0 (0.99)\n-0.12 (0.96)\n0.26 (0.97)\n0.389\n\n\nXt, mean (SD)\n0 (0.99)\n-0.12 (1)\n0.26 (0.98)\n0.377\n\n\nAge (years)\n75 (9)\n75 (9)\n75 (9)\nNA\n\n\nGender, female\n6,657 (66.6)\n5,098 (66.6)\n1,160 (67.8)\n0.024\n\n\n\n\n\n\n\nNote: Mahalanobis distance between groups, Naive: 0.072\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee Estimators for full description of weight estimation\n\n\n\n\n\n\n\n\n\n\n\nTable 1 + aSMD (unweighted)\n\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\nIPW aSMD\n\n\n\n\nN\n10,000\n7,652\n1,712\nNA\nNA\n\n\nX, mean (SD)\n0 (0.99)\n-0.12 (0.96)\n0.26 (0.97)\n0.3893079\n-0.0410679\n\n\nXt, mean (SD)\n0 (0.99)\n-0.12 (1)\n0.26 (0.98)\n0.3773564\n-0.0331280\n\n\nAge (years)\n75 (9)\n75 (9)\n75 (9)\nNA\nNA\n\n\nGender, female\n6,657 (66.6)\n5,098 (66.6)\n1,160 (67.8)\n0.0240822\n0.0027052\n\n\n\n\n\n\n\nNote. Mahalanobis distance between groups, Naive: 0.072, IPW: 0.003\n\n\n\n\nThese differences can also be presented in a modified “Love” plot, with panels for each period of follow-up. It doesn’t really make sense to do one at baseline because the differences will be small/none. But we can pick a few follow-up periods to evaluate:\n\n# aSMD Figure ----\n  i_SMD_grps = c(12, 24, 60)\n\n  d_time = dta_c_panel %&gt;%\n    dplyr::filter(time %in% c(i_SMD_grps) & censor==0) %&gt;%\n    nest(.by = time) \n  \n  d_time$SMD_naive = map(d_time$data, ~f_cpt_dist(.))\n  d_time$SMD_ipw = map(d_time$data, ~f_cpt_dist_wt(.))\n  \n  gg_bal = select(d_time, time, SMD_naive, SMD_ipw) %&gt;%\n    unnest() %&gt;%\n    select(-Variable1) %&gt;%\n    rename(`Naive` = SMD,\n           `IPW` = SMD1) %&gt;%\n      pivot_longer(cols = c('Naive', 'IPW'), \n                   names_to = 'Model', values_to = 'SMD') %&gt;% \n      mutate(flag = if_else(abs(SMD)&gt;0.1, cbbPalette[1], cbbPalette[2])) %&gt;%\n      ggplot(., aes(y = Variable, x = SMD, group = Model)) +\n      geom_point(aes(shape=Model, color = flag), size=1.8) +\n      geom_vline(xintercept = 0, linetype=2) +\n      geom_vline(xintercept = 0.2, linetype=2, color=cbbPalette[3]) +\n      scale_color_identity() + \n      facet_grid(cols = vars(time)) +\n      theme_classic() +\n      labs(x = 'SMD', y = 'Variable',\n           caption = 'weighted differences in uncensored risk set')\n  \n  gg_bal\n\n\n\n\nFigure 1: Baseline and Time-varying Differences by Treatment Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can see the weighted between group differences are significantly reduced by the unstabilized IPW. However, caution in interpretation of the differences. If the treatment/exposure of interest has a causal relationship with the outcome (or some other censoring mechanism besides the artificial censoring the weighted account for), then you might expect there to be differences between groups. This is because the groups are conditional on remaining alive, uncensored at time-point 12, which may occur after treatment/exposure. In effect you are conditioning on a collider (outcome)."
  },
  {
    "objectID": "03_inference.html#presentation-of-weighted-differences",
    "href": "03_inference.html#presentation-of-weighted-differences",
    "title": "Inference",
    "section": "",
    "text": "Most researchers would provide the naive differences between groups, and then present some adjusted difference. Most commonly the standardized mean differences (aSMD) or a P-value from a univariate test (not recommended).\nSo you could add one column for the SMD to the “Table 1”, but there are also other approaches such as reporting Mahalanobis distance.\n\n\n\nd_smd = dta_c_panel %&gt;%\n  dplyr::filter(time==12 & censor==0) %&gt;%\n  summarize(`X, mean (SD)` = (mean(X[assign==1]) - mean(X[assign==0])) / sd(X),\n            `Xt, mean (SD)` = (mean(X_t[assign==1]) - mean(X_t[assign==0])) / sd(X_t),\n            `Age, years (SD)`  = (mean(age[assign==1]) - mean(age[assign==0])) / sd(age),\n            `Gender, female` = (mean(female[assign==1]) - mean(female[assign==0])) / sd(female)\n         ) %&gt;%\n  t() %&gt;%\n  as_tibble(rownames = 'Variable') %&gt;%\n  rename(`aSMD` = V1)\n\nd_tab1 = left_join(d_tab1, d_smd, by=c('Variables' = 'Variable'))\n\n\n# Compute pooled covariance matrix\ncov_pooled &lt;- (var(dta_c_panel[dta_c_panel$assign==1, c('X', 'X_t', 'age', 'female')]) + \n                 var(dta_c_panel[dta_c_panel$assign==0, c('X', 'X_t', 'age', 'female')])) / 2\n\n# Compute Mahalanobis distance\nmahal_dist &lt;- mahalanobis(colMeans(dta_c_panel[dta_c_panel$assign==1, \n                                               c('X', 'X_t', 'age', 'female')]), \n                          center = colMeans(dta_c_panel[dta_c_panel$assign==0, \n                                                        c('X', 'X_t', 'age', 'female')]), \n                          cov = cov_pooled)\n\n\n\n\n\n\n\nTable 1 + aSMD (unweighted)\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\n\n\n\n\nN\n10,000\n7,652\n1,712\nNA\n\n\nX, mean (SD)\n0 (0.99)\n-0.12 (0.96)\n0.26 (0.97)\n0.389\n\n\nXt, mean (SD)\n0 (0.99)\n-0.12 (1)\n0.26 (0.98)\n0.377\n\n\nAge (years)\n75 (9)\n75 (9)\n75 (9)\nNA\n\n\nGender, female\n6,657 (66.6)\n5,098 (66.6)\n1,160 (67.8)\n0.024\n\n\n\n\n\n\n\nNote: Mahalanobis distance between groups, Naive: 0.072\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee Estimators for full description of weight estimation\n\n\n\n\n\n\n\n\n\n\n\nTable 1 + aSMD (unweighted)\n\n\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\naSMD\nIPW aSMD\n\n\n\n\nN\n10,000\n7,652\n1,712\nNA\nNA\n\n\nX, mean (SD)\n0 (0.99)\n-0.12 (0.96)\n0.26 (0.97)\n0.3893079\n-0.0410679\n\n\nXt, mean (SD)\n0 (0.99)\n-0.12 (1)\n0.26 (0.98)\n0.3773564\n-0.0331280\n\n\nAge (years)\n75 (9)\n75 (9)\n75 (9)\nNA\nNA\n\n\nGender, female\n6,657 (66.6)\n5,098 (66.6)\n1,160 (67.8)\n0.0240822\n0.0027052\n\n\n\n\n\n\n\nNote. Mahalanobis distance between groups, Naive: 0.072, IPW: 0.003"
  },
  {
    "objectID": "03_inference.html#bootstrapping",
    "href": "03_inference.html#bootstrapping",
    "title": "Inference",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n\n\n\n\n\nNote\n\n\n\nA Poisson bootstrap procedure is used here, see:Additional Topics for some notes on this."
  },
  {
    "objectID": "03_inference.html#inference-statistics",
    "href": "03_inference.html#inference-statistics",
    "title": "Inference",
    "section": "Inference statistics",
    "text": "Inference statistics\nInferential statistics\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "05_appendix.html",
    "href": "05_appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Note\n\n\n\nGrace period case example For working through concepts below, assume we are emulating a trial where eligible individuals are followed for up to 60 time periods from an arbitrary calendar date, and are randomly assigned to two treatment strategies:\n\nDo not receive treatment through the end of follow-up (60 time periods, e.g. 60 months ~ 5 years)\nInitiate treatment within 12 periods of index time zero\n\nIndividuals are followed from the index date through the end of follow-up as long as the observed data is consistent with their assigned treatment strategy. When the observed data becomes inconsistent, they are artificially censored at that point in time (i.e. our emulation will estimate per protocol effects from an RCT with perfect adherence).\n\n\nIt is possible to specify two or more interventions where a single person’s observed data is consistent with multiple strategies at a single point in time. Under both treatment strategies if the person does not get treatment for the first 5 periods, then up to period 12 they are faithfully adherent to both treatment strategies. At t=12, if they still are untreated they would continue to follow-up under the no treatment strategy but be non-adherent to the “treatment in 12 periods” strategy. This 12 period window we outline in our strategy is referred to as a “grace period” or “grace window”. Alternatively, if the person was assigned to “no treatment” and received treatment at any point in the 12 periods they would be censored upon initiation of treatment under the 1) “no treatment strategy” but follow-up continue under the 2) “treat within 12” strategy.\nIn a target trial emulation which employs these methods, each person’s observed data may be consistent with 1+ treatment strategies in the same time period, particularly the initial period of time. Because at baseline the person’s observed data is consistent with 1+ more strategies, the typical solution is to clone the person and assign each clone to a treatment strategy, e.g. duplicate or triplicate the person’ data and follow each clone as a unique observation.\n\n\n\n\n\n\nNote\n\n\n\nThe person could also be randomly assigned to one of the interventions (instead of cloned and assigned to both), but simulations suggest this is very statistically inefficient (much less precise estimates than cloning).\n\n\nThe cloning approach has an appealing effect of eliminating confounding at baseline (time zero). This is true because the observations in each treatment group are identical at baseline. However, as each clone is followed across time periods, their adherence to the assigned treatment strategy is monitored and each clone is censored (follow-up ends) when the observed data is no longer consistent with the assigned treatment strategy.\nThis censoring is artificial because we can still observe data for that person but we choose to ignore it because of their non-adherence to the assigned strategy. So although identical at baseline the groups observed characteristics (distributions of covariates) will diverge across time if the artificial censoring is informative and non-random (almost always true). Usually the artificial censoring is intrinsically linked to initiation of treatment, so the artificial censoring will introduce bias if the initiation of treatment also has common causes with the outcome of interest.\nIn order to adjust for this bias, inverse probability weighting is used. However, proper application of the weights requires some consideration of the probability of treatment initiation, grace periods and the artificial censoring mechanism."
  },
  {
    "objectID": "05_appendix.html#simple-example-probability-weighting-with-a-grace-period",
    "href": "05_appendix.html#simple-example-probability-weighting-with-a-grace-period",
    "title": "Appendix",
    "section": "Simple example: Probability weighting with a grace period",
    "text": "Simple example: Probability weighting with a grace period\nAssume three persons are assigned to a treatment strategy as follows: Get the treatment within 2 time periods, if you don’t get it by the second period you are censored for non-adherence. The goal of weighting is to create a pseudo-population that can estimate the effect in a counterfactual world where all those assigned to treatment take treatment by the end of the grace period, and all those assigned to not take treatment do not take it. In other words, we are estimating the causal effects of treatment in a trial where there is perfect adherence to study assignment, also known as a “per protocol” effect.\n\n\n\n\n\n\nNote\n\n\n\nObserved treatments:\nA: treated at last period\nB: never treated\nC: treated at first period\n\n\n\n\n\nSimple Weighting Example\n\n\nPerson\nPeriod\nTreat\nCensor\n\n\n\n\nA\n1\n0\n0\n\n\nA\n2\n1\n0\n\n\nB\n1\n0\n0\n\n\nB\n2\n0\n1\n\n\nC\n1\n1\n0\n\n\nC\n2\n0\n0\n\n\n\n\n\n\n\nThree persons have observed data in our trial emulation. Persons A & C get the treatment and are not censored, Person B doesn’t get the treatment and is censored.\nNow adding censoring probabilities Pr(C=1) to the table:\n\n\n\nSimple Weighting Example - Censoring Probabilities\n\n\nPerson\nPeriod\nTreat\nCensor\nPr(C=1)\n\n\n\n\nA\n1\n0\n0\n0\n\n\nA\n2\n1\n0\n1/2\n\n\nB\n1\n0\n0\n0\n\n\nB\n2\n0\n1\n1/2\n\n\nC\n1\n1\n0\n0\n\n\nC\n2\n0\n0\n-\n\n\n\n\n\n\n\nBecause people have 2 periods to get treatment, the Pr(C=1) in period 1 is 0 for all persons. In other words, people cannot be artificially censored in the first period, because by design we are allowing a grace of 2 periods. The Pr(C=1)= ½ for person A and B even though there are three people (i.e. ⅓)? Because person C already received the treatment in period 1, they are immutably uncensored from that point forward. So the only two who can censor are persons A and B, since person B censors in period 2, the Pr(C=1) = ½.\nThe probability of not censoring, Pr(C=0) is simply 1 - Pr(C=1.\n\n\n\nSimple Weighting Example - Censoring Probabilities\n\n\nPerson\nPeriod\nTreat\nCensor\nPr(C=1)\nPr(C=0)\n\n\n\n\nA\n1\n0\n0\n0\n1\n\n\nA\n2\n1\n0\n1/2\n1/2\n\n\nB\n1\n0\n0\n0\n1\n\n\nB\n2\n0\n1\n1/2\n1/2\n\n\nC\n1\n1\n0\n0\n1\n\n\nC\n2\n0\n0\n-\n1\n\n\n\n\n\n\n\nThe unstabilized inverse probability weight is 1 / Pr(C=0); in simple terms people who are likely to censor but dont are assigned higher weights and count for more in the analysis.\n\n\n\nSimple Weighting Example - IPW\n\n\nPerson\nPeriod\nTreat\nCensor\nPr(C=1)\nPr(C=0)\nIPW\n\n\n\n\nA\n1\n0\n0\n0\n1\n1\n\n\nA\n2\n1\n0\n1/2\n1/2\n2\n\n\nB\n1\n0\n0\n0\n1\n1\n\n\nB\n2\n0\n1\n1/2\n1/2\n0\n\n\nC\n1\n1\n0\n0\n1\n1\n\n\nC\n2\n0\n0\n-\n1\n1\n\n\n\n\n\n\n\nPerson A is given a weight of 2, making up for the loss of censored person B. Person C is given a weight of 1 because they received treatment in the first period, and so couldn’t censor at the end of the grace period. Three person, and the sum of the weights in both periods is 3.\n\n\n\n\n\n\nNote\n\n\n\nHow does one carry the IPW weight forward after the grace period? In this simple scheme, assign a weight of 1 for each time period after grace, and compute a cumulative product of the weights for each timepoint up until the end of follow-up. So person B, IPW=2, person C, IPW=1, for each subsequent time-point. However, if another censoring mechanism was important (i.e. censor due to death or loss to follow-up), then you might compute that probability separately and take the product of the two."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "",
    "text": "Data: Discusses creation of synthetic data used in this examples\nEstimation: Estimation describes an step-by-step approach for CCW methods.\nInference; Discussion and practical on obtaining confidence intervals.\n\nAdditional Topics: Some various related topics, how to speed up computation etc.\nAppendix: Some notes on inference and weighting approach, that may be helpful in adapting this tutorial to a specific project.\n\nFor production notes and future efforts see About."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe tutorial represents my gathered and organized notes from research projects and didactic training. Collaborators and mentors include: Issa Dahabreh, Kaley Hayes, Daniel Harris, Donald Miller and Andrew Zullo."
  },
  {
    "objectID": "03_inference.html#table-1",
    "href": "03_inference.html#table-1",
    "title": "Inference",
    "section": "",
    "text": "Many R packages will construct a “Table 1” for users. I manually code below for transparency.\n\n# Many packages can do this for you, I write out manually for transparency\n\nd_cbase = dta_c_panel %&gt;%\n  # Take one obs for the baseline column\n  dplyr::filter(time==1 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_c0 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==0 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n  \nd_c1 = dta_c_panel %&gt;%\n  # End of grace period\n  dplyr::filter(time==12 & assign==1 & censor==0) %&gt;%\n  summarize(N = prettyNum(n(), big.mark=','),\n            X = paste0(round(mean(X), 2), ' (', round(sd(X), 2), ')'),\n            X_t = paste0(round(mean(X_t), 2), ' (', round(sd(X_t), 2), ')'),\n            `Age, years (SD)`  = paste0(floor(mean(age)), ' (', floor(sd(age)), ')'),\n            `Gender, % Female` = paste0(prettyNum(sum(female==1), big.mark = ','), \n                                        ' (', round(mean(female)*100,1), ')')) %&gt;%\n  t()\n\nd_tab1 = bind_cols(Variables = c('N', 'X, mean (SD)', 'Xt, mean (SD)', 'Age (years)', 'Gender, female'),\n          `Baseline` = d_cbase, `Assign=0` = d_c0, `Assign=1` = d_c1) \n\n\n\n\nTable 1. Example of presentation of characteristics of a CCW Cohort\n\n\n\n\n\n\n\n\n\n\nUncensored at end of grace period\n\n\n\nVariables\nBaseline\nAssign=0\nAssign=1\n\n\n\n\nN\n10,000\n7,652\n1,712\n\n\nX, mean (SD)\n0 (0.99)\n-0.12 (0.96)\n0.26 (0.97)\n\n\nXt, mean (SD)\n0 (0.99)\n-0.12 (1)\n0.26 (0.98)\n\n\nAge (years)\n75 (9)\n75 (9)\n75 (9)\n\n\nGender, female\n6,657 (66.6)\n5,098 (66.6)\n1,160 (67.8)"
  },
  {
    "objectID": "03_inference.html#cloning-grace-period",
    "href": "03_inference.html#cloning-grace-period",
    "title": "Inference",
    "section": "",
    "text": "The cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)\nAssume a simple example where the design of the study includes a baseline period, “time zero” and two treatment strategies, with a grace period or interval where one strategy allows some time for treatment to occur.There being two main periods: 1) Baseline time zero, 2) End of the grace period.\nHere is one approach:\n\nPresent the baseline characteristics for everyone (prior to cloning) at time zero.\nPresent the characteristics for both treatment groups at the end of the grace period.\n\nThis would lead to a “Table 1” with three columns."
  },
  {
    "objectID": "03_inference.html#figure-of-differences-across-time",
    "href": "03_inference.html#figure-of-differences-across-time",
    "title": "Inference",
    "section": "",
    "text": "These differences can also be presented in a modified “Love” plot, with panels for each period of follow-up. It doesn’t really make sense to do one at baseline because the differences will be small/none. But we can pick a few follow-up periods to evaluate:\n\n# aSMD Figure ----\n  i_SMD_grps = c(12, 24, 60)\n\n  d_time = dta_c_panel %&gt;%\n    dplyr::filter(time %in% c(i_SMD_grps) & censor==0) %&gt;%\n    nest(.by = time) \n  \n  d_time$SMD_naive = map(d_time$data, ~f_cpt_dist(.))\n  d_time$SMD_ipw = map(d_time$data, ~f_cpt_dist_wt(.))\n  \n  gg_bal = select(d_time, time, SMD_naive, SMD_ipw) %&gt;%\n    unnest() %&gt;%\n    select(-Variable1) %&gt;%\n    rename(`Naive` = SMD,\n           `IPW` = SMD1) %&gt;%\n      pivot_longer(cols = c('Naive', 'IPW'), \n                   names_to = 'Model', values_to = 'SMD') %&gt;% \n      mutate(flag = if_else(abs(SMD)&gt;0.1, cbbPalette[1], cbbPalette[2])) %&gt;%\n      ggplot(., aes(y = Variable, x = SMD, group = Model)) +\n      geom_point(aes(shape=Model, color = flag), size=1.8) +\n      geom_vline(xintercept = 0, linetype=2) +\n      geom_vline(xintercept = 0.2, linetype=2, color=cbbPalette[3]) +\n      scale_color_identity() + \n      facet_grid(cols = vars(time)) +\n      theme_classic() +\n      labs(x = 'SMD', y = 'Variable',\n           caption = 'weighted differences in uncensored risk set')\n  \n  gg_bal\n\n\n\n\nFigure 1: Baseline and Time-varying Differences by Treatment Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can see the weighted between group differences are significantly reduced by the unstabilized IPW. However, caution in interpretation of the differences. If the treatment/exposure of interest has a causal relationship with the outcome (or some other censoring mechanism besides the artificial censoring the weighted account for), then you might expect there to be differences between groups. This is because the groups are conditional on remaining alive, uncensored at time-point 12, which may occur after treatment/exposure. In effect you are conditioning on a collider (outcome)."
  }
]