[
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "Version 0.0.1 02-19-2025",
    "section": "",
    "text": "Version 0.0.1 02-19-2025\nProject set-up\n\n\nVersion 0.1 05-12-2025\n\nSynthetic data set-up for observed effect by treatment due to confounding; no effect after appropriate adjustment\nEstimator file complete; additional edits for clarity and alternative approaches\nInference file TBD\nAdvanced file discusses some bootstrap and computation approaches\nAppendix TBD\n\n\n\nVersion 0.2 05-13-2025\n\nAdded Parallel computation step to Advanced Topics page\nSetup for the inference page (WIP)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#versions",
    "href": "about.html#versions",
    "title": "About",
    "section": "Versions",
    "text": "Versions\n\nVersion 1.0, established 04.24.2024"
  },
  {
    "objectID": "04_advanced.html",
    "href": "04_advanced.html",
    "title": "Additional Topics",
    "section": "",
    "text": "In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort, or comparison of those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals) but will provide a standardized mean difference.\nSome special considerations are needed when presenting target trial emulations.\n\n\nThe cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)"
  },
  {
    "objectID": "04_advanced.html#cloning-grace-period",
    "href": "04_advanced.html#cloning-grace-period",
    "title": "Additional Topics",
    "section": "",
    "text": "The cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then columns describing the intervention groups at a key follow-up time-point (the end of the grace period). It makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.)"
  },
  {
    "objectID": "04_advanced.html#evaluation-of-grace-period",
    "href": "04_advanced.html#evaluation-of-grace-period",
    "title": "Additional Topics",
    "section": "Evaluation of Grace Period",
    "text": "Evaluation of Grace Period"
  },
  {
    "objectID": "04_advanced.html#weighting-distributions",
    "href": "04_advanced.html#weighting-distributions",
    "title": "Additional Topics",
    "section": "Weighting distributions",
    "text": "Weighting distributions"
  },
  {
    "objectID": "04_advanced.html#covariate-balance-across-pre--post-weighting-during-follow-up",
    "href": "04_advanced.html#covariate-balance-across-pre--post-weighting-during-follow-up",
    "title": "Additional Topics",
    "section": "Covariate balance across pre-, post-weighting during follow-up",
    "text": "Covariate balance across pre-, post-weighting during follow-up"
  },
  {
    "objectID": "04_advanced.html#benchmarked-estimation-step",
    "href": "04_advanced.html#benchmarked-estimation-step",
    "title": "Additional Topics",
    "section": "Benchmarked Estimation Step",
    "text": "Benchmarked Estimation Step\n\nglm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n\n\nSpeeding up GLM ML procedure\nThere are two main things that can be done to speed up GLM, 1) initialize parameters based on prior estimation procedure. 2) Use efficient matrix functions and parallel computation.\n\nInitialization hack\nThis is a simple trick, either 1) run the GLM once and store est, or 2) run the GLM on a 10% sample.\n\n1d_fit = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n\n2d_fit_2 = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(), start = d_fit$coefficients)\n\n\n1\n\nGLM procedure with automatic initialization step.\n\n2\n\nGLM with start= option using coefficients from prior step.\n\n\n\n\n\n\nBLAS/LAPACK and Parallel computation\nThe parglm package provides much faster computations (but somewhat more unstable).\n\nlibrary(parglm)\n\nd_fit_3 = parglm(event_outc ~ poly(time, 2, raw=T)*assign, \n       binomial(), d_panel_outc, start = d_fit$coefficients,\n1       control = parglm.control(method = \"FAST\", nthreads = 8L))\n\n\n1\n\nparglm() function works mostly as glm(), the parglm.control() allows some additional options for parallel computing and QR decomposition.\n\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n           expr    min     lq   mean median     uq    max neval\n       base GLM 0.9495 0.9650 1.0120 1.0270 1.0540 1.0640     5\n GLM with inits 0.3323 0.3422 0.3422 0.3429 0.3443 0.3494     5\n         PARGLM 0.1878 0.1975 0.2185 0.2025 0.2038 0.3007     5\n\n\nEven in a small example, parglm() significantly outperforms base glm(). This will scale considerably with multiple cores and larger datasets as well."
  },
  {
    "objectID": "04_advanced.html#bootstrapping-procedure",
    "href": "04_advanced.html#bootstrapping-procedure",
    "title": "Additional Topics",
    "section": "Bootstrapping procedure",
    "text": "Bootstrapping procedure\nI consider bootstrapping to be the standard for this type of analysis, the statistical properties are not well-described, but some use influence-based statistics.\nThe typical bootstrap procedure resamples an entire dataset iteratively, but this can be very inefficient depending on how you set it up because it may involve holding the original dataset, and another new bootstrapped dataset in memory, also potentially a matrix in a regression optimization step. However some clever use of matrices and shortcuts can work around this.\n\n\n\n\n\n\nNote\n\n\n\nThe bootstrap procedure must sample at the person level to account for the cloning.\n\n\n\nInefficient Bootstrap\n\nboot_it_1 = function() {\n  \n1  d_ids = distinct(d_panel_outc, id)\n  \n  d_boot = slice_sample(d_ids, prop=1, replace=T)\n  \n2  d_panel_outc_2 = left_join(d_boot,\n                             d_panel_outc, by = join_by(id),\n3                             relationship = \"many-to-many\")\n  \n  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc_2, family=binomial())\n  \n  d_panel_outc_2$pr_ev = d_glm_pe$fitted.values \n  d_panel_outc_2[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] \n  d_res = d_panel_outc_2 %&gt;% \n    group_by(assign, time) %&gt;% \n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;% \n    ungroup %&gt;% \n    pivot_wider(., id_cols =c('time'),  \n                names_from = assign,  \n                names_prefix = 'pr_ev_', \n                values_from = pr_ev \n    ) %&gt;% \n    mutate(cid = pr_ev_1 - pr_ev_0,  \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a list of unique person IDs\n\n2\n\nSample from list with replacement\n\n3\n\nPerform left-join on resampled list back to dataset\n\n\n\n\n\n\nMore efficient bootstrap\nRather than sampling rows of the matrix with replacement, an alternative is to approximate the sampling with a frequency weight. If you randomly assign every observation a value drawn from a Poisson distribution with mean 1, and use this value as a frequency weight in estimators you will closely approximate the full bootstrap procedure as long as the overall sample size is &gt;100.(Hanley and MacGibbon 2006) This is very computationally efficient because you do not need to know the dataset size prior to assigning the frequency weight, and do not join or work with multiple large matrices.\n\nboot_it_2 = function() {\n  \n1  d_panel_outc[, freqwt:=rpois(n=1, lambda=1), by = factor(id)]\n\n  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(),\n                 weights = freqwt)\n  \n  d_panel_outc$pr_ev = d_glm_pe$fitted.values \n  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] \n  d_res = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;% \n    ungroup %&gt;% \n    pivot_wider(., id_cols =c('time'),  \n                names_from = assign,  \n                names_prefix = 'pr_ev_', \n                values_from = pr_ev \n    ) %&gt;% \n    mutate(cid = pr_ev_1 - pr_ev_0,  \n           cir = pr_ev_1 / pr_ev_0) \n  \n  return(d_res$cid[d_res$time==60])\n}\n\n\n1\n\nGenerate a frequency weight by group ID from a Poisson distribution with mean 1\n\n\n\n\n\n\nBenchmarking GLM methods\n\n\nUnit: seconds\n                 expr    min     lq   mean median     uq    max neval\n Resampling bootstrap 0.9154 0.9260 1.0270 1.0200 1.1360 1.1360     5\n    Poisson bootstrap 0.7763 0.7842 0.8348 0.8009 0.8876 0.9251     5\n\n\nThe Poisson bootstrap procedure is faster even in a small sample, but critically this performance will scale much better for large datasets.\n\n\nParallel computation\nThe next thing to work on is parallel computation steps. The efficiency depends on how the data is setup and what steps are running in parallel, it is not efficient to hold several very large datasets in memory at once so that a CPU worker can be assigned to each.\nIf you don’t believe this provides similar coverage estimates, here are the intervals from 50 bootstraps with both procedures:\nHere is a simple example for a Dell Laptop using the Furrr package with 10 workers:\n\nlibrary(furrr)\n1plan(multisession, workers = 10)\n\nresampling_res = future_map_dbl(1:100, \n                                function(x) boot_it_1(), \n2                                .options = furrr_options(seed=T))\n\npoisson_res = future_map_dbl(1:100, \n                                function(x) boot_it_2(), \n                                .options = furrr_options(seed=T))\n\n\n1\n\nAssign number of CPU workers to job\n\n2\n\nThe seed=T option is important because the bootstrap function uses RNG.\n\n\n\n\n\n\n\nComparison of Poisson and Resampling Bootstraps\n\n\nMethod\nmin\nLower CI\nmean\nq50th\nUpper CI\nmax\nSD\n\n\n\n\nPoisson\n0.021\n0.040\n0.067\n0.067\n0.094\n0.115\n0.015\n\n\nResampling\n0.038\n0.046\n0.073\n0.073\n0.101\n0.102\n0.014"
  },
  {
    "objectID": "02_est.html",
    "href": "02_est.html",
    "title": "Estimation",
    "section": "",
    "text": "I recommend the following stepwise procedure in estimation:\n\nPLR\n\nPooled Logistic Regression, a model which approximates the hazards by discretizing follow-up time.\n\nKM\n\nKaplan-Meier, non-parametric estimator which computes the instantaneous hazard at points of follow-up. Does not allow for time-varying confounding, but the non-parametric evaluation of time-trends can be useful for diagnostics and model specification of the PLR.\n\n\n\nEstimate unweighted/unadjusted cumulative incidences with KM method\nEstimate a PLR model without weights\n\n\n\nCompare KM vs PLR\n\n\n\nEstimate censoring weights\n\n\n\nEstimate PLR for treatment initiation\nA. Estimate treatment model\nB. Compare KM / PLR estimates\nC. Compute IPCW\nD. Examine IPCW weights for extreme values, non-sensical values\nE. Examine balance in covariates across time\nGenerate “table 1” with some weighted difference statistic (e.g. wSMD)\n\n\n\nEstimate weighted outcome models\n\n\n\nEstimate a weighted outcome model, no covariate adjustment\nEstimate a weighted outcome + covariates (usually main estimate)\n\n\n\nOnce satisfied with stability from steps 1-4, execute bootstraps\nFinalize report\n\n\n\nprovide KM, Cox unweighted, and PLR unweighted estimates in an appendix\nmain results reported should be the IPCW-weighted PLR analysis\n\n\n\n\n\n\n\nNote\n\n\n\nPooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of Causal Inference: What If, which explains why the odds approximates the hazard at a given time-point k, as long as the hazard is small (i.e. rare event) at time k."
  },
  {
    "objectID": "02_est.html#data",
    "href": "02_est.html#data",
    "title": "Estimation",
    "section": "Data",
    "text": "Data\nWe continue the below using the cloned dataset generated in Data.\n\nglimpse(d_cloned)\n\nRows: 10,000\nColumns: 17\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ assign     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t_clone    &lt;dbl&gt; 60, 37, 60, 14, 60, 31, 4, 1, 16, 60, 7, 1, 44, 9, 60, 60, …\n$ event_outc &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ time       &lt;dbl&gt; 60, 60, 60, 60, 60, 31, 54, 60, 29, 60, 22, 60, 44, 60, 60,…\n$ t_artcens  &lt;dbl&gt; Inf, 37, Inf, 14, Inf, Inf, 4, 1, 16, Inf, 7, 1, Inf, 9, In…\n$ t_treat    &lt;dbl&gt; Inf, 37, Inf, 14, Inf, Inf, 4, 1, 16, Inf, 7, 1, Inf, 9, In…\n$ treat      &lt;int&gt; 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ age        &lt;dbl&gt; 79.6, 91.4, 69.0, 76.9, 91.9, 67.4, 67.7, 70.5, 57.8, 74.5,…\n$ female     &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,…\n$ CC1        &lt;int&gt; 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,…\n$ CC2        &lt;int&gt; 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,…\n$ CC3        &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,…\n$ CC4        &lt;int&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,…\n$ CC5        &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,…\n$ X1         &lt;dbl&gt; -0.4224136, -0.4307282, -0.1797264, -0.5852888, -0.3290210,…\n$ X2         &lt;dbl&gt; 0.05287660, -0.28058259, -0.27317790, -0.14090988, 0.398000…"
  },
  {
    "objectID": "02_est.html#estimation",
    "href": "02_est.html#estimation",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\n\n  d_km_est = broom::tidy(\n1    survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n2    mutate(pr_ev = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n3    pivot_wider(id_cols = c(time),\n                names_from = assign,\n                values_from = c(pr_ev, pr_s,\n                                n.risk, n.event)) %&gt;%\n4    mutate(cir = pr_ev_1 / pr_ev_0,\n           cid = (pr_ev_1 - pr_ev_0))\n\n\n1\n\nNaive estimator, K-M estimator, assignment is by clone (not person). t_clone is the follow-up time for each clone, taking into account artificial censoring.\n\n2\n\nestimate from the model is the survival probability, so probability of event is 1 - estimate\n\n3\n\nData is in long form, so one colum per group with cumulative incidence/survival\n\n4\n\nEstimands, cir is ratio analogous to relative risk, and cid is analogous to risk difference"
  },
  {
    "objectID": "02_est.html#summarize-km-estimator",
    "href": "02_est.html#summarize-km-estimator",
    "title": "Estimation",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator"
  },
  {
    "objectID": "02_est.html#cox-proportional-hazards",
    "href": "02_est.html#cox-proportional-hazards",
    "title": "Estimation",
    "section": "Cox Proportional Hazards",
    "text": "Cox Proportional Hazards\nThe data was designed so treatment has no causal effect on outcome, but confounding is present Data. Adjustment for confounders identifies similar risks by treatment assignment.\n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\nassign\n1.04\n0.94, 1.14\n0.4\n\n\nfemale\n1.35\n1.22, 1.50\n&lt;0.001\n\n\nage\n0.96\n0.95, 0.96\n&lt;0.001\n\n\nCC1\n0.67\n0.61, 0.73\n&lt;0.001\n\n\nCC2\n0.89\n0.81, 0.98\n0.016\n\n\nCC3\n1.22\n1.11, 1.33\n&lt;0.001\n\n\nCC4\n0.60\n0.55, 0.66\n&lt;0.001\n\n\nCC5\n1.57\n1.43, 1.72\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio"
  },
  {
    "objectID": "02_est.html#build-a-longitudinal-panel",
    "href": "02_est.html#build-a-longitudinal-panel",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nThe PLR model is a person-time procedure, modeling the probability of the event where the observation is person at follow-up time k. The KM estimator is a person-level dataset, so the dataset much first be expanded to person-time units. If there are many persons, or many observations (timepoints) per person the dataset can become quite large.\n\n\n\n\n\n\nNote\n\n\n\nI personally use the data.table package to help with computation. It is blazingly fast, so the code reflects that approach which may not be needed for your project.\n\n\n\n1    setDT(d_cloned)\n2    d_panel = d_cloned[rep(seq(.N), t_clone)]\n3    d_panel[, exit := (seq_len(.N)), by = list(id, assign)]\n    d_panel[, enter := exit-1]\n4    d_panel[, time := seq_len(.N), by = list(id, assign)]\n5    d_panel[, event_outc := if_else( t_clone &lt;= time, event_outc, 0L), by = list(id, assign)]\n    d_panel_outc = select(d_panel, id, time, event_outc, t_treat, assign, enter, exit, female, X1, X2, \n                          CC1:CC5, age) \n\n\n1\n\nConvert data to a DT object for faster computations\n\n2\n\nGenerate rows along the sequence of .N (number of time-points) up to the last follow-up point\n\n3\n\nFor each row, compute the starting (enter) and ending time-point (exit). This isn’t really necessary for our toy example, but could be important depending on how you are setting up the longitudinal panel and planning to model the time-trend.\n\n4\n\nThis time is the key variable, a count from 1 to the last observed follow-up point by person, clone\n\n5\n\nOutcome is = 1 in row where event occurred, so in the data a person-clone should have values of zero for event_outc up until the last observed time-point where event_outc=1 IF the event occurred at that time.\n\n\n\n\n\n\nRows: 281,762\nColumns: 16\n$ id         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ time       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ event_outc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf,…\n$ assign     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ enter      &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ exit       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ X1         &lt;dbl&gt; -0.4224136, -0.4224136, -0.4224136, -0.4224136, -0.4224136,…\n$ X2         &lt;dbl&gt; 0.0528766, 0.0528766, 0.0528766, 0.0528766, 0.0528766, 0.05…\n$ CC1        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CC2        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ CC3        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CC4        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CC5        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ age        &lt;dbl&gt; 79.6, 79.6, 79.6, 79.6, 79.6, 79.6, 79.6, 79.6, 79.6, 79.6,…\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how the dataset has expanded from N=20000 to N=554k! This method is very greedy for RAM…"
  },
  {
    "objectID": "02_est.html#estimation-1",
    "href": "02_est.html#estimation-1",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\nThe PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time^2). Choose either to estimate the outcome in a model with both clones combined in one dataset OR estimate cumulative incidences separately (two models with data limited to assign==1 & assign==0 respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. time + time^2 + treat + treat*time + treat*time^2, shorthand below is poly(time, 2, raw=T)*assign.\n\n  # defined above\n1  d_glm = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial())\n  \n2  d_panel_outc$pr_ev = d_glm$fitted.values\n3  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)]\n  \n4  d_plr_naive_est = d_panel_outc %&gt;%\n    group_by(assign, time) %&gt;%\n    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    pivot_wider(., id_cols =c('time'),\n                names_from = assign,\n                names_prefix = 'pr_ev_',\n                values_from = pr_ev\n    ) %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nPLR model, with time*treat interaction. Binomial family for logistic regression.\n\n2\n\nIf event_outc = 1 when event occurs, the fitted values are the probability of the outcome at time k.\n\n3\n\nThe event-free survival probability is 1 - cumulative probability (incidence) of an event. To compute, you take the cumulative product of the fitted probabilities pr_ev, making sure to only compute within group and clone.\n\n4\n\nAfter you have cumulative event-free survival probability, then you can summarize by group/time as described above for the KM estimator.\n\n\n\n\n\n\n\n\n\nUnadjusted, unweighted cumulative incidences by treatment group (PLR)\n\n\n\n\nAt first glance, the plot looks reasonable. Direct comparison of the PLR and KM estimators is helpful for diagnosing problems in code or modeling steps.\n\n\n\n\n\nComparison of Naive PLR versus KM estimator\n\n\n\n\nSo the PLR model using a simple polynomial can reasonably approximate the KM estimate. This should work because the underlying synthetic data was generated with an exponential distribution for time, but real-world data will not play so nicely. There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider the following regarding the parametric time trend:\n\nIt may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.\nThe non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.\nThese initial steps will not guarantee a good fit after weighting is applied, it is only a first-look for diagnostic purposes."
  },
  {
    "objectID": "02_est.html#comparison-of-covariate-adjusted-plr-with-cox-model",
    "href": "02_est.html#comparison-of-covariate-adjusted-plr-with-cox-model",
    "title": "Estimation",
    "section": "Comparison of Covariate-adjusted PLR with Cox model",
    "text": "Comparison of Covariate-adjusted PLR with Cox model\nAnother approach is to compare a pooled logistic regression analysis to a time-invariant Cox model. If time-varying confounding not an issue, this should give a similar result. However, censoring weights are still necessary in final analysis even if no confounding.(Cain et al. 2010)\n\n# defined above\n  d_glm = glm(event_outc ~ poly(time, 2, raw=T)*assign + female + age + CC1 + CC2 + CC3 + CC4 + CC5, \n1              data=d_panel_outc, family=binomial())\n\n2  d_panel_outc$p.event0 &lt;- predict(d_glm, mutate(d_panel_outc, assign=0), type=\"response\")\n  d_panel_outc$p.event1 &lt;- predict(d_glm, mutate(d_panel_outc, assign=1), type=\"response\")\n  \n3  d_panel_outc[, `:=`(pr_surv_1 = cumprod(1 - p.event1)), by=list(id, assign)]\n  d_panel_outc[, `:=`(pr_surv_0 = cumprod(1 - p.event0)), by=list(id, assign)]\n  \n4  d_plr_adj_est = d_panel_outc %&gt;%\n    group_by(time) %&gt;%\n    summarize(pr_ev_1 = mean(1-pr_surv_1),\n              pr_ev_0 = mean(1-pr_surv_0), \n              .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nPLR model with some covariates for adjustment. (model fit for each treatment group)\n\n2\n\nPredict outcome from each PLR.\n\n3\n\nEstimate cumulative survival using cumulative product, within person-clone\n\n4\n\nSummarize mean survival rates by time-period\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the outcome regression without weights is just for comparison and understanding, the final analysis must include probability weights for artificial censoring even if no confounding.\n\n\n\n  d_cox = coxph(Surv(enter, exit, event_outc) ~ assign + female + age + CC1 + CC2 + CC3 + CC4 + CC5, data = d_panel_outc)\n\n  d_cox_est = surv_adjustedcurves(fit = d_cox, variable = \"assign\") %&gt;%\n      group_by(variable, time) %&gt;%\n        dplyr::filter(row_number()==1) %&gt;%\n      ungroup %&gt;%\n      mutate(pr_ev = 1 - surv) %&gt;%\n      pivot_wider(., id_cols =c('time'),\n                  names_from = variable,\n                  names_prefix = 'pr_ev_',\n                  values_from = pr_ev\n      ) %&gt;%\n      mutate(cid = pr_ev_1 - pr_ev_0,\n             cir = pr_ev_1 / pr_ev_0)\n\nFor comparison, A time-dependent Cox model is fit.\n\nCox model, with time1, time2 parameters which represent the start and stop time of each interval\nEstimation of adjusted survival curves by treatment group, with subpopulations balanced using conditional method.(“A Comparison of Different Methods to Adjust Survival Curves for Confounders - Denz - 2023 - Statistics in Medicine - Wiley Online Library,” n.d.)\nSummarizing survival probabilities by time\n\n\n\n\n\n\n\n\n\n\nSo the PLR also is also very similar to Cox model estimates and both appropriately find no difference in risk by treatment assignment (by synthetic data design)."
  },
  {
    "objectID": "02_est.html#build-a-longitudinal-panel-1",
    "href": "02_est.html#build-a-longitudinal-panel-1",
    "title": "Estimation",
    "section": "Build a longitudinal panel",
    "text": "Build a longitudinal panel\nThe person- or clone-level data must be expanded to a longitudinal panel by time. Two datasets are constructed, one for estimation of censoring probabilities and one for estimation of outcome probabilities.\n\nCensoring dataset\nIn our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is specific to a project’s definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we generate a dataset that is at the person-level with time to treatment as the outcome (no cloning):\n\nd_treat = d_cloned %&gt;%\n1    dplyr::filter(assign==0) %&gt;%\n    mutate(start=1,\n2           end = t_treat) %&gt;%\n3    group_by(id) %&gt;%\n    mutate(\n      time = t_clone,\n      outcome = if_else(time==t_treat, 1, 0)\n    ) %&gt;%\n    ungroup\n\n\n1\n\nThis may be a little confusing, but I am taking clones where assign=0 from the d_cloned dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.\n\n2\n\nWe are estimating time to treatment, not outcome\n\n3\n\nArtificial censoring time is same as treatment time unless dead first, and outcome = 1 if treated.\n\n\n\n\nThen we expand the dataset as described above:\n\n  setDT(d_treat)\n  d_panel = d_treat[rep(seq(.N), time)]\n  d_panel[, exit := (seq_len(.N)), by = list(id)]\n  d_panel[, enter := exit-1]\n  d_panel[, time := seq_len(.N), by = list(id)]\n  d_panel[, event_treat := if_else(t_treat &lt;= time, 1L, 0L), by = list(id)]\n1  d_panel_treat = select(d_panel, id, time, event_treat, t_treat, enter, exit, end, age, female, CC1:CC5, X1, X2)\n\n\n1\n\nNote we keep covariates for estimation of censoring weights."
  },
  {
    "objectID": "02_est.html#estimation-of-weights",
    "href": "02_est.html#estimation-of-weights",
    "title": "Estimation",
    "section": "Estimation of weights",
    "text": "Estimation of weights\nThe probability weight, AKA inverse probability censoring weight (IPCW) or inverse probability of no loss to follow-up is estimated in this way:\n\\[\nW^C_i = \\prod^{T}_{t_0} \\frac{P(C_t = 0| \\overline{C}_{t-1} = 0)}{P(C_t=0 | X=x, \\overline{L}_{t-1}=l_{i, t-1}, \\overline{C}_{t-1}=0)}\n\\]\nC in this case means censoring, but censoring occurs according to treatment strategy, so it really is the probability of adhering to the treatment you were assigned to. Stabilized weights are tricky, here we are using the marginal stabilized weight which is the probability of no censoring at each timepoint. The denominator is the conditional probability accounting for fixed, i.e. baseline, (X) and time-varying (L) covariates.\nWe fit a model with the outcome of censoring, including the key variables. Then we add fitted probabilities back to the outcome dataset, and calculate cumulative probabilities and weights.\n\n  # Numerator (margin probability)\n1  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T), data=d_panel_treat, family=binomial())\n  d_panel_outc$pr_censnum = predict(d_glm_wt, newdata = d_panel_outc, type='response')\n\n  # Denominator \n  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T) + female + age + \n2                   CC1 + CC2 + CC3 + CC4 + CC5 + X1, data=d_panel_treat, family=binomial())\n  \n  d_panel_outc$pr_censdenom = predict(d_glm_wt, newdata = d_panel_outc, type='response')\n\n\n1\n\nMarginal probability of no censoring\n\n2\n\nConditional probability (covariate-adjusted) for no censoring\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI think there is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of “censoring” where that may mean treatment initiation or some other thing.(Gaber et al. 2024)"
  },
  {
    "objectID": "02_est.html#calculation-of-weights",
    "href": "02_est.html#calculation-of-weights",
    "title": "Estimation",
    "section": "Calculation of weights",
    "text": "Calculation of weights\nThis step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to the Weighting section for further discussion.\n\n  setDT(d_panel_outc)\n  \n  d_panel_outc[, ipw := fcase(\n1    assign==0, 1 / (1-pr_censdenom),\n    assign==0 & t_treat==time, 0,\n2    assign==1 & time &lt; 12, 1,\n3    assign==1 & time == 12  & t_treat  &lt; 12, 1,\n4    assign==1 & time == 12  & t_treat  ==12, 1 / (pr_censdenom),\n5    assign==1 & time == 12  & t_treat  &gt;12 & event_outc==0, 0,\n6    assign==1 & time == 12  & t_treat  &gt;12 & event_outc==1, 1,\n7    assign==1 & time &gt; 12, 1\n  )]\n  \n  d_panel_outc[, marg_ipw := fcase(\n8    assign==0, (1-pr_censnum) / (1-pr_censdenom),\n9    assign==1 & time == 12 & t_treat   ==12, pr_censnum / (pr_censdenom),\n    default = ipw\n  )]\n  \n  d_panel_outc[, `:=`(ipw = cumprod(ipw),\n                      marg_ipw = cumprod(marg_ipw)),\n               by=list(id, assign)]\n\n\n1\n\n(assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored\n\n2\n\n(assign=1) Clones cannot artificially censor prior to grace period\n\n3\n\n(assign=1) If treatment started prior to grace window ending, the clone cannot censor\n\n4\n\n(assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window).\n\n5\n\n(assign=1) If a clone is not treated they censor at the end of the window\n\n6\n\n(assign=1) In periods greater than the end of the grace window, no artificial censoring occurs (they either are treated or already censored at this point).\n\n7\n\nIf died in last interval, do not set weight to zero.\n\n8\n\nMarginal IPW is same, except numerator probability replaces 1\n\n9\n\nAfter setting these conditions, we compute the cumulative product of the weights.\n\n\n\n\n\nOverall IPW Distribution\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.162   1.838   1.604  88.292 \n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3420  0.9386  1.0000  0.9921  1.0000  4.2641 \n\n\nThe unstabilized weights floor at 1, and we see high weights assigned to some. The marginalized weights have a mean of 1 (expected).\n\n\nIPW Distribution at end of grace period for treatment clones\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   2.064   1.000  88.292 \n\n\nThe weights at the end of the grace period are key.\nIt is good to see the marginal stabilized weights have a mean 1. You can also plot this across time:\n\n  d_panel_outc %&gt;%\n    dplyr::filter(marg_ipw!=0) %&gt;%\n ggplot(., aes(x = time, y = marg_ipw)) +\n  geom_violin(aes(group = cut_width(time, 12)), \n               scale = \"width\", fill = \"blue\", alpha = 0.7) +\n  geom_hline(aes(yintercept=1), linetype=3) +\n  scale_x_continuous(breaks = seq(0,60, 12)) +\n  labs(x = \"Follow-up\", y = \"IPW\") +\n  theme_bw()\n\n\n\n\nDistribution of Censoring Weights Across Time\n\n\n\n\nAnother way to examine the weights is to look at the weighted counts of individuals at risk, and number of events pre- and post-weighting:\n\nd_panel_outc %&gt;%\n  group_by(time, assign) %&gt;%\n  summarize(n = sum(ipw!=0), \n            n_wt = sum(ipw),\n            ev = sum(event_outc[ipw!=0]),\n            ev_wt = sum(event_outc*ipw),\n            .groups = 'drop') %&gt;%\n  mutate(unwt_evn = paste0(n, ' (', ev, ')'),\n         wt_evn = paste0(round(n_wt, 0), ' (', round(ev_wt, 0), ')')) %&gt;%\n  pivot_wider(id_cols = time, names_from = assign, values_from = c(unwt_evn, wt_evn)) %&gt;%\n  dplyr::filter(time %in% c(1, 12, 30, 60)) %&gt;%\n  rename(`Time` = time,\n         `Unweighted, assign=0` = unwt_evn_0,\n         `Unweighted, assign=1` = unwt_evn_1,\n         `Weighted, assign=0` = wt_evn_0,\n         `Weighted, assign=1` = wt_evn_1,\n         ) %&gt;%\n  kable(align='c', digits =0) %&gt;%\n  kable_styling()\n\n\nUnweighted and Weighted Counts, N at risk (cumulative events)\n\n\nTime\nUnweighted, assign=0\nUnweighted, assign=1\nWeighted, assign=0\nWeighted, assign=1\n\n\n\n\n1\n4856 (2)\n5000 (2)\n5122 (2)\n5000 (2)\n\n\n12\n3125 (8)\n1842 (18)\n4850 (15)\n5109 (18)\n\n\n30\n2286 (16)\n1591 (15)\n4206 (32)\n4485 (15)\n\n\n60\n1630 (14)\n1083 (16)\n2973 (28)\n3085 (75)\n\n\n\n\n\n\n\nLike most applications of probability weights, the distribution of weights should be examined and compared to the unweighted population (size and event counts) to identify any problems.\nNow with the estimated weights, it is simple to generate weighted cumulative incidences:\n\n  d_glm_pe_1 = glm(event_outc==0 ~ poly(time, 2, raw=T) + female + age + \n                   CC1 + CC2 + CC3 + CC4 + CC5 + X1, \n                   data=d_panel_outc[assign==1, ], \n1                   family=binomial(), weights = ipw)\n\n  d_glm_pe_0 = glm(event_outc==0 ~ poly(time, 2, raw=T) + female + age + \n                   CC1 + CC2 + CC3 + CC4 + CC5 + X1, \n                   data=d_panel_outc[assign==0, ], \n                     family=binomial(), weights = ipw)\n  \n2  d_panel_outc$pr_1 = predict(d_glm_pe_1, newdata = d_panel_outc, type='response')\n  d_panel_outc$pr_0 = predict(d_glm_pe_0, newdata = d_panel_outc, type='response')\n\n\n1\n\nEstimate for each assignment group separately. Note the weights = ipw argument, everything else is same as PLR model above. R glm() will generate a warning message because the weighted counts are “non-integer”, but this is expected and not a problem.\n\n2\n\nFitted probabilities from each model\n\n\n\n\n\n1  d_panel_outc[, `:=`(pr_cum_1 = cumprod(pr_1)), by=list(id, assign)]\n  d_panel_outc[, `:=`(pr_cum_0 = cumprod(pr_0)), by=list(id, assign)]\n  \n2  d_plrwt_est = d_panel_outc %&gt;%\n    group_by(time) %&gt;%\n    summarize(pr_ev_1 = mean(1-pr_cum_1), \n              pr_ev_0 = mean(1-pr_cum_0),\n              .groups = 'drop') %&gt;%\n    ungroup %&gt;%\n    mutate(cid = pr_ev_1 - pr_ev_0,\n           cir = pr_ev_1 / pr_ev_0)\n\n\n1\n\nCumulative product\n\n2\n\nSummarize across group, time as before.\n\n\n\n\n\n\n\n\n\nWeighted PLR Analysis"
  },
  {
    "objectID": "01_syndata.v2.html",
    "href": "01_syndata.v2.html",
    "title": "Simulate data for CCW project",
    "section": "",
    "text": "We assume a generic treatment, A, has no causal effect on generic outcome, Y. However, confounding is present through other random variables. This includes age, two continuous variables, X1, X2. Gender, defined through a binary variable M for male. Finally, a vector of chronic conditions, we collectively refer to as CC. X1 is defined as a cause of A but not Y, and X2 is a cause of Y but not A. Below we assign effects of these variables that will lead to identification of a causal effect between A and Y, if no controlling for the confounding is done. No colliders A -&gt; L &lt;- Y are present.\n\n\n\n\n\nFigure 1. Causal DAG of treatment and outcome\n\n\n\n\nIf a naive analysis (no adjustment) was performed the estimate will be biased, because treamtent and outcome are d-connected.\n\n\n\n\n\nFigure 2. D-connected treatment and outcome\n\n\n\n\n\n\n\n\n\nFigure 3. D-separated treatment and outcome after adjustment\n\n\n\n\nAdjustment for CC, Age and M should lead to identification of a NULL effect of treatment on outcome. Adjustment for X1, and X2 are not necessary, but may be done for variance precision."
  },
  {
    "objectID": "01_syndata.v2.html#sec-treat",
    "href": "01_syndata.v2.html#sec-treat",
    "title": "Simulate data for CCW project",
    "section": "Simulate initiation of treatment",
    "text": "Simulate initiation of treatment\n\n1x_age = 0.96\nx_fem = 1.3\nx_1 = c(0.8)\nx_cc = c(0.9, 0.7, 0.8, 0.7, 0.9)\n\n\nlp_treat &lt;- 1 / (1 + exp(- (3.5 + \n                              log(x_age) * age + \n                              log(x_fem) * female + \n                              log(x_1) * xmat[, 1] + \n                              log(x_cc[1])*cc_xmat[, 1] + \n                              log(x_cc[2])*cc_xmat[, 2] + \n                              log(x_cc[3])*cc_xmat[, 3] + \n                              log(x_cc[4])*cc_xmat[, 4] + \n                              log(x_cc[5])*cc_xmat[, 5])\n                         )\n2                 )\n\ntreat &lt;- sapply(lp_treat, function(x) rbinom(1, 1, x))\n\nmean(treat)\n\n\n1\n\nAssign effect sizes for each covariate on treatment\n\n2\n\nCompute a probability of treatment for each person, and draw random binary treatment based on individual probability. Intercept tailored so the mean ~ 50%.\n\n\n\n\n[1] 0.5024\n\n\n\n\n\n\n\n\nNote\n\n\n\nAge and female are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "01_syndata.v2.html#sec-etimes",
    "href": "01_syndata.v2.html#sec-etimes",
    "title": "Simulate data for CCW project",
    "section": "Simulate outcome",
    "text": "Simulate outcome\n\n1y_treat = 1.0\ny_gender = 1.2\ny_age = 0.98\ny_x2 = 0.8\ny_cc = c(0.8, 0.9, 1.1, 0.8, 1.2)\n\n# log-hazard\n  lp_outc &lt;- log(y_treat) * treat +\n    log(y_gender) * female +\n    log(y_age) * age + \n    log(y_x2) * xmat[, 2] +\n    log(y_cc[1])*cc_xmat[, 1] + \n    log(y_cc[2])*cc_xmat[, 2] + \n    log(y_cc[3])*cc_xmat[, 3] + \n    log(y_cc[4])*cc_xmat[, 4] + \n2    log(y_cc[5])*cc_xmat[, 5]\n  \n3    baseline_survival &lt;- rweibull(n, shape = 2, scale = 20)\n    \n# Adjust survival times based on linear predictor\n4    t_outc &lt;- round(baseline_survival * exp(-lp_outc))\n    \n    summary(t_outc)\n\n\n1\n\nSet effects\n\n2\n\nCompute the linear predictor\n\n3\n\nSimulation will use a random exponential distribution with rate of 0.05 (baseline hazard).\n\n4\n\nSurvival times\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   44.00   74.00   87.32  116.00  680.00"
  },
  {
    "objectID": "03_inference.html",
    "href": "03_inference.html",
    "title": "Reporting Results",
    "section": "",
    "text": "If the design of the study includes a baseline period, “time zero” and two treatment strategies, with a grace period or interval where one strategy allows some time for treatment to occur then the presentation of group characteristics is a little different from the typical trial or cohort study.\nThere being two main periods: 1) Baseline time zero, 2) End of the grace period.\nSo here is one approach:\n\nPresent the baseline characteristics for everyone (prior to cloning) at time zero.\nPresent the characteristics for both treatment groups at the end of the grace period.\n\nThis would lead to a “Table 1” with three columns.\n\n\n\n\n\nMost analyses would provide the naive differences between groups, and then present some adjusted difference. Most commonly the standardized mean differences (aSMD) or a P-value from a univariate test (not recommended).\nSo you could add one column for the SMD to the “Table 1”, but there are also other approaches such as reporting Mahalanobis distance."
  },
  {
    "objectID": "03_inference.html#presentation-of-weighted-differences",
    "href": "03_inference.html#presentation-of-weighted-differences",
    "title": "Reporting Results",
    "section": "",
    "text": "Most analyses would provide the naive differences between groups, and then present some adjusted difference. Most commonly the standardized mean differences (aSMD) or a P-value from a univariate test (not recommended).\nSo you could add one column for the SMD to the “Table 1”, but there are also other approaches such as reporting Mahalanobis distance."
  },
  {
    "objectID": "03_inference.html#bootstrapping",
    "href": "03_inference.html#bootstrapping",
    "title": "Reporting Results",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n\n\n\n\n\nNote\n\n\n\nA Poisson bootstrap procedure is used here, see:Additional Topics for some notes on this."
  },
  {
    "objectID": "03_inference.html#inference-statistics",
    "href": "03_inference.html#inference-statistics",
    "title": "Reporting Results",
    "section": "Inference statistics",
    "text": "Inference statistics\nInferential statistics\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "05_appendix.html",
    "href": "05_appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Grace windows\n\n\nWeight assignments in CCW design"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "",
    "text": "Data; Demonstrates how to simulate a dataset for use in CCW along with the cloning process. A key process is how to generate a “cloned” dataset, refer here\nEstimation; Main section which walks through each estimation step.\nInference; Examples on how to obtain confidence intervals.\n\nAdditional Topics\nAppendix; some notes on causal inference, theory that may be helpful in adapting this tutorial to a specific project.\n\nFor production notes and future efforts see: About."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe tutorial represents my gathered and organized notes from research projects and didactic training. Collaborators and mentors include: Issa Dahabreh, Kaley Hayes, Daniel Harris, Donald Miller and Andrew Zullo."
  }
]