[
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "Version 0.0.1 02-19-2025",
    "section": "",
    "text": "Version 0.0.1 02-19-2025\nProject set-up"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#versions",
    "href": "about.html#versions",
    "title": "About",
    "section": "Versions",
    "text": "Versions\n\nVersion 1.0, established 04.24.2024"
  },
  {
    "objectID": "04_bsnotes.html",
    "href": "04_bsnotes.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Inferential statistics\nSeveral steps in the analysis makes the assumptions of conventional standard errors invalid. The use of cloning or repeated use of persons across sequential target trial dates complicates the statistical properties of the estimand (due to correlated data), additionally the uncertainty in estimation of probability weights should be considered. Currently, most researchers are using bootstrapping to obtain confidence limits via percentile method. It is critical that 1) the bootstrapping step occur prior to sequential repeated sampling, cloning or estimation of weights, 2) the bootstrap sampling with replacement must be at the cluster-level (person). Also consider, bootstrapping may fail to produce valid estimates in cases of matching procedures, or restricted regression procedures (i.e. LASSO or other shrinkage estimators).(Campanovo 2015, Abadie and Imbens 2008) Other bootstrapping failures may arise due to limited sample size or values on the boundary of a parameter space. Note: Poisson Bootstrap procedure Especially for large datasets, the bootstrap procedure can be computationally intensive (requiring holding multiple large matrices in memory simultaneously). Rather than sampling rows of the matrix with replacement, an alternative is to approximate the sampling with a frequency weight. If you randomly assign every observation a value drawn from a Poisson distribution with mean 1, and use this value as a frequency weight in estimators you will closely approximate the full bootstrap procedure as long as the overall sample size is &gt;100. (Hanley and Macgibbon 2006) This is very computationally efficient because you do not need to know the dataset size prior to assigning the frequency weight, and do not need to sample large matrices.\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "02_est.html",
    "href": "02_est.html",
    "title": "Estimation",
    "section": "",
    "text": "I recommend the following stepwise procedure in estimation: (PLR = pooled logistic regression; KM = Kaplan-Meier estimator).\n\nEstimate unweighted/unadjusted cumulative incidences with KM method\nEstimate a pooled logistic regression model without weights\ni) Compare KM vs PLR, estimates should be similar\nEstimate censoring weights i) Estimate KM model for treatment initiation ii) Estimate PLR for treatment initation iii) compare estimates, should be similar iv) estimate full weighted model, compute IPCW v) examine IPCW weights for extreme values, non-sensical values vi) examine balance in covariates across time vii) generate “table 1” with some weighted difference statistic (e.g. wSMD)\nEstimate weighted outcome models i) Estimate a weighted outcome model, no covariate adjustment ii) Estimate a weighted outcome + covariates (main estimate)\nOnce satisified with steps 1-4, execute bootstraps\nIn report, provide KM, unweighted, PLR estimates in an appendix; main results reported should be the IPCW-weighted PLR analysis with or without covariate adjustment in the outcome model (project specific decision)."
  },
  {
    "objectID": "02_est.html#estimation",
    "href": "02_est.html#estimation",
    "title": "Estimation",
    "section": "Estimation",
    "text": "Estimation\n\n# Naive estimator ----\n\n# Run a simple Kaplan-Meier, No adjustments made ----\n  ## Kaplan-Meier function ----\n  # one row per clone, not panel data, needs time, event, treat variables\n\n# Point estimate ----\n\n  d_surv_pe = broom::tidy(\n    survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned)) %&gt;%\n    mutate(assign = str_extract(strata,  '(?&lt;=assign=)\\\\d')\n    ) %&gt;%\n    arrange(time) %&gt;%\n    select(-strata) %&gt;%\n    mutate(pr_e = 1-estimate) %&gt;%\n    rename(pr_s = estimate) %&gt;%\n    pivot_wider(id_cols = c(time), \n                names_from = assign,\n                values_from = c(pr_e, pr_s,\n                                n.risk, n.event)) %&gt;%\n    mutate(cir = pr_e_1 / pr_e_0,\n           cid = (pr_e_1 - pr_e_0))"
  },
  {
    "objectID": "02_est.html#summarize-km-estimator",
    "href": "02_est.html#summarize-km-estimator",
    "title": "Estimation",
    "section": "Summarize KM estimator",
    "text": "Summarize KM estimator\n\nggsurvplot(survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned),\n                            data=d_cloned,\n                            fun = 'event',\n                            xlim = c(0, 60),\n                            xlab = 'Follow-up',\n                            break.time.by=6,\n                            palette = c('red', 'blue'),\n                            linetype = 'strata',\n                            censor=F,\n                            cumevents=T,\n                            conf.int=F, pval=F,\n                            risk.table=T, risk.table.col = 'strata',\n                            risk.table.height=0.25, \n                            ggtheme = theme_classic())"
  },
  {
    "objectID": "01_syndata.v2.html",
    "href": "01_syndata.v2.html",
    "title": "Simulate data for CCW project",
    "section": "",
    "text": "Simulation Parameters\n\n1    n = 10000\n2    age = round(rnorm(n, mean = 75, sd = 10), 1)\n3    female = sample(c(1, 0), size = n, replace = TRUE, prob = c(0.66, 0.34))\n4    treat &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.2, 0.8))\n5    dx_1 &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.05, 0.95))\n6    dx_2 &lt;- sample(c(1, 0), size = n, replace = TRUE, prob = c(0.4, 0.6))\n7    ef_treat = 0.8\n    ef_gender = 0.9\n    ef_age = 0.98\n    ef_dx_1 = 1.2\n    ef_dx_2 = 0.8\n\n\n1\n\nUse a starting sample size of a 1000 (i.e. persons) Generate covariates:\n\n2\n\nGenerate age (normal distribution with mean=75, std=10)\n\n3\n\nFemale, 66%/34%\n\n4\n\nTreatment, 20%/80%\n\n5\n\nA rare chronic condition dx_1 5%/95%\n\n6\n\nA prevalent chronic condition dx_2 40%/60%\n\n7\n\nAssign relative effect sizes for each\n\n\n\n\n\nef_treat = 0.8\nef_gender = 0.9\nef_age = 0.98 (per unit of age)\nef_dx_1 = 1.2\nef_dx_2 = 0.8\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…\n\n\n\n\nLinear Predictor\n\n# log-hazard\n    lp_outc &lt;- log(ef_treat) * treat +\n      log(ef_gender) * female +\n1      log(ef_age) * age + log(ef_dx_1)*dx_1 + log(ef_dx_2)*dx_2\n  \n  # Simulate baseline survival times (exponential distribution)\n2    baseline_hazard &lt;- 0.05\n\n\n1\n\nCompute the linear predictor,\n\n2\n\nSimulation will use a random exponential distribution with rate of 0.05 (baseline hazard). This was selected based on trial and error.\n\n\n\n\n\n\nSimulate Event Time\n\n1    baseline_survival &lt;- rexp(n, rate = baseline_hazard)\n    \n  # Adjust survival times based on linear predictor\n2    t_outc &lt;- round(baseline_survival * exp(-lp_outc))\n\n\n1\n\nSimulate survival times using exponential distribution\n\n2\n\nAdjust the times with linear predictor and round to nearest whole number.\n\n\n\n\n\n\nSimulate Treatment Start\n\n1    lp_treat = log(0.7) * female + log(1.02) * age\n2    baseline_hazard = 0.1\n3    t_treat = if_else(treat==1, round(rexp(n, rate = baseline_hazard)* exp(-lp_treat)), Inf)\n\n\n1\n\nSimulate treatment start times based on female and age. (for weighting model); assumed dx_1 and dx_2 unrelated to treatment.\n\n2\n\nUsed a different rate, so treatment happens early relative to outcome.\n\n3\n\nGenerate treatment times and adjust for linear predictor.\n\n\n\n\n\n\nFinalize dataset\n\n  # Create dataset\n    data &lt;- data.frame(\n      age = age,\n      female = female,\n      dx_1 = dx_1,\n      dx_2 = dx_2,\n      treat = treat, \n      t_treat = t_treat, \n      t_outc = t_outc \n    ) %&gt;%\n      mutate(id = row_number(),\n1             time = pmin(60, t_outc),\n             event_outc = if_else(time==t_outc, 1L, 0L))\n\n\n1\n\nSet administrative end of follow-up at 60. No other censoring mechanism, e.g. lost to follow-up.\n\n\n\n\n\n\nSummary of survival dataset\n\nglimpse(data)\n\nRows: 10,000\nColumns: 10\n$ age        &lt;dbl&gt; 88.7, 69.4, 78.6, 81.3, 79.0, 73.9, 90.1, 74.1, 95.2, 74.4,…\n$ female     &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ dx_1       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ dx_2       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,…\n$ treat      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,…\n$ t_treat    &lt;dbl&gt; Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, 4, Inf, 2, Inf, 0, …\n$ t_outc     &lt;dbl&gt; 61, 85, 75, 28, 47, 49, 301, 57, 111, 75, 111, 206, 72, 30,…\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ time       &lt;dbl&gt; 60, 60, 60, 28, 47, 49, 60, 57, 60, 60, 60, 60, 60, 30, 60,…\n$ event_outc &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,…\n\n\n\nsummary(survfit(Surv(time, event_outc) ~ treat, data = data), times = c(0, 12, 30, 60))\n\nCall: survfit(formula = Surv(time, event_outc) ~ treat, data = data)\n\n                treat=0 \n time n.risk n.event survival  std.err lower 95% CI upper 95% CI\n    0   7924      33    0.996 0.000723        0.994        0.997\n   12   7125     844    0.889 0.003524        0.882        0.896\n   30   5956    1145    0.745 0.004897        0.735        0.754\n   60   4465    1475    0.559 0.005578        0.548        0.570\n\n                treat=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0   2076       6    0.997 0.00118        0.995        0.999\n   12   1882     200    0.901 0.00656        0.888        0.914\n   30   1635     244    0.783 0.00904        0.766        0.801\n   60   1286     347    0.616 0.01067        0.596        0.637\n\n\n\n\n\n\n\n\nNote\n\n\n\nTreat=1 means they EVER received treatment, so additional work needed to describe a treatment window etc.\n\n\n\n\nCloning procedure\nThe cloning procedure is very project specific, tied to how the treatment strategy is defined so it is hard to give general recommendations here. For this tutorial, we describe an arbitrary window in which treatment is expected to initiate and outline strategies around this:\nTreatment Strategies:  1) Do not ever receive treatment 2) Initiate treatment within 12 time periods (days, weeks etc.) and if not then treatment will initiate on week 12.\n\n\n\n\n\n\nNote\n\n\n\nThe grace window is a funny concept when you first consider it. This does not reflect any trial I have ever heard of actually being done but may identify an interesting or important effect. It is essentially outlying a treatment “protocol” and saying what if everyone adhered to this protocol counterfactual to what was observed.\n\n\nTo clone, you make two copies of the data, and make a new artifical censoring variable, which is a censoring time for the period when clones observed data are no longer consistent with their assigned strategy.\n\n1data_cloned = bind_rows(\n                     data %&gt;%\n                       mutate(assign = 0,\n                              t_artcens = if_else(t_treat &lt; time, t_treat, Inf)\n                       ),\n2                     data %&gt;%\n                       mutate(assign = 1,\n                              t_artcens = if_else(t_treat &gt; 12, 12, Inf)\n                              )\n                       ) %&gt;%\n3  mutate(t_clone = pmin(time, t_artcens),\n         event_outc = if_else(t_clone&lt;time, 0, event_outc))\n\n\n1\n\nClones assigned to strategy 1 (No treatment)\n\n2\n\nClones assigned to strategy 2 (grace window for treatment)\n\n3\n\nUpdate failure times and events counting artificial censoring\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI set failure times to Infinite when the event is unobserved, e.g. if no treatment, time to treatment is INF"
  },
  {
    "objectID": "03_tab1exs.html",
    "href": "03_tab1exs.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Description of cohort, “Table 1” In both randomized trials and observational studies comparing interventions/treatments there is typically a presentation of the overall cohort. Often researchers compare those receiving treatment versus alternatives. The table will present basic demographics and some additional characteristics of interest (confounders, important predictors/causes of outcome etc.). I generally do not present inferential statistics between groups in this table (i.e. no p-values or confidence intervals). At most I will provide a standardized mean difference. This follows some template like below: Basic “Table 1” set up often found in research manuscripts:\nTreatment No treatment Age - - Gender - - Chronic condition - - … - -\nSome special considerations are needed when presenting target trial emulations. Sequential target trials If conducting sequential daily target trial emulations, even though there may be only 100 unique persons in your study, because you repeatedly use some as controls, the number of observations in the control arm could be substantially larger. Assume 100 unique people analyzed across 2 sequential emulations, and on average 10% get treatment on any given sequence. You can expect ([100 * 0.1] + [90 * 0.1]) = 19 treated, and (90 + 81) = 171 control observations. So how do you present this in the “Table 1”? One approach is to present every observation. Since baseline is indexed to each sequence this can be important if the baseline information changes across sequences (e.g. a diagnosis is present for one sequence but not the other). However, this can be misleading (inflating sample size) or confusing to the reader especially if they are unfamiliar with emulation methods. It is important to report the number of unique persons and follow-up time if relevant as well as the total number of observations. Other possibilities include randomly selecting one observation per person within each group but analytical methods will lose precision. In the working example, 9 people used as controls in the first sequence, were included in the treatment arm for the second sequence. Present all observations Treatment N=19 (19 unique people) No treatment N=171 (90 unique people) Randomly select one observation per person Treatment N=19 No treatment N=90\nCloning & Grace Period The cloning presents an interesting problem for a “Table 1” because the group is identical at baseline. Artificial censoring will change the groups over time as the groups adhere to different treatment strategies. It probably makes sense for most projects using a clone-sensor-weight approach to present a “baseline” cohort column, and then two columns describing the intervention groups at the end of the grace period. Additionally, it makes sense to present the time-invariant covariates as well as time-varying covariates of particular interest (time-varying confounders etc.) Basic “Table 1” set up for a clone-censor-weight target trial emulation:\nBaseline (N=100) No treatment at 6 months (N=50) Treatment within 6 months (N=50) Age - - - Gender - - - Hypertension\n\n\n\n\nMonth prior to index\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "03_tab1exs.html#baseline",
    "href": "03_tab1exs.html#baseline",
    "title": "Reporting Results",
    "section": "",
    "text": "Month prior to index\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "04_wtnotes.html",
    "href": "04_wtnotes.html",
    "title": "Reporting Results",
    "section": "",
    "text": "Application of inverse-probability weighting:\nConsider a target trial emulation assigning those eligible for vaccination with Shingrix (RZV) in a nursing home to one of two treatment strategies:\nVaccination with a RZV within 6 months; No vaccination through follow-up\nThe outcome is an incident dementia diagnosis, and individuals censor for non-adherence to treatment strategy, or death\nFor practicality, we use month units of time, and we assume censoring due to death is non-informative and unaccounted for in the analysis.\nEach person is cloned and assigned to both strategies. For example, consider three persons again:\nPerson A Person B Person C Vaccinated in month 3 Unvaccinated through follow-up Vaccinated in month 6\nArtificial censoring: abbrev. HZV (vaccination within 9 months), no HZV (none ever)\nPerson A Person B Person C\nVaccinated in month 3 Unvaccinated through follow-up Vaccinated in month 6\nCensoring Censoring Censoring Period HZV No HZV HZV No HZV HZV No HZV 1 0 0 0 0 0 0 2 0 0 0 0 0 0 3 0 1 0 0 0 0 4 0 - 0 0 0 0 5 0 - 0 0 0 0 6 0 - 1 0 0 1\nThe probability of censoring is intrinsically tied to the probability of initiating treatment because we censor based on adherence to treatment strategy, it might be more appropriate to think of the weighting as “adherence weighting”.\nNo HZV strategy:\nFor the No HZV strategy, the probability of censoring is equal to the probability of initiating treatment (because you are censored if you get the vaccine!). Where V is vaccination, Pr(C) = Pr(Vt=i=1|Vt=i-1=0). Of course, 1 - Pr(C) is the probability of not censoring. The inverse probability of remaining uncensored is 1 / (1 - Pr[C]) at each timepoint.\nTime periods i, from 1 to end of followup, the IPW is 1 / (1 - Pr[C]) = 1 / (1 -Pr[Vt=i=1|Vt=i-1=0])\nPeriod Probability of censoring Probability of not censoring IPW i = 1,…I, where I is end of follow-up Pr(C) = Pr(Vt=i=1|Vt=i-1=0) 1 - Pr(C) 1 / (1 - Pr[C])\nHZV strategy:\nWhen assigned to vaccination within 6 months, the probability of censoring is 0 for each period up until the end of the grace period (G).\nPeriod Probability of censoring Probability of not censoring IPW i= 1, …, 1-G 0 1 1 / 1\nThis is because we have explicitly stated we are allowing a grace period of time (6 months) before people must be vaccinated.\nAt the end of the grace period, the probability of censoring is the probability of not being vaccinated, conditional on not yet receiving the vaccine.\nPeriod Probability of censoring Probability of not censoring IPW I = 1, …, 1-G 0 1 1 / 1 I = G Pr(C) = Pr(Vt = G=0|, Vt=G-1=0) 1 - Pr(C) 1 / (Pr[C])\nSo individuals who are vaccinated in the last period of the grace window are very important, because they are going to be assigned a weight according to the likelihood of receiving vaccination in that period. If the probability of vaccination in the last period given no prior vaccination is very small, 1 - Pr(Vt = G=0|, Vt=G-1=0), then those persons will be assigned very large weights. E.g. if 1% of the sample gets the vaccine at month 6, then the mean of the weights computed for them will be approximately 100 because 1/0.01 = 100. A key consideration is whether folks getting vaccinated in the last period meet causal assumptions of consistency, positivity etc. and you have good data to model the probability of initiation in this later period (time-varying confounders).\nNote: Conceptually, what we are doing here is emulating a trial where there is perfect adherence to vaccine strategy, but we allow a period of time (grace) before people must be vaccinated. If anyone doesn’t get the vaccine then we essentially force them to receive it at the end of the grace period. This is a somewhat extreme hypothetical, that is commented on by Jamie Robins in the appendix of the Cain article. There is some speculation that alternative weighting strategies could be applied which spread out “forced” vaccination across the grace period, but I have yet to see them in practice.\nIf a person receives the vaccine prior to month 6, they cannot be censored because they are fully adherent to treatment no matter what happens from that period forward. So if vaccinated prior to the grace period, the weight is 1.\nIn both cases however, the key estimand is the probability of vaccination at each point of follow-up. This is estimated in the full sample prior to cloning.\nSo a model must be fit which estimates probability of vaccination at time period i, given no vaccination in prior time period, baseline covariates (X), time-varying covariates (L), for each point of follow-up.\nPr(Vt=i=1 | Vt=i-1=0, X, Lt=i-1)\nOnce you have that basic probability is it easy to compute the weighting at each timepoint:\nBack to our cloned panel:\nPerson A Person B Person C\nVaccinated in month 3 Unvaccinated through follow-up Vaccinated in month 6\nIPW IPW IPW Period HZV No HZV HZV No HZV HZV No HZV 1 1 1 / Pr(Vt=i=0) 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 2 1 1 / Pr(Vt=i=0) 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 3 1 - 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 4 1 - 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 5 1 - 1 1 / Pr(Vt=i=0) 0 1 / Pr(Vt=i=0) 6 1* - - 1 / Pr(Vt=i=0) 1 / (1 - Pr[Vt=i=0])** -\n*Person A is vaccinated at month 3, so their probability of censoring is now 0, 1 - 0 = 1, 1/1 = IPW=1\n**Person C is vaccinated in the last period, so their probability of censoring is the conditional probability of not being vaccinated in the 6th period, given no vaccination up to that point. In other words their probability of remaining uncensored, is the probability of initiating treatment in that last period. So 1 / (1 - Pr[C]) is their IPW for this period. They are being weighted to estimate causal effects in a pseudopopulation where person B (who had been censored due to no vaccination) had also been vaccinated at timepoint 6 (a perfectly adherent, “per-procol” estimand).\n\n\n\n\n\n\nNote\n\n\n\nThese variables are generated independently, but in real-life would be correlated…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "",
    "text": "Data; Demonstrates how to simulate a dataset for use in CCW along with the cloning process. A key process is how to generate a “cloned” dataset, refer here\nEstimation; Main section which walks through each estimation step.\n\nFor production notes and future efforts see: About."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Tutorial for Clone-Censor-Weighting Analyses",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe tutorial represents my gathered and organized notes from research projects and didactic training. Collaborators and mentors include: Issa Dahabreh, Kaley Hayes, Daniel Harris, Donald Miller and Andrew Zullo."
  }
]