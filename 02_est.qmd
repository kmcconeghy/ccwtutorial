---
title: "Estimation"
bibliography: references.bib
---

```{r }
#| label: setup
#| echo: false
#| message: false
#| 
library(here())

source(here('scripts', 'setup.R'), echo=F)
# Set seed for reproducibility
set.seed(42)

 #increase size for bootstrapping procedure
  options(future.globals.maxSize = 4000*1024^2) 

  grid.draw.ggsurvplot = function(x) {
    survminer:::print.ggsurvplot(x, newpage=F)
  }
  
d_cloned = readRDS(here('dta', 'survdta_cloned.R'))
```

# Introduction

I recommend the following stepwise procedure in estimation: 

(PLR = pooled logistic regression; KM = Kaplan-Meier estimator).

1.  Estimate unweighted/unadjusted cumulative incidences with KM method
2.  Estimate a PLR model without weights\

<!-- -->

  i)  Compare KM vs PLR, estimates should be similar

<!-- -->

3.  Estimate censoring weights

<!-- -->

  i)  Estimate KM model for treatment initiation
  ii) Estimate PLR for treatment initiation
  iii) compare estimates, should be similar
  iv) estimate full weighted model, compute IPCW
  v)  examine IPCW weights for extreme values, non-sensical values
  vi) examine balance in covariates across time
  vii) generate "table 1" with some weighted difference statistic (e.g. wSMD)

<!-- -->

4.  Estimate weighted outcome models

<!-- -->

  i)  Estimate a weighted outcome model, no covariate adjustment
  ii) Estimate a weighted outcome + covariates (main estimate)

<!-- -->

5.  Once satisfied with steps 1-4, execute bootstraps
6.  In report, provide KM, unweighted, PLR estimates in an appendix; main results reported should be the IPCW-weighted PLR analysis with or without covariate adjustment in the outcome model (project specific decision).

::: callout-note
Pooled logistic regression is an important statistical model for target trial emulation because of its flexibility in estimating weights for time-varying confounding and the estimation of cumulative incidences. The PLR model approximates the hazards with some assumptions, see Technical Point 17.1 on page 227 of [Causal Inference: What If](https://www.hsph.harvard.edu/miguel-hernan/wp-content/uploads/sites/1268/2024/04/hernanrobins_WhatIf_26apr24.pdf), which explains why the odds approximates the hazard at a given time-point *k*, as long as the hazard is small (i.e. rare event) at time *k*.
:::

# Kaplan-Meier Estimator

A reasonable starting point is to evaluate the cloned dataset without probability weighting or pooled logistic regression models. Although this estimator is "naive" in that the artificial censoring is not considered, it is very useful for the following reasons:

1)  It allows examination of the overall time trends, censoring and sample size which can reveal fatal issues with the target trial emulation. For example, treatment is too rare in the grace window used, or event rate is unexpectedly high or low.

2)  The non-parametric model allows a visual examination of the cumulative incidences (or event free survival probabilities) across which are modeled parametrically in the pooled logistic regression. In other words, examining the time trend can help you determine if a simple polynomial or some more complex spline function is needed. This is not definitive however, because time-varying weights may change the time trends. But in my experience the unadjusted curve provides a good starting point, and practitioners should be very skeptical of weighted analysis that shows dramatically different time trends then the unweighted analysis (e.g. more likely to be an error in coding than real).

## Data

We continue the below using the cloned dataset generated in [Data](01_syndata.v2.qmd).

```{r }
#| label: desc-dta
glimpse(d_cloned)
```

## Estimation

```{r }
#| label: KM-est
  d_surv_pe = broom::tidy(
    survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned)) %>% # <1>
    mutate(assign = str_extract(strata,  '(?<=assign=)\\d')
    ) %>%
    arrange(time) %>%
    select(-strata) %>%
    mutate(pr_e = 1-estimate) %>% # <2>
    rename(pr_s = estimate) %>% # <2>
    pivot_wider(id_cols = c(time), # <3>
                names_from = assign, # <3>
                values_from = c(pr_e, pr_s, # <3>
                                n.risk, n.event)) %>% # <3>
    mutate(cir = pr_e_1 / pr_e_0, # <4>
           cid = (pr_e_1 - pr_e_0)) # <4>
```

1.  Naive estimator, K-M estimator, assign is by clone (not person). `t_clone` is the follow-up time for each clone, taking into account artificial censoring.
2.  `estimate` from the model is the survival probability, so probability of event is 1 - estimate
3.  Data is in long form, reshape so one colum per group with cumulative incidence/survival
4.  Estimands, `cir` is ratio analogous to relative risk, and `cid` is analogous to risk difference

## Summarize KM estimator

```{r }
#| label: gg-km
ggsurvplot(survfit(Surv(t_clone, event_outc) ~ assign, data=d_cloned),
                            data=d_cloned,
                            fun = 'event',
                            xlim = c(0, 60),
                            xlab = 'Follow-up',
                            break.time.by=6,
                            palette = c('red', 'blue'),
                            linetype = 'strata',
                            censor=F,
                            cumevents=T,
                            conf.int=F, pval=F,
                            risk.table=T, risk.table.col = 'strata',
                            risk.table.height=0.25, 
                            ggtheme = theme_classic())
```

Later I will show how to summarize point estimates and confidence intervals with bootstrapping.

# Pooled logistic regression

Next, we proceed to essentially replicate the finding of the KM estimator with a PLR model. No weights or covariate adjustment yet. I think this step is important to test your code, and make sure you have a good starting parametric model for the outcome to avoid any major issues downstream when you add weights. This PLR model should be able to closely approximate the KM curves, if it doesn't then something is wrong.

## Build a longitudinal panel

The PLR model is a person-time procedure, modeling the probability of the event where the observation is person at follow-up time *k*. The KM estimator is a person-level dataset, so the dataset much first be expanded to person-time units. If there are many persons, or many observations (timepoints) per person the dataset can become quite large. I personally use the `data.table` package to help with computation. It is blazingly fast, so the code reflects that approach which may not be needed for your project.

```{r }
#| label: exp-data
    setDT(d_cloned) # <1> 
    d_panel = d_cloned[rep(seq(.N), t_clone)] # <2>
    d_panel[, exit := (seq_len(.N)), by = list(id, assign)] # <3>
    d_panel[, enter := exit-1] # <3>
    d_panel[, time := seq_len(.N), by = list(id, assign)] # <4>
    d_panel[, event_outc := if_else( t_clone <= time, event_outc, 0L), by = list(id, assign)] # <5>
    d_panel_outc = select(d_panel, id, time, event_outc, t_treat, assign, enter, exit) 
```

1.  Convert data to a `DT` object for faster computations
2.  Generate rows along the sequence of `.N` (number of time-points) up to the last follow-up point
3.  For each row, compute the starting (`enter`) and ending time-point (`exit`). This isn't really necessary for our toy example, but could be important depending on how you are setting up the longitudinal panel and planning to model the time-trend.
4.  This `time` is the key variable, a count from 1 to the last observed follow-up point by person, clone
5.  Outcome is = 1 in row where event occurred, so in the data a person-clone should have values of zero for `event_outc` up until the last observed time-point where `event_outc`=1 IF the event occurred at that time.

```{r }
#| label: desc-dta-panel
glimpse(d_panel_outc)
```

::: callout-note
Note how the dataset has expanded from N=20000 to N=554k! This method is very memory hungry...
:::

## Estimation

The PLR model requires specification of a function of time. This choice is informed by the KM estimator plot of the cumulative incidences, but a polynomial is a good starting point (i.e. time + time\^2). You can choose to either estimate the outcome in a model with both clones combined in one dataset OR you can estimate cumulative incidences separately (two models with data limited to assign==1 & assign==0 respectively). In the combined data, you must specify an interaction between treatment (clone assignment) and time, e.g. `time + time^2 + treat + treat*time + treat*time^2`, shorthand below is `poly(time, 2, raw=T)*assign`.

```{r }
#| label: plr-naive-est
  
  # defined above
  d_glm = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial()) # <1>
  
  d_panel_outc$pr_ev = d_glm$fitted.values # <2>
  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] # <3>
  
  d_res = d_panel_outc %>% # <4>
    group_by(assign, time) %>% # <4>
    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %>% # <4>
    ungroup %>% # <4>
    pivot_wider(., id_cols =c('time'), # <4>
                names_from = assign, # <4>
                names_prefix = 'pr_ev_', # <4>
                values_from = pr_ev # <4>
    ) %>%
    mutate(cid = pr_ev_1 - pr_ev_0, # <4>
           cir = pr_ev_1 / pr_ev_0) # <4>
```

1.  PLR model, with time\*treat interaction. Binomial family for logistic regression.
2.  If event_outc = 1 when event occurs, the fitted values are the probability of the outcome at time k.
3.  To describe cumulative probabilities, you take the cumulative product of the fitted probabilities `pr_ev`, making sure to only compute within group and clone.
4.  After you have cumulative event-free survival probability, then you can summarize by group/time as described above for the KM estimator.

```{r }
#| label: gg-plr-naive
    d_gg_ci = d_res %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = pr_ev_0), color='red') +
      geom_line(aes(y = pr_ev_1), color='blue') + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Cumulative incidence')
    
    d_gg_ci
```

At first glance, plot looks reasonable. We now directly compare PLR to the KM estimator by plotting together to see if the modeling appears to be working:

```{r }
#| label: gg-cmp-naive-models
#| echo: FALSE
d_ci_cmp = inner_join(
  select(d_res, time, pr_ev_0, pr_ev_1, cid) %>%
    rename(plr_0 = pr_ev_0, plr_1 = pr_ev_1, plr_cid = cid),
  select(d_surv_pe, time, pr_e_1, pr_e_0, cid) %>%
    rename(km_0 = pr_e_0, km_1 = pr_e_1, km_cid = cid),
  by = join_by(time)
)

d_gg_ci = d_ci_cmp %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = plr_0), color='#A60A3D', linetype=2) +
      geom_line(aes(y = km_0), color='red', linetype=1) + 
      geom_line(aes(y = plr_1), color='navyblue', linetype=2) +
      geom_line(aes(y = km_1), color='blue', linetype=1) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = '', y = 'Cumulative incidence')
    
 d_gg_rr = d_ci_cmp %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = km_cid), color='green', linetype=1) + 
      geom_line(aes(y = plr_cid), color='forestgreen', linetype=2) +
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Risk Differences')
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr,
                       nrow=2)
    
d_gg_1
```

So the PLR model using a simple polynomial can approximate our KM estimate reasonably well (This should work because we used synthetic data generated with an exponential distribution for time, but real-world data will not play so nicely). There is some noise between the KM estimator risk differences across certain time-points. At this point it would be a project specific judgement whether to accept this, or test out other parametric functions. Consider following points:

1.  It may not matter if PLR and KM are inconsistent at earlier time-points if the plan is to only summarize the results at later periods.

2.  The non-parametric KM estimator may be imprecise with small sample sizes and/or rare outcomes. The risk difference estimates at each time-point may have considerable random variation, and the parametric model is essentially smoothing out this noise. So while they should be approximately the same, you do not want to overfit the random noise of the KM estimator.

3.  These initial steps will not guarantee a good fit after weighting is applied, it is only a beginning.

# IPCW Pooled Logistic Regression

Once the basic procedures are setup and there is some confidence in the ability to model the cumulative incidences, weighting can begin.

::: callout-note
There is no consideration here to model training, causal DAGs or covariate selection. It is simply a toy example to show the procedure.
:::

## Build a longitudinal panel

As before the person- or clone-level data must be expanded to a longitudinal panel by time. Here, two datasets must be constructed, one for estimation of censoring probabilities and one for estimation of outcome probabilities.

### Censoring dataset

In our example, artificial censoring is tied to whether treatment is initiated within a grace window or not. This is very specific to a project's definitions of treatment so proceed with caution. The basic idea is that since clones censor according to whether treatments starts (or not), the probability of censoring is essentially the probability of initiating treatment. So we generate a dataset that is at the person-level (no cloning):

```{r }
#| label: dta-censor
d_treat = d_cloned %>%
    dplyr::filter(assign==0) %>% # <1>
    mutate(start=1,
           end = t_treat) %>% # <2>
    group_by(id) %>% # <3>
    mutate( # <3>
      time = t_clone, # <3> 
      outcome = if_else(time==t_treat, 1, 0) # <3>
    ) %>% # <3>
    ungroup # <3>
```

1.  This may be a little confusing, but I am taking clones where assign=0 from the `d_cloned` dataset which is a person-clone level dataset. These are clones assigned to not receive treatment, so their censoring time is time of treatment start.
2.  We are estimating time to treatment, not outcome
3.  *Artificial* censoring time is same as treatment time unless dead first, and outcome = 1 if treated.

Then we expand the dataset as described above:

```{r }
#| label: exp-dta-censor
  setDT(d_treat)
  d_panel = d_treat[rep(seq(.N), time)]
  d_panel[, exit := (seq_len(.N)), by = list(id)]
  d_panel[, enter := exit-1]
  d_panel[, time := seq_len(.N), by = list(id)]
  d_panel[, event_treat := if_else(t_treat <= time, treat, 0L), by = list(id)]
  d_panel_treat = select(d_panel, id, time, event_treat, t_treat, enter, exit, end, age, female, dx_1, dx_2) # <1>
```

1.  Note we keep covariates for estimation of censoring weights.

## Estimation

We fit a model with the outcome of censoring, including the variables which are key. Then we add fitted probabilities back to the dataset, calculate cumulative probabilities of remaining uncensoring, and join those estimated probabilities to the outcome dataset.

```{r }
#| label: est-censor-prob
  d_glm_wt = glm(event_treat ~ poly(time, 2, raw=T) + poly(age, 2) + female + dx_1 + dx_2, data=d_panel_treat, family=binomial()) # <1>
  d_panel_treat$pr_treat = d_glm_wt$fitted.values # <2>
  setDT(d_panel_treat)
  d_panel_treat[, cumpr_notreat := cumprod(1-pr_treat), by = .(id)] # <3>
  
  d_panel_treat = select(d_panel_treat, id, time, cumpr_notreat) # <4>
  d_panel_outc = left_join(d_panel_outc, d_panel_treat, by=c('id', 'time')) # <4>
```

1.  Estimation of probability of censoring in PLR
2.  Add fitted probabilities to dataset
3.  Calculate cumulative probability of non-treatment (treatment-free survival)
4.  Join probabilities back to outcome dataset

::: callout-note
I think there is some controversy in this procedure. Others have not modeled the censoring probability off of a person-unique (no clones) dataset with treatment times, but rather directly in the cloned dataset with the outcome of "censoring" where that may mean treatment initiation or some other thing.[@gaber2024]
:::

## Calculation of weights  

This step is highly specific to a project and must be considered carefully. I have found this step is the most prone to errors due to coding or misunderstandings about what the treatment strategy entails, or if the data is setup incorrectly. I refer you to [Weighting](04_wtnotes.qmd) for further discussion. 

```{r }
  setDT(d_panel_outc)
  
  d_panel_outc[, ipw := fcase(
    assign==0, 1 / cumpr_notreat, # <1>
    assign==1 & time < 12, 1, # <2>
    assign==1 & time == 12 & t_treat < 12, 1, # <3> 
    assign==1 & time==12   & t_treat   ==12, 1 / (1-cumpr_notreat), # <4> 
    assign==1 & time==12   & t_treat    >12, 0, # <5> 
    assign==1 & time > 12, 1 # <6>
  )]
  d_panel_outc[assign==1, ipw := cumprod(ipw), by=list(id, assign)] # <7>
```
1. (assign=0) Cumulative probability of no vaccination = Probability of remaining uncensored
2. (assign=1) Clones cannot artifically censor prior to grace
3. (assign=1) If treatment started prior to grace window ending, the clone cannot censor 
4. (assign=1) If a clone is treated in the final period, then the probability of remaining uncensored is the probability of initiating treatment by the final period OR (1 - cumulative probability of no treatment at time-point of grace window). 
5. (assign=1) If a clone is not treated they censor at the end of the window
6. (assign=1) In periods greater than the end of the grace window, no artificial censoring occurs (they either are treated or already censored at this point). 
7. After setting these conditions, we compute the cumulative product of the weights.

```{r }
#| label: desc-ipw
  summary(d_panel_outc$ipw)
```

Now with the estimated weights, it is simple to generate weighted cumulative incidences:  

```{r }
  d_glm_pe = glm(event_outc ~ poly(time, 2, raw=T)*assign, data=d_panel_outc, family=binomial(), weights = ipw) # <1>
  
  d_panel_outc$pr_ev = d_glm_pe$fitted.values # <2>
  d_panel_outc[, `:=`(pr_surv = cumprod(1 - pr_ev)), by=list(id, assign)] # <2>
  d_res = d_panel_outc %>% # <2>
    group_by(assign, time) %>% # <2>
    summarize(pr_ev = mean(1-pr_surv), .groups = 'drop') %>% # <2>
    ungroup %>% # <2>
    pivot_wider(., id_cols =c('time'),  # <2>
                names_from = assign,  # <2>
                names_prefix = 'pr_ev_', # <2>
                values_from = pr_ev # <2>
    ) %>% # <2>
    mutate(cid = pr_ev_1 - pr_ev_0,  # <2>
           cir = pr_ev_1 / pr_ev_0) # <2>
```
1. Note the `weights = ipw` argument, everything else is same as PLR model above.  R `glm()` will generate a warning message because the weighted counts are "non-integer", but this is expected and not a problem. 
2. Summarize across group, time as before.  

```{r }
  ### Plot ----
    d_gg_ci = d_res %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = pr_ev_0), color='red') +
      geom_line(aes(y = pr_ev_1), color='blue') + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Cumulative incidence')

    d_gg_rr = d_res %>%
      ggplot(aes(x=time)) +
      geom_line(aes(y = cir), color='green') + 
      scale_y_continuous(limits = c(0.5, 1.1)) + 
      scale_x_continuous(breaks = seq(0, 60, 6),
                         limits = c(0, 60)) +
      theme_bw() +
      labs(x = 'Follow-up', y = 'Relative Risk')
    
    d_gg_1 = ggarrange(d_gg_ci, d_gg_rr,
                       nrow=2)
    
    d_gg_1
```

::: {#refs}

:::
